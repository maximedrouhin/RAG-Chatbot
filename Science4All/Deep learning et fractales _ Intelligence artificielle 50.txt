n'observant peric mans que le diplôme ning fonctionne spectaculairement bien quand on dispose de quantités massives de données il semble que plus on ajoute deux couches de neurones plus notre réseau de neurones atteint des performances stupéfiantes voilà qui soulève un problème de taille pourquoi est-ce qu'un nombre de paramètres égale les réseaux fin est profond ont-ils des performances meilleures que les réseaux large et peu profond d'où vient la déraisonnable efficacité de la profondeur pour répondre à cette question certains chercheurs ont réussi à déterminer des fonctions spécifiques aux réseaux profond ainsi certains ont montré qu'il existait des fonctions calculé par un réseau profond mais qu'un réseau peu profonds ne pourra pas calculer à moins de disposer d'un nombre exponentiel de neurones et de synapses cependant les fonctions en question sont très biscornus et il semble peu probable que ce soient les fonctions dignes d'intérêt en pratique en 2016 des chercheurs de google et stanford ont attaqué le problème sous un autre angle très intéressant au lieu de s'intéresser à certaines fonctions que peuvent calculer les réseaux profond ils ont cherché à étudier les fonctions typique que calcul les réseaux profond autrement dit si je prends un réseau profond au hasard à quoi ressemble la fonction que ce réseau calcul et comment la profondeur peut-elle modifier les propriétés de cette fonction ça fait des questions bien compliqué mais pour les comprendre on peut se ramener à plier une feuille de papier alors techniquement les espaces qui sont manipulés par les réseaux de neurones sont en fait des espaces de très grandes dimensions de dimension 3 cents ou parfois même un million mais par analogie on peut illustrer ce qu'il s'y passe avec un espace de dimension 2 comme une feuille de papier en particulier dès lors si les fonctions d'activation des réseaux de neurones sont des fonctions linéaire par morceaux comme l'heureux lui dont un parent est dans l'épisode 41 alors chaque neurone effectué une espèce de plis une feuille de papier en particulier une activation relu revient à prendre une feuille à la plier selon une direction une couche de neurones relu la pliera dans différentes directions et donc si on a beaucoup de neurones dans une couche autrement dit si notre réseau est très large on aura alors un très grand nombre de pays et comme vous le voyez ça va créer pas mal de régions différentes de la fin on peut alors considérer que la complexité de la fonction calculé par le réseau de neurones est en quelque sorte le nombre de régions ainsi créée est clairement plus le réseau large plus la fonction calculé sera composée de beaucoup de régions différentes oui mais à quel point le nombre de régions créé grand dit-il vite combien de régions sont créées par des pillages d'une couche 2009 rhône j'avais été matin parmi vous à mettre pause et à répondre explicitement à cette question est bien le premier pli va couper l'espace puis le second plis va couper chacun des deux sous espaces créés par le premier pays il va donc créer deux nouveaux espaces le troisième plis par contre lui ne pourra pas couper tous les sous espaces peu importe comment vous le mettez il y aura un déçu et ce parce que le troisième pays ne divisera pas le troisième clive en fait coupé trois des quatre sous espace créant ainsi trois nouveaux sous espace est en fait plus généralement le énième plis va couper n sous espace créant ainsi n nouveaux espaces alors est ce qu'on est sûr de cela bien vous pouvez vérifier que c'est le cas en notant que ce énième plis devra traverser chacun des haines - un ampli précédent chaque traversée d'un pli existant est un changement de région c'est même le seul changement de région possible donc si on suit ce pli on ne peut donc traverser au plus que n région le nombre total de sous- région créée par renggli sera donc égale à 1 + 1 + 2 + 3 + 4 et ainsi de suite jusqu'à elle et ça c'est égal à haisnes fois une pulsion divisé par deux puces 1 comme on l'a vu dans un épisode sur l'infini quand on fait beaucoup depuis ce nombre va bien sûr devenir assez grand on pourra alors la proximité au premier ordre par rennes au carré divisé par deux on dit que le nombre de régions et quadratique ans le nombre de plis et quadratique c'est bien mais ce n'est en fait pas une croissance de folie après mille plis n'aura un demi million de région et ça il faut le comparer avec ce qui se passe lorsque au lieu d'effectuer lépi en parallèle on effectue des successions de plis ce qui est intéressant c'est que faire des successions de plis c'est précisément ce que sont les réseaux profond où chaque couche plis par dessus les plis de la couche précédente autrement dit un réseau profond correspond à replier un pli déjà fait puis à l'heureux replier et ainsi de suite et quand on déplie tout ça on voit que cette fois le nombre de régions est en fait bien plus grand il est en fait multiplié par deux à chaque repli ce qui fait qu'après n repli on a deux puissances n région on dit que le nombre de régions est exponentielle on la largeur du réseau et ça ça fait une grosse différence parce qu'avec 1000 plie le nombre de régions est cette fois de deux puissances 1000 ce qui est formidablement plus que le nombre de particules dans l'univers observable en particulier ça n'a strictement rien à voir avec le demi million du réseau large en fait ce que les chercheurs de google et stanford ont démontré en 2016 c'est que le nombre de régions d'un réseau de neurones avec activation relu et de l'ordre de largeur puissance profondeur mais à mon sens ce n'est pas le plus intéressant le plus intéressant ce qui est même très frappant c'est que les figures qu'on obtient avec une plus grande profondeur ou du coup ne cessez de vaciller dans tous les sens et former des figures stupéfiante pour le comprendre on peut plier et repliée non pas une feuille de dimension 2 mai une espèce de feuilles de dimensions 1 après beaucoup beaucoup de pie et même plutôt à l'infini on obtient alors une figure étrange appelé la courbe du dragon dont michael parle dans cette vidéo et cette courbe du dragon porte en fait assez mal son nom car ce n'est pas vraiment une courbe c'est plutôt une fractale les réseaux profond semble avant tout calculer des fractales ben on pourrait croire qu'il s'agit là d'une propriété spécifique aux fonctions de l'activation relu et bien pour étudier les réseaux de neurones avec des fonctions d'activation dit sigmoïde al ces mêmes chercheurs de google ont étudié des réseaux de neurones aux paramètres aléatoires ils se sont demandés comment ces réseaux de neurones transformer des figures assez simple mieux comprendre je vous rappelle qu'un réseau de neurones correspondait en fait à une fonction d'un espace vectoriel vers un autre espace vectoriel leur idée est de dessiner un cercle dont le premier espace vectoriel et de voir comment ce cercle était bidouillée par un réseau de neurones à l'aide ouah en particulier ils se sont intéressés à une espèce de mesures de fractal it et de la courbe alors plus précisément cette mesure consiste à intégrer la norme des variations du vecteur tangent unitaire à la courbe intuitivement plus la courbe gigote dans tous les sens plus le vecteur tangent unitaire varie vite et plus la norme de ces variations est grande et de façon intéressante une telle mesure est invariante par homothétie en particulier la figure fermé qui minimise cette espèce de frac qualité d'une courbe fermée et le cercle qui ne se courbe que pour se refermer si vous n'avez pas compris ce charabia mathématiques c'est normal quoi qu'il en soit les chercheurs ont montré que pourvu que les pois synaptique des réseaux de neurones soit suffisamment agité la fraternité moyenne de l'image d'un cercle par un réseau de neurones aléatoire augmenter de façon polynomiale avec la largeur du réseau mais de façon exponentielle avec sa profondeur autrement dit intuitivement les réseaux de neurones profond semble beaucoup plus approprié pour calculer des fonctions quasi fractale et ça ça me laisse très songeur les fractales ces objets biscornu que visiblement vous adorez puisque ma vidéo la plus vue et celles qui on parle longuement ces objets auxquels micmac à dédier une de ses meilleures vidéos ces objets fantastique dont la plus incroyable est sans doute la fractales demande de bruts qu'elle a décrits dans cette superbe vidéo serait donc intimement liée à la déraisonnable efficacité du type learning voilà qui suggèrent fortement que le figure fractale sont bien plus omniprésente que ce que l'on pourrait croire naïvement et qu'elles sont en fait crucial pour bien comprendre le monde qui nous entoure après tout une fois que le concept même de fractal a été formalisée par wendel drogue dans les années 70 ou n'a cessé de les trouver un peu partout de la bourse aux filaments cosmique de notre univers en passant par toutes sortes de processus biologiques de structures géologiques et de mathématiques des systèmes dynamiques les fractales semble partout et il semble que la profondeur des réseaux de neurones soit crucial pour les comprendre mais toutes ces fractales ce ne sont que des fractales visualisables or les quasi fractales que calcul les réseaux de neurones correspondent à des structures souvent bien plus abstraite en fait la réflexion vraiment fascinante que me suggère le succès des réseaux de profond et les travaux des chercheurs de google et stanford c'est la question suivante est ce que finalement la bonne sémantique pour bien parler des images de votre quotidien ou de notre langage naturel ne corresponde est elle finalement pas à identifier des structure fractale par exemple parmi l'ensemble des images il ya des images de chats et des images sans chat on peut donc a priori à peu près séparer les normes espace vectoriel des images en deux parties chat est pacha question à quoi ressemblerait le sous-ensemble des chats en particulier se pourrait-il que le sous-ensemble des chats cette structure géométrique à l'intérieur d'un espace de dimensions 1 million se pourrait-il qu'il s'agisse d'une fractales ou du moins de l'intérieur d'un espace dont la frontière serait elle une fractale eh bien j'ai envie de parier que oui il me semble extrêmement probable que toute définition satisfaisante de ce qu'est une image de chats qu à ce monde a identifié une structure quasi fractales dans des espaces de très grande dimension voilà qui me laisse rêveur [Musique] avant de revenir à votre question il n'a plusieurs parmi vous qui m'ont demandé où est ce que vous pouvez trouver ces super casquette la casquette bayésienne j'ai aussi d'autres une cassette altruisme efficace que vous avez vues dans la vidéo de mardi la confiance que j'ai donnée est aussi des t-shirts qui correspondent à tous ce prosélytisme bavaisien absolument scandaleux bain vous pouvez les retrouver sur une boutique est ouverte sur le site spreadshirt donc allé voir si vous voulez vous habillés de tenues qui font un peu ostentatoire de cette secte qui est le gaz à nice mme où la crise efficace alors sachez que je ne touche rien des ventes de ce site si j'ai vers ce site en fait c'est surtout pour m'acheter moi même ce genre de produits est également pour permettre à ceux qui voudraient aussi d'en acheter parce que je pense que c'est un bon moyen en fait de faire une action qui est peut-être finalement une action altruiste efficace que d'essayer de promouvoir la bonne façon de réfléchir et bonne façon de faire des bonnes actions le gars est assez cool d'avoir un peu au quotidien des équations aussi importante et aussi fondamental que les équations de la forme le buzz ou bien sûr ça veut pas dire qu'il faut une fois vous avez porté cette casquette et voir les gens et leur dire il faut que tu sois beau dans tout ça non juste porter la casquette c'est la 1ère fois un moment dans la vidéo j'ai démontré deux images et vous allez demander laquelle des deux images a été créé par un gagnent et d'après vous à l'image du mec est à l'air beaucoup plus d'être une image de fabriquer que l'image de la fille et que sar axa a vu le troll venir en fait ce genre de blagues sur des questions je l'avais déjà posé plusieurs fois en conférence en avant en scène politique avec deux autres images et ouais bah j'aime bien montrer les deux images en fait qui ont été générés par le gan pour se dire que bon ben voila si on cherche à c'est de trouver les failles dans les images eh bien on peut les trouver mais si on ne les cherche pas tant que ça si l'on montre une image on nous dit même pas qui a été créé par un gagnent là on s'est beaucoup plus facile de se faire avoir bref il faut aussi travailler notre esprit critique lorsque l'on voit ce genre d'image et finalement ça revient à jouer le rôle d'adversaires dans la structure de béganne des dettes génératif anniversario et de façon très générale en fait l'adversaire des modèles génératif adverse à riom ça correspond vraiment une sorte d'esprit critique qui permet de distinguer ce que l'on croit avoir compris en testant ce qu'on croit avoir compris en générant des données fictives et notre esprit critique éthique doit être capable de distinguer notre capacité à imiter notre capacités d'imitation avec ce qui est vraiment observer et distinguer le faux du vrai donc en fait ces structures de gan c'est quelque chose qui est très utile d'un produit algorithmique et ça marche du tonnerre c'est ferran de truc spectaculaire est sans doute qu'ils auraient des trucs encore plus impressionnant dans l'avenir dans l'année à venir mais c'est également quelque chose qui me semble extrêmement utile à intégrer dans notre propre façon de réfléchir pour essayer d'être de bien analyser le monde il faut toujours avoir nous si possible un adversaire voire même si possible même plusieurs adversaires plusieurs avis qui leur façon de critiquer l'authenticité de différentes données et en particulier la capacité que notre modèle intérieur du monde à à expliquer les vraies données et sam hanna aussi un autre point pour que cet esprit critique fonctionne pour que l'adversaire et quelque chose à dire il faut absolument qu'il y ait un modèle générateur quelque part dans leur tête c'est à dire quelque chose qui cherche constamment essayé de faire des prédictions essayez de projeter par exemple le futur a essayé d'imaginer le passé en essayant de reconstituer le mieux passé et en confrontant ça avec des bras donné ce travail de reconstitution de prédiction c'est quelque chose qui est indispensable à la structure des gan et il ya fort à parier que ce soit quelque chose d'absolument indispensable également pour l'apprentissage typiquement si vous voulez devenir bon en mathématiques il faut absolument qu'il y ait un système rookie il soit qu'ils fassent l'effort de générer des preuves mathématiques rigoureuse civiquement et il faut aussi que vous soyez vous même l'adversaire de vos propres démonstration de proposer en essayant de trouver les failles dans vos propre raisonnement ça c'est quand même le genre de truc que j'adore avec le machine learning j'ai rarement avec ses mathématiques appliquées à l'apprentissage et au savoir c'est que ça nous apprend souvent énormément de choses sur nos propres de façon de fonctionner et notamment ceux qu'on devrait faire pour mieux réfléchir est typiquement dans le livre la formule de savoir que j'ai publié udp science s'il est beaucoup beaucoup beaucoup question de cela pas gagné 75 lui réagi notamment au fait que dans la vidéo que j'ai présentée à la semaine dernière j'ai présenté vraiment la structure de gamme la plus classique notamment celles proposées par liane goût de fellow et ses collaborateurs qui consiste notamment à juger la performance du générateur et de l'adversaire à l'aide de cette divergence kallel de cette pénalité le gars rythmique de la probabilité de l'otan cité à signer par l'adversaire aux différentes données à la fois réel et aux données générées par le générateur et pas gagné 75 ans a fait la remarque que cette approche est pas forcément la meilleure approche quand cette mesure de la qualité d'un fake typiquement n'est pas forcément la meilleure approche en particulier récemment notamment ce qui est licite trois papiers d'octobre 2018 donc c'est tout chaud il y a pas mal d'efforts pour utiliser une autre métriques qui serait notamment reliées à ce qu'on appelle la métrique et de wasserstein qui est liée au problème du transport optimal problèmes sur lesquels notamment un certain cédric villani a beaucoup travaillé et a priori sa médaille fields a été attribué en partie pour ces travaux reliés au problème du transport optimal alors qu'est ce que ce problème du transport aux tilleuls en quoi est-ce que c'est différent d'un divorce qu'elle en fait le problème des divergences cas elle sait que vous pouvez avoir une image quasiment réaliste mais avec un petit défaut et la divergence cas elle vous dira bas cette image est pas du tout réaliste parce que même si elle ressemble beaucoup bahia il ya un défaut majeur intuitivement la métrique vasseur chien va prendre en compte le fait que la plupart de l'image est quand même assez bonne et que du coup il ya un petit effort seulement à faire pour passer de l'image générée à la vraie image en particulier une métrique naturelles ont l'espace des images si on change quelques pixels on sent que ça fait pas une grande différence et la métrique de vassar chine va essayer de prendre ça en compte dans sa mesure de la distance en fin de la similarité entre deux images alors en fait la métrique wasserstein s'est encore plus subtil que ça puisque en fait ces histoires de transport de distribution ça sera un petit peu trop mou qui a expliqué sont ici j'en parle par contre dans le livre du coup je me lance en plus il ya un chapitre là dessus après le truc que je pensais à priori avant de lire le commentaire de pagan 75 c'est que le problème de cette métrique de wasserstein était qu'elle était trop difficile à calculer mais apparemment il y aurait des variantes de cette métrique de bathurst high notamment la distance si chrome qui permettrait d'avoir un peu cette notion de métriques de serge taille mais de façon plus calculable qui se prêterait du coup assez bien aux réseaux de neurones celle ci ça ça marche d'un point de vue algorithmique il me semble que ce sera une solution qui sera bien meilleure que la divergence locale puisqu'elle permet de refléter cette idée très intuitive et qui semble très pertinente que deux images qui se ressemblent ne sont finalement pas si différente et qu'il faut prendre en compte le fait qu'elle ne soit pas si différente d'autres manières de mesurer le degré d'authenticité d'une image profite encore plein de recherche à faire est sans doute plein de trucs trop cool encore à découvrir j'espère que j'avais mais cet épisode là plus loin on va continuer dans notre exploration des explications théoriques et du succès empirique du dip learning de la profondeur plutôt que la largeur des réseaux de neurones on parlera notamment d'un article que j'ai coécrit avec rachid jean-louis un professeur à l'epfl vous pouvez d'ores et déjà kaki l'article qui est mort non ici on nous l'a assez vu si vous avez aimé cet épisode tant celle là quelques commentaires et le partage et pense à vous abonner pour une parodie et futurs des visages merci au team powered et j'espère que vous serez là la prochaine donc récapitulons si on fait un pli en obtient elle comme ceux ci deux plis trois plis on observe ces figures là et bien entendu on peut poursuivre ce processus aussi longtemps qu'on veut si vous le faites avec top 10 vous pouvez essayer chez vous où vous obtiendrez cette figure et puis après allez 5.6 pelisse 7 puis 8 lits 9,6 deeply et ainsi de suite et vous voyez que plus vous ferez de pluie plus vous vous rapprocherez d'une figure limite une figure qui apparaît comme ceux ci sous nos yeux avec ce simple petit processus de pliane et c'est cette figure limites qu'on appelle la courbe du dragon et c'est toujours quelque chose d'assez extraordinaire de ces magnifiques et même d' assez émouvant mathématiques quand on voit ce genre de phénomène qui apparaît quand on parle d'une petite règles de construction toutes simples comme la juste le pliage d'une bande de papier et qu'on voit apparaître comme ça comme par magie une sorte de figure absolument merveilleuse sous nos yeux on peut maintenant faire comme tout à l'heure et coloré en noir les points c'est pour lequel l'afrique talent de julia de formule des dockers et pucés et connexes dernière d'un seul morceau et en couleurs les autres on obtient alors ceci on peut voir ça comme une carte des fractales de julia où chaque point correspond à un ensemble de julien cet ensemble porte le nom d'ensemble demandé le brottes et est pour bien des raisons là fractales la plus importante des mathématiques cette carte des fractales est elle même une fractale puisqu'elle présente encore une fois de l'auto similarités même si cela ne saute pas aux yeux au premier abord la somme demandée le broad présente des copies de lui-même un peu partout