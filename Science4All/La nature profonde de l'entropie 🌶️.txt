tu devras l'appeler entropie pour deux raisons tout d'abord ta fonctionne en certitude a été utilisée en mécanique statistique sous ce nom elle a donc déjà un nom deuxièmement et surtout personne ne sait vraiment ce qu'est l'entropie de sorte que dans un débat tu auras toujours l'avantage tel fut le conseil que le génie americano hongrois John von Neman donna à Claude Shannon lorsque le second proposa sa mesure de quantification de l'incertitude dans les années 1940 enfin pourrait-on croire la nature profonde de l'entropie a été saisie 80 ans plus tard forcé de connaître que si la science de l'entropie a énormément avancé très peu de gens savent aujourd'hui vraiment ce qu'est l'entropie en fait tout physicien sera même bien embarrassé lorsqu'on lui demandera de calculer l'entropie d'un ordinateur ou de tout objet complexe qui est très loin d'être un gaz à l'équilibre thermodynamique contrairement à la mesure de la pression ou du volume des objets qui ne souffrent pas d'ambiguté scientifique là où il semble y avoir un paradoxe profond c'est que ce concept est pourtant au cœur du second principe de la thermodynamique l'une des théories les plus indéboulonnables de la science tandis que les lois de la mécanique de Newton les lois de l'électromagnétisme de Maxwell et les lois de l'espace-temps de Galilée ont tout été ébranlé par les sciences modernes et alors que le premier principe de conservation de l'énergie est aujourd'hui réinterprété comme une simple conséquence de l'invariance dans le temps des lois de la physique ce second principe résiste encore obstinément à l'expérimentation et à l'entendement plus encore que les bizarreries quantiques le second principe est à mesieux le mystère ultime de la science même si les physiciens sont en fait encore incapables de le définir aussi précisément que l'énergie la courbure de l'espace-temps ou encore l'électrodynamique quantique ce principe semble gouverner plus encore nos sociétés en imposant des limites strictes au champ des possibles après tout comme l'explique très bien David de scienceces étonnante ce qui fait défaut à nos sociétés n'est pas vraiment l'énergie en fait le changement climatique n'est autre qu'un excédent d'énergie sur terre et en particulier quand on dit que l'énergie est le moteur de la croissance on dit en fait quelque chose de profondément erroné sur le plan scientifique en tout cas si on prend énergie au sens scientifique en fait si nos machines tournent c'est davantage en augmentant l'entropie qu'en consommant de l'énergie ceux qui le comprennent peuvent alors être tentés de se jeter sur le concept d'entropie et faire croire que en vertu notamment du second principe de la thermodynamique nous traversons une crise de l'entropie certains vont même jusqu'à prétendre que le second principe garantit le désordre et qu'il y aurait une limite physique fondamentale et indépassable à l'augmentation du chaos et de la pollution qui justifierait un profond climatoofatalisme et là ça commence à me faire sérieusement grincer des dents surtout quand on sait à quel point ce genre de climatofatalisme est désormais abusé par les ex climatodénialistes pour freiner l'action car eux semblent en fait surtout davantage scientifiquement armé et oui je parirais sans trop flipper que des producteurs de doutes ont lu cette étude et ont compris que le climatopatalisme poussait à l'inaction personne ne sait vraiment ce qu'est l'entropie et visiblement certains abusent de de cette mécompréhension en utilisant ce terme scientifique pour feutrer des discours poétiques voir des discours politiques en invoquant une limite physique dont nous sommes en fait extrêmement loin plutôt que de parler des limitations technologiques économiques et politiques concrètes qui elles contraignent vraiment le champ des possibles aujourd'hui mais qui n'ont rien de fondamental et indépassable aujourd'hui je vais essayer de clarifier au mieux ce que les scientifiques comprennne vraiment de l'entropie pour mieux évaluer ce qu'il est raisonnable de dire de l'entropie ce qui est beaucoup plus discutable et ceux qui relèv de discours trompeurs voire dangereux pour la mobilisation contre le changement climatique et plutôt que de suivre le tortueux chemin de l'histoire des sciences pour en arriver à l'état de l'art de la compréhension de l'entropie thermodynamique je vais suivre les pas de Shannon et prendre la voix informationnelle ça permet quand même de voir des trucs et oui on va voir que en un sens un peu grossier ce qu'on appelle problème énergétique est en fait en bonne partie une histoire d'information même si c'est une information un peu particère comme on va voir quelle est d'après vous la quantité d'information contenue dans le mot les comment pourrait-on quantifier cette information intuitivement même si ce mot n'a que deux lettres c'est un mot très informatif puisqu'il permet d'identifier un objet à savoir moi parmi l'ensemble de tous les objets de tout l'univers c'est quand même pas mal mais alors est-ce que l'information dans le mot les est grande l'idée de génie de Claude Shannon c'est de supposer qu'information et incertitude doivent être les deux faces d'E d'un même pièce plus précisément selon Shannon l'information d'un message c'est la quantité d'incertitude que la lecture de ce message fait disparaître autrement dit si un message était très attendu bah il est peu informatif hein quoi un nouveau scandale impliquant Elon Musk j'aurais pu le deviner ce n'est pas très informatif dit autrement plus je suis surpris par le message plus il contient d'informations nouvelles pour moi et plus je risque alors de devoir changer d'avis et d'ailleurs cette attention à la surprise je vous en avais déjà a parlé tout au début de cette série sur le baisianisme en l'an 1 avant le covid puisqu'elle est essentielle pour mettre à jour correctement notre état de connaissance du monde l'information est intimement liée à la surprise mais alors à quel point le mot lit est-il surprenant et bien comme on l'a vu encore et encore dans cette série sur le bayisanisme la réponse à cette question est nécessairement contextuelle si le mot les vient juste après le créateur de sence for hall dont le prénom est trois petits points alors vous qui suivez cette chaîne depuis un bon moment serez sans doute cunement surpris par le mot lit à l'inverse si vous entendez le mot lit lors d'une réunion de travail dans votre entreprise et si votre entreprise ne travaille pas dans la sécurité des intelligences artificielles votre surprise sera probablement plus grande et ce mot sera plus informatif on ve dire que si c'est le cas elle est quand même cool votre entreprise sans conteste sans contexte c'est la mauvaise probabilité conteste comme le dit le proverbe baahisien formellement l'information dans le mot l a été défini par Shannon comme le logarithme en base 2 de l'inverse de la probabilité du mot L bon ça peut paraître un peu compliqué et je vais pas rentrer dans les détails de tout ceci je l'ai fait dans un live que je vous invite à le voir un live que j'ai fait pendant le covid mais passons cette formule garantit que l'information augmente avec la surprise qui plus est le logarithme est utile pour garantir que conformément à l'intuition l'information de deux messages indépendants sera la somme des informations des deux messages enfin Shannon a choisi la base 2 car cela permet de lier directement cette information au nombre de bits nécessaires pour encoder le message mais c'est détail porte peu pour aujourd'hui ok ça c'est l'information d'un message et d'Urs techniquement c'est pas tout à fait une définition extrêmement standard mais je la trouve personnellement extrêmement pédagogiquement utile et assez conforme avec l'intuition qu'on pourra avoir de l'information maintenant qu'est-ce que l'entropie et bien l'entropie de Shannon c'est la quantité d'information que je m'attends à recevoir ce qu'on appelle dans le jargon l'espérance de l'information et qui est donné explicitement par la somme des quantités d'informations des messages que je peux recevoir pondéré par la probabilité de ces message c'est simple hein c'est juste des matths vous dit autrement en utilisant la dualité information incertitude l'entropie mesure l'incertitude qu'on a sur les messages qu'on va recevoir par exemple la position politique qu'adopteront les journalistes de C news et de Blast sur le sujet de l'immigration la semaine prochaine aura peu d'entropie pour moi car j'ai très peu d'incertitude sur l'angle journalistique qui sera choisi par ces médias à l'inverse l'entropie sur le prochain lieu de confier armée majeur est énorme à mes yeux car ça part un peu dans tous les sens en CE ce moment petit aparté mais les fameux algorithmes génératifs comme chat GPT fonctionnent vraiment sur un principe de minimisation de l'entropie parfois appelé crossentropy pour générer des contenus crédibles sachant leur donner d'entraînement donc c'est pas juste des principes abstraits en tout cas l'entropie est l'objet au cœur de la plus grande contribution de Claude Shannon retranscrite dans son article fabuleux de 1948 intitulé mathematical theory of communication qui pour les matateux parmi vous est extrêmement lisible vraiment ce papier est un bijou non seulement mathématique mais aussi d'ingénierie de linguistique et je pense de philosophie et surtout il est extrêmement bien écrit la lecture de cet article est vraiment l'un des plus grands bouleversements intellectuels de ma vie pas loin de solomonov 2009 Turing 1950 et la place 1814 la place j'avoue je mets la place au-dessus des autres appelons Oméga l'ensemble de toutes les choses à savoir en théorie des probabilités ou en mathématique fondamentale Oméga est parfois appelé l'univers priori on dispose d'une grande incertitude sur Oméga qu'on peut appeler l'entropie d'Oméga et écrire h de Oméga maintenant imaginons qu'on s'informe appelons x ce qu'on a découvert alors l'entropie qui reste sur éga sera h de sachant x c'està-dire l'incertitude qui reste sur Oméga conditionnellement à l'information x que l'on connaît désormais on rentre pas dans les détails de tout ce que ça signifie je renvoie au live encore une fois pour en savoir plus mais sachez que c'est extrêmement standard en théorie de l'information et bien un théorème assez simple à démontrer dit que H de ω sachant X est inférieur ou égal à H de ωé et là patatra l'entropie d'Oméga en fait diminué oui vous avez bien entendu l'entropie a diminué d'ailleurs la réduction de l'entropie sera précisément égal à l'information espérée contenue dans X et là on ne parle pas d'un principe de physicien qui ne sera considéré vrai que jusqu'au jour où on se rendra compte que ce n'est pas si vrai on parle ici d'un théorème mathématique l'entropie du monde diminue et bien sûr ça ça peut paraître complètement contradictoire avec le second principe de la thermodynamique qui affirme que l'entropie ne peut qu'augmenter que se passe-t-il a-t-on enfin démontré la stupidité des physiciens qu'en pensez-vous prenez le temps de mettre pause pour bien essayer de comprendre ce qu'il se passe et je suis sûre que vous savez exactement de quoi je parle sans contestte sans contexte c'est la mauvaise probabilité conteste en particulier il ne faut pas perdre de vue que si on définit l'entropie comme une mesure d'incertitude celle-ci est nécessairement contextuelle elle dépend des informations auxquelles on a accès souvenez-vous de l'exemple du mot l pour quelqu'un qui n'a jamais entendu parler de Saint forol le mot les dans la phrase le créateur de saintfall dont le prénom est bla bla bla est très grande puisqu'il sera surpris par ce prénom mais pour ceux parmi vous qui me connaissez bien ce mot ne contient presque aucune information vous le saviez déjà l'entropie du mot lait a été réduite par vos connaissances préalables dans le jargon païsien on parlerait d'a priori ou de préjugés et clairement ça ce n'est pas du tout ce que la physique cherche à décrire l'entropie thermodynamique vise à décrire quelque chose qui ne dépend pas de l'observateur à contrario l'entropie informationnelle de Shannon est fondamentalement subjective elle dépend des informations auxquelles un sujet a accès comme le dirait la place à propos des probabilités de manière plus générale il s'agit d'une description de notre connaissance et de notre ignorance en un sens on pourrait parler d'entropie entropique puisqu'elle se réfère à une personne qu'on peut généralement supposer humaine et je parle d'entropie entropique parce que enfin surtout ça me fait marrer cette expression entropie entropique mais elle est un peu flatteur pour les humains oui parce que en fait le concept d'entropie de Shannon ne devrait s'appliquer rigoureusement à un sujet que si ce sujet est en fait baahésien dans le sens où il est capable d'appliquer constamment les lois des probabilités dit autrement il ne s'applique en fait vraiment qu' a une forme de super intelligence qui dans mon livre la formule du savoir est appelée le démon de solomonov le démon de solomonov est cette entité imaginaire qui collecte cumulativement de l'information et a la faculté d'appliquer systématiquement la formule de base pour sans cesse améliorer son modèle du monde pour ce démon l'entropie est une fonction décroissante du temps par contre en pratique l'entropie entropique c'est-à-dire l'incertitude de nous autres Homo sapiens celle-ci bah elle peut augmenter parce que nous autres humains on a tendance à perdre la mémoire ou on a des bi cognitif qui font qu'on raisonne mal sur le monde et qu'on peut être surpris par des choses qu'en fait notre version du passé savait déjà ok maintenant qu'on a compris un peu l'entropie de Shannon qu'est-ce donc que l'entropie thermodynamique a-t-elle vraiment quelque chose à voir avec la théorie de l'information de Shannon pourquoi von nean conseillait-il à Shannon d'appeler sa mesure d'information entropie et si l'entropie thermodynamique est bien une forme d'entropie de Shannon de quel sujet dépend-elle et ben je sais ce que vous en pensez ça risque d'être compliqué alors c'est là qu'on va rentrer dans une zone grise puisqu'on est encore très loin aujourd'hui d'avoir un consensus sur la définition de l'entropie thermodynamique pour des systèmes qui ne sont pas ultra simplifiés comme des gaz parfaits à l'équilibre thermodynamique et bon ça ne dérange pas forcément les ingénieurs qui s'intéressent avant tout à des variations d'entropie thermodynamique lors de processus artificiel voir les chimistes les biologistes et les physiciens plus appliqués pour qui des définitions pas toujours au ard des mathématiciens suffisent en fait largement pour faire d'énormes progrès explicatifs dans leur disciplines respectives mais pour un théoricien comme moi ce manque de définition rigoureuse du mot entropie a de quoi être intrigante alors est-ce que tu aurais pas des truc intéressant à nous dire et bien dans cette vidéo je vais vous proposer une interprétation qui à ma connaissance colle parfaitement avec les théories les plus standards en tout cas clairement pour les cas les plus simples de gaz à l'équilibre thermodynamique mais l'interprétation dont je vais vous parler est en fait inhabituelle et je dirais même inédite et comme on va le voir elle va vraiment interpréter l'entropie thermodynamique comme un cas particulier de l'entropie informationnel de Shannon pour y parvenir je vais introduire un être un peu spécial que je vais appeler le démon de Gibs en référence au grand physicien Josa William Gibs reconnu au côté de James Clark Maxwell et Ludwig wolsman comme l'un des pères de la physique statistique et si je parle du démon de Gibs c'est en partie en comparaison avec d'autres démons notamment le démon de la place la place on rappelle le démon de la place c'est un être omniscient qui connaît toutes les positions et toutes les vitesses de toutes les particules de l'univers autrement dit sa connaissance de l'univers va jusqu'aux échelles microscopiques je cite le maître une intelligence qui pour un instant donné connaîtrait toutes les forces dont la nature est animée et la situation respective des êtres qui la composent si d'ailleurs elle était assez vaste pour soumettre ses données à l'analyse embrasserait dans la même formule les mouvements des plus grands corps de l'univers et ceux du plus léger atome rien ne serait incertain pour elle et l'avenir comme le passé serait présent à ses yeux à contrario le démon de Gibs c'est un être que je définis comme uniquement macroscopiquement omniscient il connaît tous les macro état c'est-à-dire les descriptions de tous les objets du monde ainsi que leur position leur vitesse leur volume leur température et leur pression mais il ne connaî pas les positions et les vitesses des particules qui composent ces objets et bien au moins dans le cas des gaz à l'équilibre thermodynamique l'entropie thermodynamique peut vraiment être défini comme l'incertitude du démon de Gibs sur les positions et les vitesses des particules malgré son omniscience macroscopique et le tout mesuré selon les équations de Shannon dit autrement l'entropie thermodynamique quantifie les informations du démon de la place auxquelles le démon de Gibs n'a pas accès elle calcule la différence de connaissance entre les deux démons quelles sont les choses à propos des particules qu'on ne peut pas savoir même une fois qu' sait tout du monde à notre échelle macroscopique voilà qui fournit une nouvelle compréhension du second principe de la thermodynamique d'un point de vue informationnel ce principe affirme désormais que le différentiel de connaissance entre les démons de la place et de Gibs ne peut que croître avec le temps au fur et à mesure que des opérations thermodynamiques irréversibles ont lieu l'incertitude du démon de Gibs sur les positions et les vitesses des particules augmentera typiquement parce que ces informations à propos d'un gaz se sont mélangé avec cell à propos d'un autre gaz le démon de Gibs deviendra alors davantage ignorant comparativement notamment au démon de la place qui lui ne perd pas ses informations même en cas de mélange gaz et là en fait je vous propose une interpréation qui est très proche des travaux de Gibs lui-même et qui me semble de sur croix être quelque chose de très profond en tout cas très pédagogiquement utile ceci dit c'est loin de faire consensus chez les physiciens pour les systèmes complexes hors équilibre donc prenez tout cela avec quelques pincettes dans le cadre général cependant encore une fois pour les compositions de gaz à l'équilibre ce que je vous ai présenté ici c'est vraiment uniquement une reformulation informationnelle de principe d'une physique statistique déjà ultra consensuelle et on va la détailler pour bien comprendre l'entropie thermodynamique et donc l'incertitude du démon de Gibs je vous propose de calculer sa valeur pour un gaz parfait monoatomique dans un volume V à température T et à pression P euh une démonstration mathématique là qu'on va faire ça va pas être trop difficile à suivre ce qu'on va faire dans cette section est un peu technique donc si vous voulez la sauter bah hésitez pas les termes codes sont en description mais pour bien sentir en quoi l'entropie thermodynamique et l'entropie informationnelle du démon de Gibs et surtout pourquoi ceci aide beaucoup à cerner la nature profonde de l'entropie les calculs qu'on va faire me semblent très utiles et à ma connaissance assez inédite aussi en tout cas la page wikipédia a une démonstration classique du calcul entropie de ce gaz mais dans le cas de l'approche de Boltzman alors qu'ici je vais vous proposer une alternative beaucoup plus proche de interprétation informationnelle à la chanon qui d'ailleurs me semble plus simple allez trêve de bavardage c'est parti pour commencer un petit rappel un gaz est parfait si les particules de ce gaz n'interagissent que via des chocs en particulier les particules sont généralement trop éloignées pour qu'il y ait des effet électromagnétique ou gravitationnel entre elles et l'hypothèse selon laquelle le gaz est monoatomique revient simplement à dire que toutes les particules du gaz sont les mêmes et ressemblent à des billes en gros la loi des gaz parfaits PV = NKT nous dit alors que ce gaz contient n est égal à PV divisé par KT particule selon l'approche informationnelle de l'entropie il s'agit maintenant pour nous de quantifier notre incertitude sur les positions et les vitesses de chaque particule concentrons-nous sur une seule particule pour commencer à l'équilibre thermodynamique la particule a eu le temps de se cogner un grand nombre de fois avec les autres particules et de se perdre un peu n'importe où dans le volume si bien que l'incertitude sur sa vitesse peut-être considéré indépendante de l'incertitude sur sa position dès lors en vertu des lois des probabilités on peut calculer C séparément l'incertitude sur la position et sur la vitesse et l'incertitude totale sur notre particule sera la somme de ces deux incertitudes commençons par la position comme la particule a eu le temps de se balader un peu partout la particule peut être n'importe où dans le volume on dit que la distribution de ces positions probaes est uniformément répartie ça mathématiquement ça revient à dire que la probabilité de trouver la particule dans un petit volume DV de l'espace est égal à la taille de ce volume DV divisé par le volume total V en appliquant la formule de l'entropie de Shannon on trouve une incertitude sur la position de la particule égale à l'intégrale sur tout le volume du log en base 2 le volume fois la probabilité d'être dans la partie DV du volume et ça c'est égal à l'intégrale surtout le volume du log en base 2 de V x DV divis par V le log 2 de V divis par V peut-être ressorti de l'intégral on a juste l'intégral sur DV l'intégral sur DV c'est égal à V tout se simplifie on obtient au final seulement log en base 2 de V l'entropie informationnelle sur la position est donc le logarithme du volume c'est simple hein c'est juste des maths et ça intuitivement ça se comprend en fait très bien si on double le volume alors il suffit de poser une question binaire à savoir dans lequel des deux sous-volumes de volume V se trouve la particule pour revenir au cas précédent or cette question binaire nous fournit exactement un bit d'information puisqu'elle peut-être oui ou non avec écu probabilité l'entropie pour un volume de 2 x V est donc supérieur d'un bit au volume V avoir multiplié le volume par 2 augmente donc l'entropie d'un bit et ça c'est exactement ce que décrit la fonction logarithme en base 2 en fait juste avec ce raisonnement on aurait pu obtenir directement la formule log 2 de V pour l'incertitude sur la position de la particule voilà je préfère ça passons maintenant à la vitesse là ça va être un poil plus compliqué mais grâce à maxuell et boltsman on sait que la distribution des vitesses possibles d'une particule monoatomique est donnée par la distribution de Maxwell Boltzman qui correspond à une loi normale isotrope en dimension 3 dont l'amplitude typique selon chaque direction est donnée par racine carré de K x t divis par M où K est la constante de bolsman M est la masse de la particule et T est la température en particulier à chaque fois que la température quadruple la vitesse typique des particules selon une direction donnée va doubler en fait on aboutit rapidement à cette distribution normal avec un raisonnement purement informationnel plutôt qu'avec les arguments à base de MICR qu'on trouve dans les livres de cours sur la physique statistique après tout la loi normale est celle qui maximise l'entropie informationnelle à énergie cinétique moyenne fixée bon je vais pas trop vous ennuyer avec les détails mathématiques même s'il y a des jolies preuves derrière tout ça j'ai juste envie de vomir toujours est-il que l'entropie de cette distribution est bien connue et elle est égale à 3/i X log en base 2 de la température + 3/2i x log en base 2 de Tau X e x k divis par M où cette fois tau est le rapport de la circonférence du cercle par son rayon qui est égal à 6,28 que certains parfois appelle 2 pi mais ça s'appelle tau alors j'ai séparé ça en deux termes parce que le deuxième terme en fait est une constante si on fait varier en particulier le volume ou la température bah ça va pas changer ce second terme du coup on va considérer que c'est une constante et qu'on va pouvoir du coup l'ignorer puisqu'on va s'intéresser pas mal notamment aux variations d'entropie là encore au-delà du calcul précis on peut s'en faire une intuition quand on quadruple la température on double l'amplitude typique de leur vitesse selon chaque direction ce qui augmente d'un bit l'incertitude sur la vitesse selon une direction donnée et comme il y a trois dimensions d'espace il faut multiplier par trois ces incertitudes ce qui conduit à ajouter trois bit d'entropie donc multiplier la température par 4 augmente l'entropie de 3 bit et ça bah il y a une seule fonction qui permet de faire ça c'est la fonction 3 log en base 4 de T qui est égal à 3/2 log en base 2 de T c'est limpide non notez que pour les gaz qui ne sont pas monoatomiques comme par exemple les gaz diatomiques que sont le diazote et le dioxygène il faudra aussi considérer d'autres sources d'incertitude sur la répartition de l'énergie cinétique dans une particule comme à travers les rotations des particules sur elles-même ou à travers des vibrations entre les atomes atom qui composent ces particules et ça ça va typiquement augmenter les effets de la température qui augmenteront davantage encore l'incertitude pour le démon de Gibs mathématiquement ça va conduire à une dépendance légèrement différente en la température au lieu d'avoir 3 demi de log en base 2 de t on pourra avoir 52i de log en base 2 de T si l'information sur l'énergie cinétique sera partie sur cinq dimensions comme par exemple TR de translation et de de rotation au lieu des trois comme c'est le cas pour les particules monoatomiques dans le jargon on parle de degré de liberté et ce nombre de degré de liberté peut typiquement augmenter ou diminuer en fonction des symétries des particules ah là le le plus compliqué c'est d'arriver à suivre le rythme en tout cas dans le cas monoatomique l'entropie pour une particule est h est égal à log en base 2 de V + 3/2 de log en base 2 de T plus une constante pour obtenir l'entropie informationnelle pour toutes les particules on pourrait vouloir simplement ajouter les entropies de chaque particule mais il y a un petit détail dont il faut tenir compte et qui va nous permettre d'éviter le paradoxe de Gibs on va considérer les particules ind cernable même aux yeux du démon de la place autrement dit si j'échange deux particules alors pour le démon de la place l'état physique est en fait le même l'univers n'a pas changé voilà qui revient à considérer que les permutations des particules n'augmentent pas l'incertitude et ça mathématiquement ça revient à retirer à la somme des entropies des particules le logarithme du nombre de permutation de N particules qui est égal à log en base 2 de N factoriel alors là encore je sauté quelques détails mathématiques mais d'après la formule de Sterling ce terme est asymptétiquement égal à environ n log en base 2 de N- n ce terme peut paraître un notant mais c'est vraiment lui qui garantit l'extensivité de l'entropie thermodynamique c'est-à-dire le fait que l'entropie de deux systèmes disjoints et la somme des entropies de chaque système potentiellement c'est plus pratique on obtient alors l'entropie informationnelle des particules du gaz aux yeux du démon de Gibs qui est donné par H est égal à n fois le log en base 2 de V donc l'incertitude sur les position de toutes les particules + N - N x log 2 de N donc ça ça correspond aux permutations + 3n / 2 log en base 2 de T donc ça ça correspond aux incertitudes sur les vitesses des particules plus une constante et tout ça l'unité ce sont je sais pas si vous avez suivi l'unité c'est le BIT en utilisant les propriétés du logarithme on peut réécrire cette expression sous la forme h est égal à n x log en base 2 de V / n + 3n/ 2 log en base 2 de t + N + constante tout ça encore une fois en bit c'est simple hein c'est juste des maths écrit comme ça comme v/is par N est une constante pour deux gaz identiques on voit bien que l'entropie de deux systèmes identiques va être la somme des entropies de chaque système pour obtenir maintenant des joules par Kelvin c'est-à-dire avoir les unité standard du système international il nous faut multiplier tout ça par K x ln 2 où ln c'est le logarithme npérin je peux pas rentrer dans trop de détail en tout cas ça nous donne s est égal à kn x log v/ n + 3 kn/ 2 log de t + kn plus constante le tout en joules par Kelvin et bien ça c'est exactement la formule de la variation de l'entropie thermodynamique qui a été conçu pour les gaz parfaits monoatomiques sans passer par l'entropie informationnelle du démon de Gibs bah oui ou plutôt vous trouverez dans le manuel pour un gaz qui passe d'un volume et d'une température v1t1 à un volume et une température v2t2 de manière adiabatique variation d'entropie d'après les cours de physique thermodynamique sera de la forme Delta s est égal à CV log de t2/ T1 + NR log de v2/ V1 ou V est égal à 3 demi de NK pour les gazes monoatomiques et petit NR est égal à grand N x k on voit bien que notre entropie informationnelle du démon de Gibs coïncide parfaitement avec les équations de l'entropie thermodynamique classique d'accord pourquoi pas mais et alors et donc l'interprétation que je donne ici n'est pas du tout une spéculation pour les gaz parfaits en fait c'est simplement une reformulation qui me semble en fait beaucoup plus profonde que l'approche non atomique de la thermodynamique type closus est beaucoup plus élégante et limpide que l'approche en nombre de micro pro état souvent présenté par les cours de physique statistique ces cours de physique statistique privilégient souvent le chemin plus tortueux de l'histoire des sciences avec closus et ou bolzman pour information la page Wikipédia anglophone sur le paradoxe de Gibs et ce que j'ai lu qui se rapproche le plus de ce que j'ai présenté mais même cette page fait un moment appel à des arguments que je trouve moins élégants sur l'incertitude sur énergie totale l'interprétation informationnelle que je donne me semble non seulement plus direct plus simple mais surtout elle me semble beaucoup plus profonde en sens où elle va nous permettre de généraliser plus facilement au-delà du cadre restreint des gaz parfaits dans tous les mécanismes d'irréversibilité il semble y avoir une mécanique de la sorte en jeu de l'information semble échapper au démon de Gibs elle semble échapper au monde macroscopique au moment où elle passe au monde microscopique elle devient de l'incertitude sur les positions des vitesses et des positions des particules et cette perte d'information du démon de Gibs semble irréversible il semble impossible pour cette information de redevenir macroscopique connu du démon de Gibs si ça vous paraît étrange comme affirmation maintenant sachez qu'on y reviendra un peu plus tard dans cette vidéo au moment de parler du principe de landower malgré les énormes efforts de Gibs pour combler les manquements de sa théorie le démon de Gibs reste néanmoins nettement moins bien défini que ceux de la place et de solomonov en particulier sa description dépend de la distinction entre le monde macroscopique et le monde microscopique et ce n'est pas là une limite de mon analogie je parle là d'un problème fondamental au cœur de la thermodynamique statistique qui dépend d'une distinction entre les macro États et les micro états s'il est clair que les micro états décrivent les positions et les vitesses de toutes les particules il n'est pas tout à fait clair ce que sont les macro états intuitivement il s'agit des descriptions à base de position et de vitesse des groses objets ainsi que de leur pression de leur température de leur volume de leur état physique de leur champ électromagnétique et de leur composition chimique mais qu'est-ce qu'un gros objet le gros trou parle-on de l'échelle du milligram du nanogramme combien faut-il de particules ou de volume d'espace pour que l'objet soit macroscopique est par exemple à la dimension de colmogorof dont parle cet excellente vidéo de TH BL Brown est-ce qu'un atome qu'on manipulerait individuellement et dont on connaît assez précisément la position et la vitesse forme un objet macroscopique et en quoi une telle distinction d'échelle serait-elle objective au point d'impliquer quelque chose qui paraît aussi fondamental et indépendant de tout observateur que le second principe de la thermodynamique pour les gaz parfaits à l'équilibre thermodynamique cette distinction micro macro est bien défin et bien comprise cependant pour les systèmes complexes hors équilibre et qui ne sont pas juste des gaz ou qui sont des gazes mais en situation turbulente déterminer le seuil de la transition micro macro est beaucoup moins évident clairement plus le seuil de connaissance macroscopique est petit plus le démon de Gibs aura une connaissance fine du monde et plus l'entropie thermodynamique c'est-à-dire ce que ce démon de Gibs ne sait pas à propos du monde microscopique sera faible l'entropie thermodynamique dépend ainsi fondamentalement de la granularité du monde macroscopique et de tout ce que j'ai lu bien définir les contours de la frontière entre les États microscopiques et les états macroscopique reste une limite des connaissances actuelles de la recherche en thermodynamique statistique surtout pour les systèmes hors équilibre les physiciens ne savent pas encore vraiment comment définir l'entropie encore plein de travail pour les chercheurs et même pour les systèmes à l'équilibre lorsqu'ils sont trop distincts des gaz la thermodynamique conduit à des mesures cocasses comme ce gaz quantique dont la température est est en dessous du zéro absolu oui vous avez bien entendu non seulement les physiciens ont déterminé une température minimale appelée le zéro absolu qui se trouve à - 273,15° celus et qui intuitivement correspond à l'absence d'énergie cinétique des particules et ils ont aussi montré qu'il était possible d'atteindre des températures en dessous de ce zéro comment est-ce possible et bien c'est parce que la définition moderne de la température dépend en fait de l'entrthopie thermodynamique notamment pour éviter des difficultés liées au nombre de degrés de liberté des particules également parce que pour certains systèmes physique très particulier cette entropie se comporte très différemment de celle des gaz il peut arriver en particulier qu'ajouter de l'énergie dans un système réduise l'incertitude du démon de Gibs sur les états microscopiques bref l'entrthropie thmodynamique dès qu'on ne parle pas de combinaison de gaz à l'équilibre thermodynamique c'est très loin d'être simple d'ailleurs selon le physicien historien de la physique Martin Klein c'est Gibs a procrastiné pendant plus d'une décennie la publication de son ouvrage fondateur de la physique statistique c'est probablement à cause de son incapacité à répondre à la question quelle est l'entropie d'un système hors équilibre une question absolument fascinante donc je suis personnellement extrêmement loin d'avoir un Yota de réponse satisfaisante alors quelques petites notes historicoterminologiques que vous pouvez complètement ignorer si les méandres de l'histoire des sciences ne vous intéressent pas encore une fois les TH codes pour la section suivante sont en description pour commencer l'entropie thermodynamique n'a pas été définie historique cement de manière informationnelle ma présentation est extrêmement anachronique mais je pense plus pédagogiquement efficace pour comprendre l'entropie on cherche l'efficacité on remonte son origine à Joseph fourriier sad Carna puis à rudolp closus qui faisait littéralement de la thermodynamique c'est-à-dire ils étudiaent les mouvements de chaleur et en gros ces chercheurs ont constaté que la chaleur allait du chaud vers le froid et que ce phénomène pouvait se calculer avec des grandeurs comme la température la pression et le volume et d'ailleurs je vous recommande vivement les excellentes vidéos de TH BR one Brown qui illustre très bien l'irréversibilité des équations de la chaleur de Fourier qui je trouve et par ailleurs trop souvent auise dans les introductions à la thermodynamique et au second principe y compris sur les pages Wikipédia francophone et anglophone sur la thermodynamique alors que thermodynamique signifie littéralement mouvement de la chaleur je dire l'équation de la chaleur répond pas mal à cette question en particulier dans les équations de Fourier l'augmentation de l'entropie thermodynamique est extrêmement liée a une fonction des variations de température qu'on appelle le laplacien Laplace par ailleurs pendant longtemps et aujourd'hui encore on parle davantage d'entropie statistique que d'entropie informationnelle ainsi en physique statistique en suivant les pas de Maxwell et surtout bolsman on va davantage avoir tendance à compter les nombres de micro états possibles et à considérer qu'ils sont équis probables dès lors l'entropie est définie comme le logarithme du nombre W de microétats mais en fait dire que tous ces microétats sont éc probables c'est et leur assigné une probabilité 1/ W en appliquant la formule de Shannon on retrouve h est égal à la somme des 1/ W log de 1/ par 1/ W et tout ça c'est égal à log de W je vous laisse faire les calculs en fait c'est Gibs en 1902 et donc un domiiècle avant Shannon qui comprit que l'approche de bolzman était le cas particulier d'une description probabiliste le génie de Shannon est peut-être davantage d'avoir superbement relié tout cela à l'information et en particulier à la notion de codage mais à bien y réfléchir ce lien entre entropie statistique et informationnel n'est pas si étonnant plus il y a de microétat poss possible aux yeux du démon de Gibs moins il sait lequel est le bon en devinant au hasard il n'a qu'une chance sur W de tomber juste son incertitude est plus grande quand W est grand c'est limpide non enfin j'ai surtout envie de dire que tout ce bon monde n'est surtout qu'une descendance d'un illustre précurseur qui avait peut-être tout anticipé avant tout le monde et qui est la personne de l'histoire à qui j'aimerais tant parler de l'interprétation informationnelle des flux de chaleur un homme qui précède closus Carno et Fourier j'ai nommé le grand l'immense le merveilleux Pierre Simon non seulement a-t-il lui-même contribué à la thermodynamique à travers notamment la loi de la place pour les gaz parfaits il a aussi pleinement adopté l'hypothèse atomiste pourtant encore contraversé jusqu'à l'article d'Albert Einstein de 1905 sur le mouvement bronien un siècle plus tard un article qui aura également succédé aux travaux atomistes de Maxwell Boltzman et Gibs l'interprétation atomiste de l'entropie elle trouve un peu son origine chez la la place mais surtout toute mon interprétation informationnelle repose sur la définition de la probabilité comme une description de l'incertitude qu'on retrouve déjà dans le mémoire de la place de 1774 mais que le grand officier de la Légion d'Honneur français a beaucoup clarifié dans son extraordinaire essai philosophique sur les probabilités de 1814 je cite la courbe décrite par une simple molécule d'air ou de vapeur est réglé d'une manière aussi certaine que les orbites planétaires il n'y a de différence entre elles que celles qui met not ignorance la probabilité est relative en partie à cette ignorance en partie à nos connaissances en fait pour tout vous dire je suis avant tout surpris qu'il m'ait fallu écrire cette vidéo pour me rendre compte de l'influence monumentale du personnage sur la thermodynamique allez dites-le avec moi autre détail historicoterminologique le terme démon de Gibs je l'ai vraiment inventé pour cette vidéo initialement j'étais tenté d'appeler ce démon le démon de bolsman car l'interprétation Mac versus micro état est souvent beaucoup plus associé à bolzman malheureusement la notion de démon de bolzman est déjà utilisée pour décrire un argument de mélange aléatoire dans les équations de Boltzman plus précisément dans l'argument de boltsman la perte d'information surtout sur les vitesses surgit vraiment des collisions entre les particules intuitivement ces particules vont avoir tendance à homogénéiser leur vitesse respective typiquement au billard l'énergie cinétique de la boule blanche va se répartir à toutes les boules bon ce n'est pas toujours vrai puisque les équations de la physique sont réversibles et qui qu'il est en principe possible que les collisions mettent toutes les autres boules à l'arrêt de sorte à concentrer l'énergie cinétique dans la boule blanche mais ce second scénario semble intuitivement extrêmement improbable pour formuler cette uniformisation de manière mathématique bolzman a supposé que la prochaine collision pouvait être décrite comme celle entre deux particules tirées au hasard c'est là qu'il y a un tour de pass-p dans l'argument de boltman c'est rarement bon signe et bien d'autres chercheurs ont souligné cette hypothèse discutable en introduisant un démon de bolzman qui choisit aléatoirement les prochaines particules qui rentreront en collision ça pour dire que le démon de bolzman est déjà pris ceci dit démon de Gibs bah c'est pas si mal puisque Gibs a énormément contribué à consolider les idées parfois floues de bolsman selon Henry pancaré celui qui a vu le plus clairement l'irréversibilité de processus physiquees macroscopi en terme probabiliste dans un livre trop peu lu car un peu difficile à lire c'est Gibs dans ses principes élémentaires de mécanique statistique d'ailleurs enseigné à Yelle University un cours intitulé sur la déduction a priori des principes de la thermodynamique de la théorie des probabilité et oui selon Gibs la thermodynamique ne sera qu'une conséquence des lois des probabilités je nuancerai un peu cela mais ça reste une perspective déficiente aujourd'hui et qui vient de l'un des plus grands expés de l'entropie de l'histoire cette citation de Gibs n'est en tout cas pas s'en rappeler la conclusion de l'essai philosophique sur les probabilités de la place qui écrivait je cite la théorie des probabilités de devient le supplément le plus heureux à l'ignorance et à la faiblesse de l'esprit humain si on considère les méthodes analytiques auxquelles cette théorie a donné naissance la vérité des principes qui lui servent de base la logique fine et délicate qui exige leur emploi dans la solution des problèmes les établissements d'utilité publique qui s'appuie sur elle et l'extension qu'elle a reçu et qu'elle peut recevoir encore par son application aux questions les plus importantes de la philosophie naturelle et des sciences morales si l'on considère ensuite que dans les choses même qui ne peuvent pas être soumises au calcul elle donne les aperçus les plus sûres qui puissent nous guider dans nos jugements et qu'elle apprend à se garantir des illusions qui souvent nous égar on verra qu'il n'est point de science plus digne de nos méditations et qu'il soit plus utile de faire entrer dans le système de l'instruction publique j'ai rien à rajouter incroyable incroyable plusieurs décennies avant la publication de l'ouvrage de Gibbs en 1867 le génie écossais James Clerk Maxwell proposa une expérience de pensée pour montrer en quoi l'information microscopique est au cœur de la thermodynamique c'est-à-dire de la dynamique des échange de chaleur son expérience de pensée précède d'ailleurs les travaux de bolzman et ils ont certainement aidé bolzman puis Gibs à formaliser l'entropie thermodynamique en tant qu'incertitude microscopique l'expérience de pensée de Maxwell fait intervenir un être puissant qu'il est désormais de coutume d'appeler le démon de Maxwell et oui il y a beaucoup de démons dans ces histoires de physique statistique de façon remarquable en ne faisant que traiter de l'information ce démon était capable de violer le second principe de la thermodynamique en faisant baisser l'entropie thermodynamique au sens classique non informationnel de closius je suis perplexe pour comprendre cela imaginez deux boîtes complètement isolantes avec une trappe qui les sépare imaginez que initialement la boîte de gauche contient de l'air froid alors que la boîte de droite a de l'air chaud en M 855 closus fil la remarque que conformément aux équations de la chaleur de Fourier lorsque la trappe était ouverte la chaleur allait nécessairement se déplacer du chaud vers le froid de sorte que les températures des deux boîtes finissent par s'homogénéiser inspiré par Carno closus mathématisa cela en définissant l'entropie comme une grandeur additive qui augmentait nécessairement avec le temps la remarque de Maxwell toutefois c'est que si on considère que le gaz est composé de particules et que la température est en gros la vitesse moyenne de ces particules alors on peut en fait réduire l'entropie en faisant simplement un tri en gros ouvrir et fermer la trappe de sorte à mettre les particules rapides à droite et les particules lentes à gauche plus précisément Maxwell imagine un démon de Maxwell capable d'ouvrir la trappe lorsqu'une particule rapide va de gauche à droite et de la fermer quand elle va de droite à gauche de même il ouvre la trappe quand une particule lente va de droite à gauche et il la ferme si une particule lente va de gauche à droite ainsi les particules à haute énergie cinétique vont se retrouver à droite tandis que les particules à faible énergie vont se retrouver à gauche ce ce faisant la différence de température entre les boîtes va augmenter ou dit autrement l'entropie va diminuer en ouvrant et en fermant la trappe ce qui est clairement une opération réversible mais en faisant cela au bon moment le démon de Maxwell pouvait violer le second principe de la thermodynamique en particulier ainsi il pouvait transformer de l'énergie thermique inutilisable en une différence de chaleur qui peut être utilisée pour extraire du travail dit autrement la simple exploitation informationnelle microscopique suffit à créer une capacité de travail et donc à résoudre les problèmes d'énergie dans nos sociétés en fait tous les mots énergétiques de nos sociétés ne sont pas des pénuries d'énergie il s'agit avant tout d'une incapacité à exploiter ces informations microscopiques comme le démon de Maxwell sait le faire d'ailleurs le démon de Maxwell peut insi non seulement créer un différentiel de température mais aussi un différentiel de pression en mettant notamment toutes les particules d'un seul côté comme le professeur almadi M Hamdi et moi en parlons dans notre livre Le fable chantier dans le chapitre 4 vraiment il y a quelque chose de profond dans notre traitement des informations microscopiques en 1960 le physicien Rolf Landauer poussa la réflexion encore plus loin avec une interprétation qui fait un pont entre une information cédée au monde microscopique et l'énergie dissipée en énergie thermique non récupérable et oui on va parler de Landauer qui a gagné le droit d'être le nom du groupe signad où rodolp Jean-Lou et moi avons échangé pour préparer nos vidéos sur l'entropie pour comprendre lundauer il suffit de considérer le problème des deux boîtes avec cette fois une seule particule dont on sait qu'elle se trouve à gauche ou plus précisément on va supposer que le fait que la particule se trouve à gauche est une information macroscopique et donc connu du démon de Gibs en ouvrant la trappe la particule va maintenant naviguer de gauche à droite et de droite à gauche de sorte Queau bout d'un moment l'information sur la boîte qui contient la particule sera inconnue en tout cas dans le monde macroscopique cette information sera perdue par le démon de Gibs en particulier à la question la particule se trouve-t-elle à gauche le démon de Gibs doit maintenant répondre il y a une probabilité ind demi que ce soit le cas en supposant que les deux boîtes sont macroscopiquement indiscernables l'information avant ouverture de la trappe correspond à une information au sens de Shannon et relativement au démon de Gibs égale au logarithme en base 2 de l'inverse de 12i tout ça est égal à 1 bit en ouvrant la trappe le démon de Gibs a perdu un bit d'information ou dit autrement l'entropie thermodynamique a augmenté d'un bit landawer montra que cette description informationnelle est équivalent à dire que l'entrthropie thermodynamique a augmenté de kn de 2 joules par Kelvin on avait déjà vu cette conversion de 1 bit en kn2 joule par Kelvin de plus si l'expérience a lieu à température ambiante t alors cette augmentation de l'entropie correspondra à une dissipation de KT ln 2 joules aujourd'hui ce principe dit de Landauer est souvent exprimé sous la forme d'un coûp énergétique à l'effacement de l'information pour être plus précis toutefois il faut bien voir qu'il s'agit d'une destruction d'une information macroscopique uniquement c'està-dire une destruction de l'informationnelle pour l'observateur qu'est le démon de Gibs mais l'information sur la particule à l'échelle microscopique aux yeux du démon de la place n'a pas été détruite et alors en pratique nos machines effectuent en effet à foison de telles destructions d'information macroscopique puisqu'elle repose énormément sur des transistors qui prennent deux informations en entrée et qui en sortent une seule par exemple si le transistor encode une porte logique end et s'il reçoit un 1 et un zé en entrée alors il sortira zéro en sortie voilà qui est forcément une opération destructrice sachant le zéro en sortie on ne peut pas savoir quels étaient les bits en entrée et bien cette information effacée et donc perdue par le démon de Gibs est inéctablement dissipé sous forme d'énergie thermique le principe de Landauer semble alors suggérer que le traitement de l'information nécessite nécessairement un coût énergétique et bon en pratique de toute façon il reste une marge énorme de progression dans l'efficacité des calculateurs d'aujourd'hui qui est d'ailleurs très aligné avec les incitatifs économiques en fait même si on prolonge notre optimisation exponentielle de l'efficacité en coût énergétique des machines connu sous le nom de loi de Kouet on a en fait encore jusqu'en 2080 pour atteindre la limite de Lander à ce moment-là un calcul coûtera 5 millions de fois moins d'énergie qu'aujourd'hui en dissipation d'énergie sous forme thermique ça serait top mais surtout le principe de landawer ne concerne en fait uniquement que l'effacement de l'information macroscopique or effacé de l'information on conviendra qu'il s'agit d'une opération assez particulière parmi l'ensemble de toutes les opérations utiles au traitement de l'information par exemple s'il s'agit de trier de l'information du web pour recommander uniquement les plus pertinentes comme cherche à le faire tourol notamment alors en principe ces opérations ne nécessitent pas de destruction informationnelle il est donc possible en principe de faire tournesol sans dissipation d'énergie en énergie thermique oui c'est possible même si je vous l'accorde aujourd'hui on na vraiment pas les technologies nécessaires pour y arriver ceci dit il y a tout un champ scientifique entre la physique et l'informatique qui étudie le calcul TER thermodynamiquement réversible c'est-à-dire l'ensemble des opérations informationnelles sans augmentation d'entropie thermodynamique ainsi en 1973 le physicien Charles Bennett a démontré l'existence de machine de Turing réversible et donc que tout calcul pouvait en principe être rendu réversible et en gros il suffit de ne jamais effacer de données pendant le calcul or comme tout thermodynamicien vous le dira si une opération est réversible c'est qu'elle n'augmente pas l'entropie thermodynamique bien entendu une absence totale de pert par dissipation d'énergie est technologiquement impossible mais en principe il n'y a pas de limite inférieure à cette perte et en pratique si aujourd'hui il n'y a pas plus de technologie de calcul physiquement réversible c'est avant tout parce que cette technologie est en compétition avec d'autres bien plus efficaces selon des métriques autres que le coût en augmentation d'entropie thermodynamique comme par exemple le nombre d'opérations par seconde ou le coût de fabrication à moins que petite aparté sur la mécanique quantique car je sens qu'il y en a qui vont faire le lien avec le hasard de l'interprétation de copenhag de la mécanique quantique en bref il y a plusieurs interprétations complètement déterministes de la mécanique quantique et donc parfaitement conforme avec l'existence d'un démon de la place et selon lesquels il n'y a pas de perte d'information microscopique et donc non a priori la mécanique quantique ne viole pas nécessairement ces principes thermodynamique je suis sûr que je vais avoir plein de commentaires néanmoins la recherche dans ce domaine progresse et en 2020 des chercheurs ont annoncé avoir enfin réussi la fabrication de portes logiques thermodynamiquement réversible qui vont typiquement avoir des sorties qui permettent de reconstruire les entrées bon par contre je suis très loin de connaître suffisamment le domaine pour fournir un avis pertinent sur la validité de leurs affirmations et pour vous dire si ça va changer quoi que ce soit à l'informatique et à son impact sur l'entropie thermodynamique dans les décennies à venir encore plein de travail pour les chercheurs bien entendu en pratique de facto nos machines à traiter de l'information aujourd'hui dissipent énormément d'énergie en énergie thermique ce qui pousse certains à affirmer que le numérique serait impossible sans l'abondance énergétique mais première cette énergie dissipée qui est de l'ordre de 56 TWh par an en France reste beaucoup plus faible que dans le cas du transport et du chauffage et notamment en France elle est beaucoup plus décarbonée l'urgence écologique ne réside donc pas aujourd'hui dans les émissions directes du numérique comme je vous en parle ici il me semble qu'elle réside en fait beaucoup plus dans ce que le numérique fait de l'information sur l'urgence écologique et ce qu'elle fait de l'information qui pousse à la surconsommation quand on s'intéresse aux émissions des jets privés on ne doit pas seulement s'intéresser à à leurs émissions directes ni aux émissions indirectes dues à leur production et leur démantelllement ce qu'on appelle généralement le cycle de vie mais aussi à toutes les émissions indirectes du à la façon dont les gens vont changer ou pas leur comportement en apprenant qu'on laisse les jets privés voler super ça deuxièmement réduire les problèmes d'information à cette dissipation d'énergie c'est précisément faire du réductionnisme énergétique très malencontreux car il emet toutes les préoccupations majeures à avoir dans le traitement et la diffusion actuelle de l'information qui échoue complètement à être décrite par ce réductionisme énergétique comme par exemple la désinformation de masse et le cyberharcèlement et le déclin des démocraties c'est amusant que dans une vidéo où j'explique que Monsieur janovi tombe dans ce travers j'ai énormément de commentaires de ces fans qui bah justement illustre ce travers je suis sûr que je vais avoir plein de commentaires et enfin troisièmement on peut imaginer des dispositifs notamment à base de Réau de chaleur pour que la dissipation thermique du calcul ne soit pas complètement perdue en parlant justement de l'impact environnment mental du calcul je vous recommande vivement cette excellente vidéo de Monsieur bidouille à propos de l'infrastructure d'infomaniaque qui est le sponsor de cette vidéo infomaniaque fournit de nombreuses solutions Cloud que l'association tournosol utilise de la loocation de serveur au client mail en passant par sus transfer la solution de visioconférence camit ou encore cadrive une alternative à Google Drive qui respecte la vie privée et qui est entièrement développée et hébergée en Suisse contrairement aux alternatives des géant de la tech comme Google et Amazon infomaniaque n'est pas soumise au Patriot Act et à la loi d'espionnage visa et vous protège ainsi d'éventuels espionnag par les autorités américaines infomaniaque est aussi très investi dans la souveraineté de sa solution avec un développement exclusivement en Suisse et dans la maîtrise de son impact environnemental comme l'explique monsieur bidouille l'entreprise a mise en place un système pour que toute l'énergie utilisé par son nouveau data center soit réutilisé pour chauffer 6000 ménages à l'année via un réseau de chaleur de sorte que l'augmentation de l'entropie thermodynamique par les calculs d'infomaniaque ne soit pas maximal et oui tant qu'elle n'est pas à la température que l'atmosphère l'énergie thermique reste utile et loin du maximum d'entropie pour chaque vidéo où je les promeux ce que je fais avec plaisir infomaniaque reverse un don à l'association tournesol Merci beaucoup à eux de soutenir notre projet non lucratif pour vulgariser l'entropie il est courant de la définir comme étant une mesure du désordre ou du chaos prenons par exemple l'exemple d'une chambre mal rangée ou d'un mouvement social ces analogies peuvent être utiles en première approximation pour avoir une intuition du concept après tout si une chambre est mal rangée c'est qu'il y a sans doute beaucoup d'incertitude sur les positions des chaussettes dans la chambre pour un observateur humain on peut ainsi dire que l'entropie au sens de Shannon de la chambre mal rangée sera plus grande que celle d'une chambre bien rangée avec des étiquettes un peu partout cependant j'espère qu'après cette vidéo vous aurez pris le réflex de ne pas confondre cette entropie de l'observateur humain avec l'entropie thermodynamique qui est vu selon les lentilles du démon de Gibs ça se ressemble donc à la fin bah on fait plus trop la différence ne l'oubliez pas le démon de Gibs est omniscient vis-à-vis de l'information macroscopique or les chaussettes sont des objets macroscopiques le démon de Gibs n'a donc clairement aucune incertitude vis-à-vis de leur position en fait il n'est pas clair que pour le démon de Gibs l'entropie de la chambre mal rangée est plus grande que celle de la chambre bien rangée tout dépend vraiment de l'incertitude sur les échelles microscopiques de même ce n'est pas en triant des pommes que vous diminuerez l'entropie thermodynamique encore une fois le tri de pomme est une opération qui peut être eff effectué de manière réversible ce tri ne peut pas avoir réduit l'entropie thermodynamique en fait il faut bien voir que l'entrthropie thermodynamique ne mesure pas de façon générale le chaos pour être moins erroné on pourrait dire qu'elle mesure davantage le Cha microscopique mais je pense que là encore si on balance ça juste au détour d'une phrase on risque d'induire en erreur car le mot microscopique peut facilement être mal compris lui aussi en particulier dire que le second principe de la thermodynamique prédit un effondrement civilisationnel qu'il rend une économie verte impossible ou concours à notre perte en dissipant de l'énergie en chaleur en sousentendant que ces limites physiques concernent les millénaires à venir s'est effectué un grave contesens vis-à-vis des lois de la physique et des ordres de grandeur du monde actuel du reste depuis 1789 la France a beaucoup gagné en structure avec l'instauration de l'éducation populaire la liberté de la presse l'indépendance de la justice mais aussi en créant de nombreuses institutions publiques pour permettre à la démocratie de fonctionner de manière stable durable et satisfaisante à l'échelle macroscopique si on regarde l' éolution des sociétés au cours des trois derniers siècles on est très loin d'assister à une amplification inéluctable du chaos et pourtant bien entendu pendant ce temps l'anthropie term dynamique de l'univers a augmenté même si on inclut que le Soleil et la Terre et les rayonnements perdus dans l'espace pour obtenir un système fermé il y a eu une structuration macroscopique même si une partie de l'information macroscopique a été dissipée et perdue dans les échelles microscopiques en fait même si on coupait le robinet solaire du jour au lendemain et si on mettait d'énormes miroirs autour de la Terre pour forcer l'énergie ip par radiation à rester sur terre et éviter de perdre de l'énergie on serait encore extrêmement loin de l'équilibre thermodynamique en particulier d'un point de vue physique énormément de travail utile pourra encore être exploité en principe par exemple via les ressources fossiles la géothermie la fission nucléaire ou encore la fusion nucléaire pendant au moins des milinaires et si on utilise la fusion nucléaire même on en a pour des milliards d'années concrètement tu fais comment alors je ne dis pas ça pour dire que tous les problèmes environnementaux sont faciles à régler loin de là le potentiel derrière la fusion nucléaire est extrêmement difficile à dompter avec les technologies d'aujourd'hui mais il faut bien voir qu'il s'agit d'une limite technologique et non pas d'une limite physique bien sûr une limite technologique ça reste une limite et c'est déjà une raison suffisante pour ne pas compter sur la fusion nucléaire au moins pour les 30 ans à venir mais cette limite technologique a beaucoup moins de chances d'être aussi indépassable que les limites physiques du point de vue des limites physiques on est extrêmement loin de tout équilibre thermodynamique avec ou sans soleil il est par conséquent extrêmement déplacé de par parler de l'entropie thermodynamique comme du limite planétaire et du reste le JEC ne le fait pas Reff invoquer les limites thermodynamiques pour parler d'urgence écologique aujourd'hui c'est en fait beaucoup plus erroné que d'affirmer que la planète Terre va brûler à cause de la transformation inéluctable de notre Soleil en géant rouge c'est non seulement être complètement à côté de la plaque en terme d'échelle de grandeur mais c'est aussi révélé sa profonde confusion sur l'état des crises actuelles et s'ouvrir à des critiques parfaitement justifiées mais ça rodolp et Jeanlou le débunque avec beaucoup plus de précision avec deux autres vidéos qui sont sortis en même temps que la mienne et que je vous invite à aller voir après le visionnage de cette vidéo bref s'il vous plaît évitez absolument d'invoquer l'entropie pour parler de chaos social ou de l'inexorable effondrement de nos sociétés on fait quoi bon alors déjà on se calme il s'agit donc de recadrer le débat si vous voulez parler d'urgence écologique il y a suffisamment d'autres limites planétaires beaucoup moins bullshit à évoquer comme le changement climatique l'acidification des océans les cycles biogéochimiques l'usage des sols et l'intégrité de la biosphère et pour lesquels il existe des stratégie d'atténuation des risques concrets qui s'appuie fortement sur une transition énergétique déjà enclenchée et qu'il faut surtout beaucoup plus soutenir notamment par exemple avec des fonds publics ça fait beaucoup et de façon plus générale il y a beaucoup d'autres problèmes civilisationnels encore qui nous menace au 21e siècle y compris certains qui me terrifient encore plus que l'urgence écologique comme l'accroissement des inégalités la croissance exponentielle du cybercrime la montée de l'autoritarisme à travers le monde l'amplification des appels à la haine et l'augmentation des tensions géopolitique entre des puissances nucléarisées concentrons-nous sur les vrais dangers et évitons de détourner l'attention de tous vers des considération injustifié