dans les derniers épisodes on a parlé en long en large et en travers du problème de la surinterprétation surinterpréter s'est expliqué ce qui n'est que le fruit du hasard or de telles explications se gênent et harry souvent mal et le problème c'est que détecter et comprendre la surinterprétation c'est un problème très très très difficile on a vu que la théorie formelle de l'apprentissage statistiques à coups de dimension wc et d'apprentissage pack parvenait à fournir des conditions suffisantes pour éviter un trop grand risque de trop grande sur interprétations notamment parlé du théorème fondamental de l'apprentissage statistiques et j'ai passé beaucoup de temps à le critiquer en affirmant notamment qu'il s'agissait d'un formalisme restrictif et pas universellement applicable aujourd'hui on va voir en particulier qu'en pratique bien souvent les datas scientiste ne tire pas une explication d'un chapeau d'explication car ce procédé de sélection est précisément une cause majeure de la surinterprétation en fait au lieu de tirer des explications d'un chapeau il est préférable de voir l'apprentissage statistiques comme la pousse d'arbre formant toute une forêt et même si chaque arbre individuellement sur interprète de façon assez étonnante la forêt elle évite la surinterprétation et devinez quoi c'est très bel et bien comme principe alors je présente les arbres et les forêts comme une métaphore il en fait il s'agit de vrais algorithmes d'apprentissage et pour comprendre cela on va commencer par parler des arbres un arbre une suite de questions de sorte que chaque question sein de l'arbre en branche la première question est appelée la racine de l'arbre et chaque question qui suit correspond à ce que l'on appelle des noeuds de l'arbre chaque ne divise l'arbre en branche est au bout des branches on trouve les feuilles de l'arbre dans le cas des arbres dit de décision chaque feuille correspond à une décision ou une prédiction par exemple dans le cas des chocolats d'albert on peut construire un arbre à deux étages on peut d'abord se demander quel est le type de chocolat puis pour coller aux données lorsque la praline est au chocolat noir il nous faut aussi nous demander si elle contient des éclats de noisettes l'arbre alors construit va alors nous permettre d'effectuer des prédictions en effet pour tout praline il nous suffit de suivre le parcours établi par les réponses des questions des noeuds l'arbre dans le cas le plus général où les données sont décrites par des features de grande dimension il ya une approche pour automatiser la construction de l'arbre prenons l'exemple d'un espace défi jazz des dimensions de pour déterminer une question à se poser on va choisir l'une des coordonnées par exemple l'abscisse puis ce qu'on va faire c'est couper selon abscisse de sorte à assez bien séparer les likes des dislike la question qui se posera à la racine de notre arbre sera alors de savoir de quel côté de cette coupure on se trouve puis pour raffiner notre classification on peut répéter la procédure on tire à nouveau un axe disons à nouveau la psy ce puits pour chaque région on essaie de couper selon l'abc ce de façon à séparer au mieux les like des dislike et voici à l'image la construction d'un nouvel étage si on suppose que l'on a maintenant tirer l'acce désordonnée cette procédure a le bon goût de pouvoir se complexifier à l'infini de parfaitement se généraliser aux très grandes dimensions et d'être calculable très rapidement par des ordinateurs mais les arbres ont bien sûr un gros défaut et ce défaut est bien entendu la surinterprétation en particulier si vous poursuivez la découpe de l'espace jusqu'à ce que votre arbre de décisions coordonnées alors vous pourrez être sûr que vous serez en plein sur interprétations oui parce que chaque nouvel étage de l'arbre est une sorte d'explication haddock et donc la dimension wc de notre chapeau d'explication c'est en gros le nombre maximal des tâches de l'arbre que l'on s'autorise bon en fait c'est encore plus compliqué que cela parce qu'il ya aussi la question du choix de la coordonnée sur laquelle on coupe reste que la tentation de coller aux données est grande et que le danger de la surinterprétation est donc énorme et bien dans ce genre de cas la proche privilégié des datas scientiste ne veut pas être de réduire la dimension vc lyon davantage faire pousser une forêt en fait chaque choix de l'ordre des coordonnées sélectionné conduit à la pousse d'un arbre différents l'idée de l'algorithme des forêts c'est de faire pousser tout plein d'arbres de taille assez grande et de considérer que le modèle retenu ne sera aucun de ces arbres mais davantage une combinaison de ces arbres typiquement pour effectuer une prédiction avec une forêt on va demander aux différents arbres d'aller voter et on va faire de l'avis majoritaire la décision collective de la forêt intuitivement il y aura alors beaucoup moins de biais de sélection du au choix d'une seule explication du chapeau d'explication oui parce que souvenez vous vous n'avez vu que le problème de la surinterprétation notamment dans le cas de facto nice etc avec des petites fluctuations dans les données il en venait à conclure tout et son contraire alors oui de petites brises dans les données invaliderait certains arbres de la forêt cependant on peut espérer que la majorité des arbres de la forêt restera robuste aux grands changements climatiques si bien que malgré les changements climatiques la position de la forêt sera inchangé alors oui tout ce que je commente assez juste une analogie mais l'analogie avec la forêt et le climat est loin d'être farfelue mais surtout de façon cruciale grâce à la robustesse générés par la diversité de notre forêt on peut t'autorise et chaque arbre à explorer davantage de complexité en particulier si les seules explications pas trop mauvaise des données requiert un niveau de complexité supérieur au nombre de données alors la forêt pourra s'autoriser de telles explications tout en luttant contre la surinterprétation en misant sur la multiplicité des explications bien entendu ce que je dis là des arbres est valable pour toutes sortes d'autres algorithmes d'apprentissage typiquement la combinaison de plusieurs réseaux de neurones distant réduira la surinterprétation de chacun des réseaux de neurones il ya même une façon d'automatiser la diversité des prédictions au sein même d'un réseau de neurones via une technique appelée drop out mais je veux laisser ça de côté pour aujourd'hui dans tous ces cas il s'agit de miser sur une forêt de modèles prédictifs différents pour gagner en robustesse et diminuer la variance de prédiction ce qui revient à lutter contre assure interprétation alors on pourrait se dire qu'en misant ainsi sur la diversité on en vient quand même à autoriser lé influence des moins bons modèles et en effet c'est le cas pour permettre la sagesse de la forêt il est nécessaire de faire cohabiter deux grands chênes avec de petites fougères même si ces petites fougères ne sont pas de super explications des données et l'astuce pour préserver la sa graisse de la forêt tout en permettant de faire émerger la vie des arbres les plus pertinents c'est de limiter l'influencé des différents arbres en fonction de leur performance explicative ceci correspond à des techniques titre dance bling de biking de boosting dont le saint graal est un algorithme appelé à dabou sts qui parvient à combiner de mauvais modèle pour en faire un très bon à dabou et gagnera ainsi le prix goodell de 2003 qui récompense des avancées majeures en informatique et bien sûr comme tout algorithmes du marché du tonnerre à davos a des fondements très bel et bien en fait il s'agit d'une approximation de la forme de baisses mais comme vous commencez à y être habitué je veux laisser ça pour une autre fois bref pour lutter contre la surinterprétation contrairement à ce qu'affirme le théorème fondamental de l'apprentissage statistiques il n'est pas forcément nécessaire de ne s'autoriser qu'une explication haddock toutes les 100 nouvelles données on peut également s'autoriser la multiplicité des explications et en pratique pour résoudre de vrais problèmes cette multiplicité des explications est incontournable l'exemple historique de maxi problème de prédiction est le fameux prix netflix en 2006 netfix disposait déjà de centaines de millions de notes de films que des centaines de milliers d'utilisateurs dont entrée netflix posa le défi suivant étant donné une fraction de ces notes que netflix candy public prédire les autres notes netfix promis un million de dollars pour qui surpasserait la performance de l'algorithme de netflix de plus de 10% après près de trois ans de recherche de dizaines de milliers d' équipe de dada scientiste et après de nombreuses péripéties qui mériterait une vidéo à elle seule c'est l'équipe belle corse pragmatique chaos qui est parvenu leur solution consistait en une jungle de 800 modèles prédictifs différents tous plus sophistiqués les uns que les autres la sagesse de la forêt avaient eu raison de tout arbre individuel donc la diversité explicatif semble en fait indispensable à tout système prédictif de qualité et ça en fait d'un point de vue philosophique ça peut paraître assez troublant oui parce que du coup pour qu'ils aient une vraie diversité dans notre forêt d'explication ces explications devront inéluctablement se contredire et will typiquement si je reprends mes arbres chaque arbre est une explication différente des données qui a en fait de bonnes chances d'être logiquement incompatible avec l'explication proposé par un autre arbre autrement dit le principe de sa graisse de la forêt nous invite à célébrer les combinaisons de théories incompatibles et en particulier à rejeter toute quête de théories utile plus étrange encore elle semble contredire le principe de rasoir d'occam qui semble nous inviter a préféré la simplicité d'un arbre explicatif à la complexité de toute notre forêt d'explication un philosophe pour air ainsi dire que l'espèce de jungle mais non gêne que je semble créer la pose des problèmes de parcimonie ontologique oui je me la pète un petit peu avec des mots des trois syllabes que j'ai appris en lisant la thèse de monsieur fille alors qu'en fait la forêt dont je parle ici n'est pas vraiment une jungle mis nos gènes reste que le manque de parcimonie de ma forêt d'explication reste intuitivement dérangeante pour beaucoup en particulier si notre but est la quête de la vérité ou d'une vérité simple et interprétable alors cette forêt d'explication devrait vous perturbée au plus haut point eh bien le succès des forêts taille d'arbustes et de belles cordes pragmatique kaos nous pousse à rejeter une fois de plus toute quête de la vérité hashtag tous les modèles sont faux certains sont utiles j'irai même plus loin pour lutter contre la surinterprétation la quête de la vérité me semble profondément nuisibles car elle nous pousse à omettre la sagesse des forêts avant de passer aux commentaires une petite annonce je serai à la geek touch le 8 avril prochain à lyon et je vous ai préparé une petite conférence très bayésienne les neuf en a parlé des facteurs de confusion et du paradoxe de simpson et notamment je vous avais demandé de réfléchir ce problème de kanté versus radio sachant que n'avait des statistiques qui disait que globalement radio est meilleur que quand tu es mais que contre tout adversaire contre les petites équipes et contre les grandes équipes quand et faisait en fait toujours mieux que radio alors comment est-ce que ça a c'est possible et bien je vous propose une preuve essentiellement visuel donc je reprends le tableau qu'on avait la dernière fois hélas que je vais faire c'est souligner le fait qu'en fait les matchs de radio sont contre des petites équipes et les matches de caen tesson contre des grosses équipes et du coup le pourcentage de victoires de kanté est essentiellement ce pourcentage de victoires contre des grandes équipes alors que le pourcentage de victoires de rillieux est essentiellement celui contre des petites équipes alors y en a plusieurs dans les commentaires qui ont fait remarquer qu'ils avaient trouvé ça très rapidement avoir une explication félicitations à vous bravo alors si vous avez trouvé très très rapidement en fait c'est un petit peu dommage parce que fait bien on se rencontrent des difficultés de ce prêt mais si on me résous trop rapidement on se rend pas forcément compte que ce problème était en prob extrêmement difficile et donc notamment pour ceux qui ont trouvé 1 ère fois mais pour tout le monde je vous propose un autre problème de compréhension du paradoxe de simpson alors si vous passez dans les années 80 il veut une étude sur est ce que le tabac causé plus de décès et du coup j'ai fait une étude épidémiologique verts qui sont les sondés de nombreuses personnes ils les ont suivis pendant 20 ans et ceux qui ont découvert c'est qu'au cours de ces vingt années le taux de décès était plus grand chez les non fumeurs que chez les fumeurs et du coup la question à laquelle j'aimerais qu'on réfléchisse et c est ce que ça veut dire que du coup fumée c'est bien pour la santé ou est-ce qu'il n'y aurait pas un facteur de confusion et si oui quel est ce facteur de confusion encore une fois essayé de ne pas spoiler la réponse dans les commentaires il veut même troller un maximum en ne vous donnant jamais la réponse à cette question elle existe quelque part sur internet dans une vidéo youtube qui est dans un des liens de l'une de mes vidéos ouvert pas facile à trouver mais ce que je veux c'est que vous vous rendiez compte vraiment que trouver le facteur de confusion dans ce cas c'est extrêmement difficile et il est alors incroyablement tentant de croire dans la statistique et de croire du coup on fait que fumer est bénéfique pour la santé bon je me permets quand même donné un petit indice en gros pour trouver les facteurs de confusion à chaque fois il faut absolument se demander s'il n'ya pas une différence a priori entre par exemple les matches qu'on joue et radio et quand est ce qui n'aura pas une différence a priori promesses qui ont été faites par obama ou trump ou encore à ce qu'il n'y aura pas une différence a priori entre les lycées financer et ici qui ne sont pas financés fait la clairon pour comprendre le paradoxe de simpson s'est essayé de raisonner avec ses a priori et c'est trés bayésiens tout ça enfin fort bien et antoine roussel font remarquer il ya une grosse erreur de calcul dans la vidéo restantes désolé vous est demandé de vérifier les calculs un moment de la vidéo et si vous l'avez bien fait vous devriez avoir vu que en fait dans mes notes le nombre de lycées favoriser financé et est égal à quinze mille et pas à 5000 fait que les résultats sur un feuille mais à l'écran ça la fout mal j'espère que vous avez aimé cet épisode que la sagesse des forêts vous fera réfléchir notamment quant à l'épistémologie la philosophie des sciences de façon plus générale parce que je pense que laurence a des implications à ce niveau là la prochaine fois on va parler de l'autre solution utilisée par les praticiens des machines erding pour pouvoir éviter assure interprétation sans diminuer les dimensions wc de leurs modèles à savoir la régularisation notamment pour vous préparer pour la semaine prochaine vous pouvez vous poser la question suivante imaginez que vous étudiez un phénomène et vous disposer d'assez peu de données mais il ya énormément de cause possible de la conséquence que vous observez du coup comment est ce que vous pouvez faire pour déterminer la cause sachant qu'il ya beaucoup plus de cause possible que de données que vous observez parce qu'il ya quand même un moyen d'identifier quelles sont les causes les plus probables ou quelque chose comme ça sachant que si on fait typiquement de la régression linéaire dans ce cas là on risque fort d'être foutu et de pas réussi à refaire cette agression il ya ce genre de cas c'est typiquement le cas de l'analyse de l'adn savoir quels sont les facteurs de risque au niveau du code génétique des tic en personne comment est ce que le code génétique influence les risques de l'hôtel maladie ça c'est un problème majeur qui s'est posé avec le séquençage notamment de là et ce sont des quelques on appelle le typiquement large prisme inspire beaucoup de paramètres beaucoup de causes possibles toutes les valeurs des nucléotides deux adn sont potentiellement des causes imaginable mais on a peu de haine on a peu de donner à ce que le nombre de données qu'on dispose est très inférieur au nombre de lettres dans son plan d'affaires laisse un petit peu réfléchir à tout ça si vous avez aimé cet épisode penser à le lac et à le commenter le phare faisait penser à vous avez des futurs épisodes merci aux outils performants et j'espère que vous serez là la prochaine fois besides the used in the famous et de boost à gauche et easyjet reasons eelv en 2012 on y perd en dessous koulou week end risque reste une autre relative prison