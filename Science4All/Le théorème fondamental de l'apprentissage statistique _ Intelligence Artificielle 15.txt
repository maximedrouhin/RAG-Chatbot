la dernière fois on a vu comment les informaticiens va cliquer sheridan qui ont su formaliser le concept de complexité d'une explication à travers la notion de dimension wc et on avait vu que la dimension wc permettait de formaliser la notion intuitive que si la complexité d'une explication est une petite fraction du nombre de données alors la généralisation de l'explication a détonné non observé ne sera pas trop mauvaise aujourd'hui on va pousser cette notion un peu plus loin en parlant du théorème fondamental de l'apprentissage statistiques ce théorème dit la chose suivante si la dimension bce n'est qu'une petite fraction du nombre de données d'apprentissage alors il n'y aura pas trop de risque de trop grande sur interprétations donc en gros ça dit à peu près la même chose que les thermes de vals nick deux épisodes précédents mais j'ai choisi deux camps même vous parlez du thaurac fondamental de l'apprentissage statistiques parce qu'il est souvent considéré comme un monument de la théorie formelle du machine learning et de son approche rigoureuse et du coup je pense qu'on pourrait vous reprocher de ne pas en avoir parlé mais comme vous le sentez peut-être à ma voix en fait je suis pas particulièrement fan de ce théorème enfin c'est clairement un magnifique théorème mais je trouve qu'il ne mérite pas vraiment son titre de fondamental d'ailleurs à la fin de cette vidéo je vous expliquerai pourquoi est-ce que je pointe cela il se sera sans doute la partie la plus intéressante de cette vidéo mais pour commencer il nous faut d'abord comprendre ce théorème et sa formalisation rigoureuse qui repose sur une notion introduite par l'informaticien leslie valiant tu cette notion elle l'apprentissage probablement approximativement correct apprentissage pâques ou encore pack learning gagnante remportera d'ailleurs le prix turing de 2010 il ya un peu le prix nobel de l'informatique en partie pour avoir introduit la notion d'apprentissage pâques mais aussi pour avoir introduit toute une théorie de l'apprentissage de façon intuitive l'apprentissage pack formalise la notion de pas trop de risque de trop grande sur interprétations pour comprendre cela il faut bien se rendre compte que le hasard fait qu'il ya toujours un risque de collecter des données qui ne sont absolument pas représentative du phénomène étudié surtout si on ne connaît que peu de données il ya toujours un risque que ces données nous conduisent à mal à prendre cependant plus on collecte de données plus intuitivement la probabilité que ces données soient représentatives sera grande c'est pour ça que même si on ne peut pas entièrement exclure la possibilité d'avoir des données non représentative quand on connecte suffisamment de données on peut néanmoins affirmer que cette probabilité est faible tout ce que j'ai décrit la correspond qu'aux à trop de risques dans la formulation pas trop de risque de trop grande sur interprétations et homo probablement dans l'expression probablement approximativement correct formellement en a pris l'habitude de noter delta la probabilité que les données donnent vraiment n'importe quoi et on va résonner avec le cas beaucoup plus probable que ces données ne sont pas trop n'importe quoi techniquement pour les matheux parmi vous delta se déterminent via les inégalités de concentration comme les inégalités de tirs 9 et correspond intuitivement à retirer les que des destructions quasi normal qu'ils sont des sommes d'un grand nombre de variables indépendantes ce genre d'astuce et vraiment omniprésent en théorie du machine learning toutefois même si on a des données représentatives il reste alors peu probable que l'on arrive à déterminer exactement la bonne explication des données et en particulier si le chapeau d'explication écran alors il ya peu de chances que les données nous amène à tirer la toute meilleure description qui est dans le chapeau l'apprentissage paquinou vide ne pas faire la fine bouche et de nous satisfaire d'une explication quasi optimale dans le sens où le taux d'erreur de l'explication qui a été sélectionné ne doit pas être beaucoup plus grande que le taux d'erreur de l'explication optimale on utilise typiquement la lettre epson pour mesurer l'écart entre l'explication qui a été sélectionné et l'explication optimale si le taux d'erreur de l'explication qui a été sélectionnée est un - d'epsilon du taux d'erreur de l'explication optimale alors on dit que l'apprentissage et epsilon correct on a tout ce qu'il faut pour formaliser la notion d'apprentissage probablement approximativement correct on dit qu'un apprentissage et epsilon delta pâques si avec probabilité 1 - delta lorsque je collecte l'étonné l'apprentissage va conduire à la sélection d'une explication qui et epsilon approximativement correct littéralement l'apprentissage sera un - delta probablement epsilon approximativement correct je peux maintenant annoncer la version formal du théorème fondamental de l'apprentissage statistiques ce théorème dit que pour qu'un apprentissage soit delta epsilon pas qu il faut que le nombre de données d'entraînement soit de l'ordre de wc plus longue de 1 sur delta le tout divisé par epsilon carré ou wc et bien sûr la dimension wc du chapeau d'explication ça c'est ce que beaucoup appellent le théorème fondamental de l'apprentissage statistiques sachant qu'en pratique les logarithmes joueur est négligeable ça veut dire que comme le théorème de wernicke la firme aussi pour éviter un trop grand risque de trop grande sur interprétations il faut que le nombre de données soit de l'ordre de wc / ypsilon carré donc si je prends une valeur de epson a ses petites disons 0,1 ça veut dire qu'il faut environ 100 fois plus de données que la dimension wc de notre chapeau d'explication ok donc ça que c'est un peu l'application directe du théorème mais je pense qu'ils aient aussi des leçons philosophie qui a tiré de ce théorème en particulier il me semble que ce théorème indiquant quoi il est souhaitable que les sciences ne cherche pas à découvrir la vérité oui parce qu'une vérité fondamentale doit être en mesure d'expliquer toutes les données or toute explication parfaite des données est quasi certaine de ne pas se généraliser comme l'informatique théorique la statistique nous invite à rejeter toute notion de vérité fondamentale et a sans doute davantage préféré le hashtag tous les modèles sont faux certains sont utiles autre enseignement important de ce théorème on ne peut jamais être garantie d'avoir évité la surinterprétation tant qu'il y aura de la l'ead en automne et il y aura toujours une probabilité que l'apprentissage se passe mal même si vous avez pris toutes vos précautions dans la collecte de données en particulier il me semble que prendre en compte toute l'étendue de notre ignorance soit une étape indispensable à toute analyse de données et en particulier que le langage des probabilités soit indispensable à toute philosophie du savoir ignorer les probabilités en épistémologie c'est négliger les probabilités pourtant souvent non négligeable d'avoir des données non représentative et toutes ces conclusions font du théorème fondamental de l'apprentissage statistiques un théorème très intéressant mais il ya quand même des bémols à mettre à toutes ces interprétations du théorème fondamental de l'apprentissage statistiques oui parce que ce théorème est beau mais il a aussi ses limites et même beaucoup de limites pour commencer ce théorème est une condition suffisante pour éviter à surinterprétation ça veut dire qu'il reste possible de ne pas être en régime de surinterprétation quand bien même la dimension wc du chapeau est bien plus grande que le nombre de données auxquelles on a accès comme le chercheur en informatique jo farrington non pas pour ce séminaire ça semble d'ailleurs être le cas du cerveau humain souvenez vous que l'on a estimé la taille du cerveau à 10 puissance 15 pitt et donc sa dimension b c est sans doute de l'ordre de 10 puissance 15 aussi alors même que ce cerveau ne vit que 10 puissance 9 secondes notre cerveau semble en fait naturellement plus tôt dans un régime de surinterprétation est alors oui en effet souvent clairement on surinterprète mais de là à dire que le cerveau humain sur interprète tout le temps disons que c'est très discutable et dans de futurs épisodes on verra que la méthode privilégiée des praticiens pour lutter contre la surinterprétation ce n'est pas de réduire la dimension wc de leurs modèles bref les théories fondamentales et joli sur le papier mais je ne suis pas sûr qu'il soit si pertinent en pratique une deuxième faiblesse majeure du torrent fondamentale ces kilos mais complètement la surinterprétation par biais de sélection dont on a pourtant parlé avec hygiène mentale en effet en pratique il arrive très souvent que les deux années d'apprentissage soit biaisée par rapport aux données que l'algorithme rencontrera en pratique typiquement les expériences de psychologie sont souvent réalisés sur des étudiants des universités occidentales tout bêtement parce que ce sont les données qui sont accessibles aux chercheurs en psychologie de même l'un des plus grands problèmes de prédiction et celui des résultats des élections à partir de sondages sauf que l'on sait que les sondages sont des intentions de vote annoncé qu'ils ont de bonnes chances d'être biaisés vis-à-vis des votes à venir vouloir trop collé aux données d'apprentissage sans un modèle extérieur qui prend en compte ses billets dans les données c'est sauter à pieds joints dans le piège de la surinterprétation et aboutir à une conclusion qui se généralisera très mal d'un point de vue théorique ce que je critique la revient à critiquer le fait que la distribution des données d'apprentissage soit la même que la distribution qui est utilisé pour juger du fait que l'explication retenu est bien epsilon approximativement correct une troisième critique plus général et le fait que le théorème fondamentale repose sur le paradigme iiie des idées ça veut dire indépendant est identiquement distribué ça sous entend notamment l'existence d'une loi de probabilité fondamentale qui serait la vraie loi de probabilité d'autres cherchent des approximations alors dans certains cas c'est une hypothèse qui est très raisonnable mais bien souvent les données sont corrélés et la loi de probabilité qui les gouvernent si elles existent peut varier en fonction de tout plein de paramètres en fait les trois critiques que je viens de dresser la ne sont pas du tout spécifique aux théories fondamentales l'apprentissage statistiques il s'agit d'une critique d'un formalisme plus globale qui fait de nombreuses hypothèses communément admise notamment statistiques et que l'on appelle le fréquentent isme et il y aurait énormément à dire sur le fréquentent isme sachant que c'est un petit peu l'anti biyaïsme du côté obscur de la force mais je vais laisser ça de côté pour aujourd'hui une quatrième limite du torrent fondamental c'est qu'il ne précise pas comment choisir le chapeau à explication alors que le choix de ce chapeau ce qui revient à se poser la question du choix de l'algorithme d'apprentissage est précisément l'un des grands défis de la recherche actuelle et en particulier l'une des grandes questions de la recherche actuelle c'est de déterminer quelle est la bonne architecture de réseaux de neurones artificiels enfin une cinquième et dernière limite du théorbe fondamental c'est qu'il ignore toute notion de complexité algorithmique or de nos jours l'une des grandes limitations du machine learning est le fait que celui ci soit très gourmand en calcul et en particulier il faut absolument que nos algorithmes d'apprentissage soit assez rapide bref d'études théoriques du machine learning comme vous commencez à le comprendre c'est compliqué et en particulier toute cette théorie semble encore assez distante de ce que les praticiens du machine learning font vraiment en pratique ceci étant dit le théories fondamentales de l'apprentissage statistiques restent très pertinent puisque il met le doigt sur l'une des causes majeures de la surinterprétation de façon grossière si vous n'avez pas beaucoup moins de paramètres dans vos modèles il n'ya de données sur lesquelles vous pouvez vous appuyer vous devez absolument vous demander si vous ne seriez pas en grave danger de sur interprétations j'espère que vous avez aimé cet épisode avant de passer à vos réponses aux commentaires une petite annonce puisque je serai à lyon le lundi 26 mars en soirée ou jeu donnera une présentation sur les mathématiques de la démocratie j'ai mis plus d'informations en description de cette vidéo les enfants n'a parlé des explications à un dock et de la dimension wc et vous êtes nombreux à voir ce qu'elle n'est que c'était pas facile à comprendre en effet alors entre eux sur des concepts qui sont pas évidents j'ai envie de dire que si vous n'avez pas tout compris à la vidéo précédente c'est pas grave je pense qu'on a un peu cette idée à l'école que je sois absolument tout comprendre quand on parle de mathématiques mais bon il ya des concepts qui sont difficiles en machine learning du coup bon bah c'est pas grave de ne pas tout comprendre je pense que ce qui était vraiment intéressant c'est un peu l'idée intuitive que finalement plein d'explications haddock france notamment le fait que lorsqu'on sort une explication et qui a une donnée que l'on découvre qui va à l'encontre de cette explication n'attendant c'est à ce moment là à sortir une explication haddock et bien le fait de sortir de telles applications haddock c'est une habitude à ne pas prendre ou en tout cas une s'autoriser qu'une fois toutes les 100 nouvelles données quelque chose comme ça c'est un petit peu cette idée globale qui est envie de précédentes ainsi que ce travail de formalisation pour passer de l'intuition à quelque chose de formaliser ladislas nal bord c'est que nous fait remarquer que des notions comme la dimension wc sont en fait intimement lié à d'autres notions utilisées en pratique comme par exemple le critère d'information d'accueil qu'est l'occasion pour moi de souligner l'étendue de mon ignorance puisque je ne connaissais pas ces critères mais il est tout simple de ce critère c'est de pénaliser le fait que un modèle est trop complexe donc d'une part on veut qu il explique bien les données donc ça c'est le terme blog n qui s'appelait aussi à l'ope vraisemblance et dont on parlera mais pas aujourd'hui donc maximiser la loque vraisemblance est quelque chose de très classique en statistiques mais pourra éviter d'avoir des modèles trop complexe ce que propose à calquer ses de pénaliser les modèles trop compliqué en introduisant notamment une pénalité égale au nombre de paramètres du modèle dans les faits ça c'est une très bonne idée et c'est en fait intimement liée à un truc qu'on verra dans une future vidéo qui s'appelle la régression de l'assaut est ce également intimement liée à des notions de base yanis monde et notamment d'à priori bayésiens et d'induction de solomonoff dans tous ces cas ce qui est amusant c'est de voir que dans ses modèles de machines downing d'apprentissage statistiques qui sont très mathématique a priori en fait on voit surgir très naturellement le concept de rasoir d'occam que l'on appelle aussi le principe de parcimonie qui dit que les modèles trop compliqué en fait sont d'une certaine manière mauvais parce qu'en fait ils conduisent à la surinterprétation et du cou à des modèles qui se généralise en fait très mal accésss yeux et skippy l'un des formats rich en fait il ya une excellente vidéo sur youtube qui montre très bien tout ce travail qu'il faut faire pour rendre des jolies théorème vrai et cette vidéo c'est une excellente vidéo dj que je recommande vivement sur le théorème de bisous qui apparemment est le théorème préféré d'alj donc ça vaut vraiment le coup et c'est particulièrement intéressant de voir comment un théorème faut peut être rendue en un théorème vrai à condition de le bidouiller un peu comme il faut tout en essayant de coller encore à l'intuition originale à tout ce travail quelque chose que je trouve absolument fascinant en fait j'ai quand même que c'est ce que je préfère en mathématiques voilà au fait remarquer que être grammar nazis des notations mathématiques en fait ça a du bon puisque par exemple si on a une hauteur et moi autant appeler à chelsea on n'a rien on tourne appelé air tôt que d'intervertir les notations quelques on se retrouve vite avec des trucs qui sont lisibles en tout cas pour les mathématiciens entraîné en effet utiliser les bonnes notations c'est un peu comme utiliser le bon langage les bons mots pour exprimer une idée c'est quelque chose qui facilite beaucoup la transmission de la connaissance mais également le choix de transmissions des intuitions la réflexion également autour des idées ça évite de s'embrouiller avec des détails qui ne sont en fait que des détails ceci étant dit comme le fait très bien remarqué hydroxy chloride ceci a aussi la mauvaise conséquence que des étudiants qui ne sont entraînés qu'avec un seul formalisme et qui ne sortent jamais de ce formalisme peuvent être complètement perdu dès que l'on change un tout petit peu le formalisme dès qu'on remplace par exemple des x ou y j'avais un ami qui aimait d'hyères de ses étudiants que faire une somme de 1 jusqu'à n essaye sa ferme et 2 1 jusqu'à cas qu'ils sont complètement perdu et c'est vraiment ça qu'il faut absolument éviter dans l'apprentissage des mathématiques cette dépendance vraiment à la notation et ça ça devient en fait particulièrement pertinent quand on regarde dans les recherches interdisciplinaires puisque voilà les physiciens ont tiré certaines notations les mathématiciens de notation les informaticiens d'autres notations encore puis ça peut même varier à l'intérieur d'une communauté peut être que les cryptographes vont peut être y mène notation que les chercheurs en machine learning qui n'ont pas utilisé une notation que chercher à enterrer la complexité informatique bref faut bien se rendre compte que les notations qui on utilise pas c'est une opération qui ont utilisé un moment donné parce que c'est pratique pour nous à ce moment là mais du coup il a de très bonnes chances que d'autres personnes utiliseront d'autres notation il faut être capable de voir que des formules écrite de manière différente sont en fait équivalente et pour cela je pense que c'est pas une si mauvaise habitude que de raisonner avec des symboles qui sont pas toujours les mêmes et signent et d'être à l'aise avec ces notations abstraite plutôt que de s'enfermer dans ses propres notations une certaine manière ça revient un petit peu la différence entre apprendre et comprendre d'une certaine manière tant qu'on est capable d'exprimer ses idées qu avec les symboles que l'on a appris bien j'ai envie de dire qu'on ne comprend pas vraiment la nature des choses que l'on manipule et j'espère que vous avez aimé cette vidéo la prochaine fois on va parler mais quelque chose que je trouve ultra important qui vraiment peut-être le tout le plus important à savoir dans toutes les statistiques et qui est très peu enseignée et surtout très peu compris des gens qui ont appris pourtant les statistiques par exemple le statisticien elle arriva sur man 2 10 de ce que nous autres on va parler prochaine il sait qu'il s'agit de quelque chose de très confondant oui je sais que ce mode histoire mais j'ai eu l'autorisation de me suffit de l'utiliser dans un épisode actions y compris pour des statisticiens bien formés et pour illustrer ce dont on va parler la semaine prochaine imaginez l'exemple suivant supposons qu'il ya deux joueurs de football qui voudraient m'équipe et supposons que lorsque le joueur ajout son équipe gagne 80 % du temps et supposons que lorsque le voir belge où son équipe gagne 50% du temps est ce que ça veut vraiment dire est ce qu'on peut conclure que le joueur a et du coup meilleur que le joueur b je vous invite à réfléchir à cette question à vous demander pourquoi est ce que cette conclusion serait peut-être prématuré oui parce que bien sûr il ya un petit piège si vous avez aimé cet épisode penser à la ligue et à le commenter à le partager pense à vous abonner pour ne pas manquer futurs épisodes merci aux ti peur pour leurs dons et j'espère que vous serez là la prochaine fois depuis huge de quality of hope i pour celle à un peu de cette date de 1916 m which we shall kolda ouvert aux pénuries m 10/06 bio marc beauvillain