en un mot oui il y a c'est lire mais bon je vais suivre les conseils de thibault et faire preuve d'un peu de charité interprétative bien entendu quand on pense à savoir lire on pense aussi à comprendre ce qu'on dit est ce qu'il n'ya comprend la sémantique de ce qu'elle lit dans quelle mesure peut-on dire qu'une ia exposé un livre comprend ce qu'elles y trouvent en novembre 2010 est une collaboration entre facebook l'université du mans et l'université pierre et marie curie a produit une ia et ses tirs semblent montrer une étonnante maîtrise de la sémantique des mots pour comprendre cela je vais vous décrire l'expérience qui a été réalisé en partant de rien il y à a commencé à lire énormément de textes en anglais et à partir de ce cas lisez en anglais elle a cherché à un ferrer le sens des mots anglais plus précisément elle a construit une interprétation sémantique des mots qu'elle lisait ce qui revient à construire une fonction qui prend des données brutes ici des mots et en extrait une représentation sémantique comme on l'a vu la dernière fois ça correspond à une fonction qui transforme un espace de très grande dimension en un espace de dimensions raisonnables typiquement en passant d'une dimension des est égal à 1 million à une dimension c'est égal 300 bref après avoir lu énormément en anglais lille s'est mise à trouver une interprétation sémantique de ce qu'elle disait mais on peut se demander si cette interprétation sémantique est bonne et si elle correspond à notre interprétation sémantique berra je vais laisser cette question en suspens pour aujourd'hui convenons-en à l'expérience client en question ensuite fait la même chose pour d'autres langues comme le français elle s'est mise à à lire énormément de textes en français mais je précise que ces textes en français n'était absolument pas des traductions des textes en anglais elle a juste lui du français et comme pour l'anglais l'ira s'est mise à construire une représentation sémantique des mots de la langue française à partir de ce qu'elle disait en france elle donc j'ai capitulé on a des textes en anglais et des sémantique de l'anglais ainsi que des textes en français et des ses antiques du français serait ce possible d'exploiter la sémantique pour créer ainsi un canal de traduction intuitivement c'est comme ça qu'il faudra effectuer des traductions de l'anglais au français il faudrait lire le texte en anglais en inférer le sens puis exprimer ce sens en français et intuitivement la clé de cette approche c'est le fait que la sémantique est quelque chose d indépendants du langage dans lequel elle est exprimée j'ai presque envie de dire que si on parvient déterminé quelque chose qui est indépendant de toute langue alors ce sera la sémantique non vous pouvez deviner où je veux en venir les cinq chercheurs ont montré que la sémantique de l'anglais la sémantique du français était en fait deux espaces quasi isomorphe dans le sens où on peut tourner l'espace de sémantique anglais pour le faire quasiment coïncidé avec l'espace de sémantique français voilà qui permit aux cinq chercheurs de créer un canal qui va des textes en anglais à la sémantique anglaise à la sémantique française au texte français ce canal permis aux cinq chercheurs d'effectuer des traductions de l'anglais au français devinez quoi oui cette approche surpasse toutes les autres approches de traduction mot-à-mot y compris celles où fournissait aux ja ne liste de traduction botha mots et en particulier je pense que cette expérience montre que en un certain sens les ja sont capables d'inférer du sens de ce qu'elle lise alors ça veut pas dire qu'elles ont atteint un niveau humain en lecture je pense qu'un élève de ce1 est encore meilleur que les ya notamment parce que les ja ont du mal à un ferrer le sens d'une phrase d'un paragraphe mais la recherche onia avance et al avance très vite je ne serais pas surpris si dans les années à venir les ja atteignent un niveau collège ou lycée en lecture voire plus alors bien sûr il ya plein de détails que j'ai sauté dans ses explications il y aurait énormément plus à dire notamment parce que ça repose sur des réseaux de neurones et des modèles adverse are you on y reviendra mais pour la 2e partie de cet épisode j'aimerais m'arrêter sur l'opération que je trouve être la plus spectaculaire dans cette expérience à savoir la construction de la sémantique comment est ce possible pour lisions des textes en anglais on puisse donner un sens à l'anglais qu'est ce qui fait que les mots ont un sens et qu'est-ce qui fait le sens des mots et bien l'hypothèse fondamentale de l'apprentissage de la sémantique des mots est le postulat du linguiste john firme qui disait vous connaîtrez un mot par la compagnie qu'il conserve autrement dit ce n'est pas le mot en lui même qui lui donne son sens ce qui donne un sens au mot l ensemble des phrases où il apparaît le sens des mots est donné par les contextes où ils sont utilisés et ça peut vous paraître bizarre mais si vous avez bien suivi ma série sur l'infini c'est quelque chose dont on a déjà parlé ce qui fait qu un objet mathématique est intéressant ce n'est pas l'objet en lui-même mais ses relations aux autres objets l'ensemble des relations entre les objets formation est une structure et c'est finalement les propriétés de cette structure qui importe plus que le nom qu'on donne aux objets de cette structure en particulier deux structures parleront finalement de la même chose si elles sont idiots morts bref tout ça pour dire que le sens d'un mot c'est avant tout son contexte d'utilisation et ce contexte est tout simplement l'ensemble des mots qui l'accompagnent c'est ce principe que les informaticiens ont cherché à à formaliser particulier en 2013 quatre chercheurs de google nikolov chaîne corrado et dean ont réussi une percée dans ce domaine à l'aide d'un réseau de neurones capable d'exploiter les relations d'un mot homos qui l'entourent pour créer une représentation sémantique des mots sur réseaux de neurones et appelé world check il est devenu un ustensile incontournable dans l'analyse automatisée des textes de façon spectaculaire quand deux mots sont sémantiquement proches comme chats et chiens world check les transforment en des vecteurs sémantique également proche au sens où l'angle entre les deux vecteurs est petit la proximité des représentations sémantique des mots correspond bel et bien à une proximité sémantique au sens intuitif de la sémantique mais ce n'est pas sur le réseau de neurones qui a réussi cette prouesse que j'aimerais m'arrêter ce dont j'aimerais vraiment vous parler c'est d'une publication de 2014 de marc levy et yoav kohlberg le tour de force de cette publication fut de montrer que world check est en fait une sorte de svd l'algorithme quand un appareil dans l'épisode précédent et qui a servi à prédire si les utilisateurs de netflix allaient aimer tel ou tel film oui parce que souvenez vous svd consistait à exploiter des relations entre les films et les utilisateurs comme les notes d'utilisateurs pour des films pour créer un espace de sémantique film utilisateurs et on avait vu que cet espace représente très bien des relations entre les films typiquement les films violents seront des vecteurs semblables dans l'espace de sémantique film utilisateurs de la même façon world studec consiste finalement à un ferrer la sémantique des mots à partir des fréquences auxquelles ce mot est utilisé dans tel ou tel contexte ces relations entre les mots et les contextes permettent alors de créer un espace de sémantique beaux textes et bien c'est cet espace de sémantique mot contexte que j'ai tout simplement appelé espace de sémantique des langues anglaise et française ces espaces de sémantique sont tellement bien foutu qu'ils permettent la traduction d'une langue à l'autre en passant par le calcul du sens des mots alors il ya pas mal de subtilité concernant watts tu veilles notamment dans la manière dont les relations entre les mots et les contextes sont comptabilisés en particulier certains mots sont très fréquents et d'autres sont peu fréquents du coup il faut un peu lisse et les fréquences des mots typiquement avec une fonction logarithmique et puis un autre problème c'est que les mots peu fréquents sont cohérents avec plusieurs contextes mais il ya de bonnes chances qu'il n'y ait aucun exemple de l'apparition de ces mots dans ces contextes dans les textes qu'à lui il ya du coup il faut faire gaffe à ne pas pénaliser l'absence de ces exemples nous assez les détails près avec quelques autres watch tu veux vraiment une sorte de svd bref ce que je trouve fascinant dans toute cette histoire c'est peut-être avant tout les conséquences épistémologique de ses travaux sur la sémantique désir en particulier world check suggère qu'il est tout à fait possible d'insérer le sens des textes simplement en lisant des textes voilà qui peut paraître très bizarre intuitivement on a envie de dire que le sens des mots qu'on lit est relié à toutes sortes de données sensorielles notamment et qu'on ne peut pas vraiment comprendre le sens de ces mots ne faisant que lire des livres par exemple si je vous dis que je vois que cette robe est bleu et noir n'ont ville de dire qu'une ya ne pourrait jamais ressentir cette sensation de bleu et noir en tout cas par ne faisant que lire des textes alors on pourra rétorquer cugnac il y eut ipdia y verra aussi des images et pourra du coup ressentir les couleurs mais si je prends maintenant l'exemple d'une sensation comme peur ou la joie là on a vraiment envie de dire qu'il s'agit de sémantique qu'unia ne pourra jamais vraiment ressentir en tout cas pas en ne faisant que lire des textes et bien je n'en serais pas si sûr au final la peur ce n'est qu'une forme d'activation de certains neurones de nos cerveaux et je serai du genre à parier qu'une ia qu'il y ait suffisamment de textes notamment des romans à suspense où l'auteur décrit la peur des personnages par exemple cette ia pourrait bel et bien finir par créer une sémantique de la peur dont la relation au contexte de la peur et isomorphe à ce qu'il se passe dans nos cerveaux au quelqu'un les sensations de peur chelia pourrait être physiquement indiscernable des sensations de peur chez un humain plus généralement on parle beaucoup de la nécessité de big data pour les ir pour comprendre le monde qui nous entoure sauf que des données massives et publics de grande qualité sont déjà disponibles par exemple sur wikipédia ou sur youtube et unia qui sait lire et écouter mais genre vraiment bien lire eh bien écoutez c'est à dire on se cherchant constamment à un ferrer un maximum de sémantique de ce qu'elle lit si cette ia ne cessait pas de lire et relire wikipedia d' écouter réécouter youtube je parierais centre au flipper que cette ia atteindrait un niveau de compréhension du monde bien supérieure à celle de n'importe quel humain elle comprendrait le sarcasme et l'humour elle distinguerez l'effet que mieux et les trolls et elle saurait mieux que vous comment améliorer sa propre intelligence et vous qu'en pensez vous unia qu'ils ne partiraient de rien si ce n'est un excellent algorithmes de machine learning et j'entends par là un truc bien plus performant que tout ce qu'on fait aujourd'hui plus parisien notamment une telle ya qui n'auraient accès qu'à wikipedia peut-elle devenir en tout point supérieure à l'homme avant de revenir à vos commentaires une petite annonce je ferai un youtube live si tout se passe bien mercredi donc ce mercredi là le 2 mai à 16 heures donc c'est de vous brancher en fait ce sera une conférence que je vais donner ici à le pfl qui sera se mirent informel et qui soit aussi soumis scientifique voire carrément hostiles aux scientifiques c'est un petit peu pour tester le matériel et le youtube blague puisque thibaut giraud viendra monsieur fille viendra à le pfl donné une conférence si on essaie d'organiser un youtube live également pour ça donc on est un petit peu le matériel mais du coup c'est aussi l'occasion pour moi de donner une conférence et en l'occurrence ça va je vais parler d'un truc que je trouve absolument fondamental même si ça se traite difficilement avec la méthode scientifique notamment puisque je vais parler de philosophie on va dire sur la différence entre à apprendre et comprendre qui est je pense une distinction extrêmement importante à faire tant d'un point de vue pédagogique que au niveau de la recherche notamment en ce moment avec les intelligences artificielles qu'on ne comprend pas on peut se demander comment penser ces intelligences artificielles et qu'est ce que pourrait vouloir dire que comprendre le fonctionnement des intelligences artificielles et pour répondre à toutes ces questions notamment je vais avoir un angle très algorithmique pouvoir vraiment utilisé la théorie de l'informatique et de l'algorithmique pour essayer d'avoir une réponse à cette question et je suis prêt à parier que la vie que je vais donner va être complètement différent de tout ce que vous avez entendu à ce sujet l'anr fois on avait parlé d'état cp2 svd vous avez une fois qu'ils avaient utilisé par les équipes qui vont chercher à résoudre le prix netflix pour prédire les notes que des utilisateurs donnera à des films notamment et pour y arriver il fallait un terme en place et les films les utilisateurs dans un espace de sémantique film utilisateurs et a priori dans secret présenté un art fois chaque note pour chaque film correspondait à quelque chose qui avait le même poids que n'importe quel autre note importe quel autre film pour placer les gens dans cet espace d'à sémantique et joseph me sente nous demande aussi justement une note pour un film qui est déjà très populaire ne pourrait pas avoir du coup moins d'influx ans que d'autres notes données par exemple à des films moins connue il ya deux pas dans cette question est la partie qui je pense est pas la question mais j'ai quand même répondre qui est ce que ces influences beaucoup la position du film par exemple titanic qui ont été notés pas énormément de gens bien a priori le problème c'est que le point films comme titanic eu beaucoup trop de notes par rapport à l'espace des sémantique qui est vision de dimension on va dire sans et du coup s'il y a un million de notes il ya en fait les contraintes sont incompatibles en fait les hyper plan ou est-ce qu'il voulut doit être compatible et à ce moment là on va essayer de trouver une position qui est autant compatible que possible avec les différentes notes données par différentes personnes et du coup donc ça revient reprennent d'optimisation et qu'il faut résoudre mais du coup à ce moment là chaque note d'un utilisateur va avoir une influence qui va être assez limité sur la position in fine et du film titanic dans l'espace des sémantique film utilisateurs donc l' influence pour les films populaires et très faible au plus on peut s'y attendre c'est par être pas très surprenant en revanche la question plus intéressant de savoir est ce que pour un utilisateur et fait qu'ils aient noté le titanic par exemple est ce que c est raisonnable dû donner la même importance qu une note qu'il a donnée à des films peu connus qui finalement exprime quelque chose de plus particulier finalement par rapport à la personne en question qu'a priori il dirait que svt prend pas tellement ceux-ci on compte 1 dans les deux cas à chaque note ça correspond à un hyper plan intersections des hyper plan et c'est peut-être un peu dommage ce qu'on pourrait peut-être vouloir dire c'est que mon ip repent avec une incertitude sur nous on est par rapport à cette hyper plan et finalement ça c'est exactement le problème qui se pose lorsqu'on essaie de faire svd aux problèmes de la sémantique entre les mots et les contextes puisqu'il ya des mots qui vont toujours toujours toujours apparaître et des mots qui vont peut apparaître et on veut d'une certaine manière donner plus d'importance à certaines apparitions de certains mots dans certains contextes c'est quelque chose qui est rouée faire c'est automatiquement par world culek notamment bref tout ça pour dire qu'en fait elle s est un peu un paradigme truc qui marche très très bien mathématiquement donc c'est pour ça qu'on ne présente plus et c'est généralement parce qu'il utilise est directement et puis il ya plein de bidouillages que l'on peut faire de svd pour soi l'adapter au problème soit faire en sorte que ça marche mieux notamment svd très gourmand au temps de calcul en tout cas potentiellement très gourmande en temps de calcul notamment parce que benjamin gates mais qui est pas forcément adapté pour les matrices rappelle creuse c'est à dire une matrice bien peu de notes d'utilisateurs pour un film bonne école je veux dire en général si on prend un film réutilisateurs l'utilisateur a pas vu ce film du coup je peux pas lui donner de notes donc dans le cas de matrix creusant fait svd assez gourmande en temps de calcul et c'est pour ça que des solutions comme world to make qui sont rapidement beaucoup plus préférable et qu'on utilise souvent des algorithmes et qui sont plus pas directement ceux de la dja binaire mais plus des algorithmes comme la descente de gradient stochastique dont on reparlera noms de futurs épisodes valérian réagit un truc que j'avais dit dans la vidéo que j'ai dit que je n'ai pas comparé les pommes et brahimi de sacs les pommes et les bananes ça je pense que si on veut vraiment dire s'il faut pas comparer disons des bananes avec des raisins plus compliqué et ça c'est finalement parce que un raisin on sent intuitivement que ce n'est pas vraiment comparable à une banane parce que en termes de taille ou des apports nutritifs tout ce que vous voulez et façon plus générale en fait ce problème notamment ce que des unités très différente donc si j'avais donné aussi l'exemple du nombre d'années d'existence d'une chaîne youtube qui se compte tours en 1 2 3 4 5 6 j'aimais pas plus que disent que rarement plus que 10 puisque 10 mai avant puisque vent on va dire très clairement puisque youtube si je m'abuse qui nous barrer au nombre de vue d'une chaîne qui peut se compter en millions voire en milliards pour est très conscient que si on fait une assez paix sur ce genre de données eh ben on voit que les variations selon la dimension nombre de vues seront beaucoup plus grande puisque elles varieront les variations seront de l'ordre du milliard alors que si je regarde les variations selon nombre d'années n'a quasiment pas du coup automatiquement la cpval ignorer la donner sur une ombre d'année c'est pas parce que les nombres d'années n'est pas une variable pertinente c'est parce que l'échelle qu'on a choisi en nombre d'années fait que les variations sur cette échelle sont de l'ordre de 1,2 enfin 5 quelque chose comme ça si on avait repris cette même dimension on avait changé l'unité dont était passé en seconde par exemple en nanosecondes en temps de planck d'existence tout de suite ça fait désirer complètement différente et la cp aurait complètement changé à son interprétation et se serait mise complètement ignoré les variations selon le nombre de vues peuvent de façon plus générale des conclusions de la cpd pente très fortement des unités qui sont choisis et avec certaines unités de la cp peut être amené à ignorer les variations qui en fait sont extrêmement pertinente pour d'autres applications en particulier si je reprends le cas dans mon assiette en fait il est possible que pour certaines analyses ils soient vraiment primordial de prendre en compte les variations selon cette direction et auquel cas en fait la cp manquerait vraiment quelque chose y compris lorsque les unités de mesure dans les différentes dimensions sont les mêmes a cp a ses limites ça veut dire qu'il faut jamais utiliser mais faut savoir aussi que la cpa certaines limites et qu'il ne va pas être adapté dans certains cas ceci étant dit dans d'autres cas ça marche vraiment du tonnerre notamment dans le cas de son application pour svd pour résoudre le prix netflix enfin je vais me permettre de donner encore un peu plus de détails sur les fonctionnements de la cp et de sauver des fêtes dans les deux cas ces deux algorithmes repose énormément sur la notion d' orthogonale it et contrairement à la régression linéaire et la notion monde orthogonale it et en particulier pour ses différentes matrices à cette propriété assez fondamental que si vous prenez une matrice qui a certaines propriétés notamment si elle est symétrique donc si on trouve des directions qui sont bien en un certain sens à ce qu'ils sont directement dans le plus grand des relations ils sont en particulier des vecteurs propre une certaine maîtrise on appellerait de convergence alors en gros tout ce qui se passe de façon orthogonale qui est perpendiculaire à cette direction va rester de certains venir indépendant de cette direction et donc les variations vraiment d'un nuage de points dans une direction orthogonale dans toutes les directions orthogonale à sa direction de variation principal toutes ces directions lattois assez indépendante ans à 60 ans fait son non corrélées aux variations dans l'autre sens et c'est un peu la clé de s&p est celui qui est la clé en fait une des théories fondamentales qui est sur lesquels reposera 6 p à savoir le fait que tout motrices symétrique et au diagonal is able embase orthonormé si vous prenez un vecteur propre d'une matrice symétrique en effet vous pouvez prendre n'importe quels vecteurs orthogonale à ce lecteur propre l'image de sept de ses lecteurs qui orthogonale aux vecteurs propre par la matrice symétrique bon bah il va il va tourner quelque part mais cette image sera encore orthogonale aux vecteurs propre et ses lecteurs propre là il ya un changé entre que sa direction ayant changé par la matrice ses métriques par des filles sont des vecteurs propres et c'est ça qui fait que tout matrice symétrique et diabolisent abl blazer très normé et notre bonne chose par rapport à la cbc que la matrice de covariance rs quand elle se mit défini positive mais bon je pas disserter plus encore sur tout ça mais cette propriété de matrix matrix me définis positive c'est un truc qui apparaît énormément notamment en optimisation puisque c'est lié à son comble c'est qu'ils ont du coup facile à résoudre bref il y aurait énormément plus à dire tout ça pour vous dire que la diabline hier que vous apprenez ans bakulin au bac + 2 c'est un truc qui en fait fondamental en machine en ligne et j'espère que vous avez aimé cet épisode il reste en fait deux épisodes encore sur la sémantique la prochaine fois on va creuser un petit peu cette histoire de sémantique des mots et pas que des mots d'ailleurs aussi la sémantique des images et on va voir que notamment on peut faire de l'arithmétique sur les cinématiques ce qui fait un truc assez mind boeing notamment les chercheurs qui ont découvert ça grard par qui on sent la surprise qu'ils ont ressentie au moment de découvrir que il avait réussi à faire ça alors que c'était pas du tout objectif originale de leurs articles si vous avez aimé cet épisode penser à le lac et à le commenter à le partager pensez à vous abonner pour ne pas manquer les futurs épisodes merci aux ti peur pour leurs dons et j'espère que vous serez là la prochaine fois et terme en 2011 et un mec s'appelle tenenbaum yahoo qui a publié un article où il présente trois images d'un truc qu'il appelle tu fais ça veut rien dire les images représentent rien de réel il dit bon bah ça c'est un tu face à saintes uva et ça c'est un tu fais ensuite il montre 39 image et il nous demande de trouver les tu vas tu vas sont pas les mêmes c'est des objets c'est comme des animaux que des plantes est ce qu'il faut c'est que on est tous d'accord dit ce texte sinon il lui a dit ils donnent tous les fraudeurs ce trio magique