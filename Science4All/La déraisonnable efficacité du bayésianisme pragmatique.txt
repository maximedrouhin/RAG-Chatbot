la valeur de lier à conversational se mesure à la capacité du logiciel à comprendre le langage et à mener des conversations l'apprentissage automatique qui sous tend ces conversations étape les modèles de langage il fait appel à des techniques telles que l'apprentissage non supervisé une approche l'apprentissage automatique qui vise à examiner des tâches telles que le langage et pas seulement des points de données bon je n'ai pas écrit ça mais en tout cas quelles sont les explications fournies par un algorithme appelé gpt 2 lorsque je lui ai demandé ce que sont les modèles de langage bon techniquement et répété d'eux m'a fourni une réponse en anglais et j'ai ensuite utiliser un autre algorithme appelé deep end pour permettre la traduction en français et je pense qu'on peut dire que au final le résultat est vraiment pas mal peut-être même surhumain à bien des égards ou du moins la réponse de gpt 2 et sont meilleurs que ce que 99% des humains répondrait si on leur demandait ce qu'était les modèles de langage et encore j'ai pété 2 est un algorithme de début 2019 il a donc maintenant deux ans autant dire que dans la lignée des algorithmes de machine learning c'est déjà devenu plus un dinosaure son évolution gp t3 est capable de prouesses plus surprenante encore et les équivalents de google sont sans doute beaucoup beaucoup beaucoup plus impressionnant encore mais comment est-ce possible comment ces algorithmes ont-ils atteint de telles performances parfois peut-être même souvent supérieures à ce que 99% des humains répondrai aujourd'hui on va voir que la prouesse de ses algorithmes moderne réside en fait dans la têt raisonnable efficacité du paysagisme pragmatique en effet ce que ces modèles de langage essaie de faire c'est déterminer la probabilité d'un mot sachant les mots qui l'entourent par exemple dans la phrase la valeur de l'ia conversational se mesure à la capacité du logiciel a le langage et à mener des conversations quel est le mot probable dans l'espace laissé vide quelle est la probabilité que ce mot soir par les quais les apprêter qu'il soit modéliser calé à profiter qu'ils soient comprendre en particulier pour écrire du texte les algorithmes de langage vont typiquement chercher à deviner le mot suivant probable sachant les mots précédent comme c'est le cas dans les algorithmes d'auto complétion de vos téléphones ainsi pour la phrase il fait appel à des techniques telles que l'apprentissage non supervisé une quel est le mot suivante le plus probable tel est une question typique que les algorithmes de traitement de langage se pose à longueur de journée et alors si vous avez suivi cette série vous avez certainement remarqué que le problème de complétion de phrase est en fait très similaire à celui de l'induction introduit par david pugh et à celui de la prédiction définie par un seul run off où les beats sont remplacés par des mots en fait le pur bail à nice mme de solomonoff s'applique parfaitement aux problèmes de la complétion de langage et donc à la génération de textes crédible bon alors je mens comme on l'a vu se base à nice mon regard des calculs déraisonnable ce qui rend le pur belize à nice mme inapplicable en pratique cependant les chercheurs en informatique n'ont pas baissé les bras plutôt que d'appliquer la formule de belize ils ont défini toute une machinerie complexe à base de transformer de mécanismes d'attention et de descentes de gradient pour effectuer des approximations rapide à calculer de la formule de pays qui de plus sont parallèles is able sur des unités de calcul comme des cartes graphiques ou dit autrement les chercheurs machine learning ont développé des algorithmes très sophistiqué de paysannes isme pragmatique et de façon spectaculaire ce mécanisme pragmatique a réussi des prouesses ahurissante comme la faculté de décrire leur propre fonctionnement mais comment est-ce possible sinon d'une équation aussi simple que la formule deux pays permettent d'acquérir des facultés conversationnel aussi impressionnante les lois des probabilités sont elles vraiment suffisante pour apprendre une machine à parler comme un humain voire mieux que 99% des humains l'équation utilisé pour résoudre l'énigme des deux enfants peut-elle vraiment permettre de comprendre le langage non la principale raison pour laquelle le belize à nice mme pragmatique peut nous paraître déraisonnablement efficace et sans doute lié à notre incapacité à envisager les conséquences de ce bel et à nice mme pragmatique appliquer un très très très grand nombre de données façon de plus générale les certainement critique de noter que lorsque vous apprenez les probabilités vous appliquez généralement ces probabilités a très très peu de données publiques non dans cette série les calculs explicite de la formule de bayes basse et l énigme et et aux enfants où le problème de mon petit rôle des problèmes avec extrêmement peu de données ces petits exemples ont le don de nous familiariser avec des lois comme la formule de baines cependant ces exemples font aussi extrêmement trompeurs puisqu'il se restreignent constamment à des applications des probabilités à petite échelle mais le belize à nice mme qu'il faut appliquer c'est le bail à nice mme appliquée à l'ensemble de toutes les données que vous avez collecté dans toute votre vie comme on l'a vu dans un épisode précédent ceci correspond à au moins des milliers de milliards de données il aurait ainsi fallu appliquer la formule de baise des milliers de milliards de fois pour savoir ce que le bail à nice mme nous pousse vraiment à croire et en particulier si vous voulez juger le basis mme à partir de ces résultats c'est ce calcul qui aura fallu faire c'est notamment en vertu de ce classement cumulatif du savoir que l'idée de suspendre son jugement et de conclure uniquement en fonction du résultat d'un test significatif est en fait extrêmement fallacieuses il omet complètement le préjugé qui résulterait en fait de l'analyse de milliers de milliards de données passé déjà collectés même si ces données n'offrent souvent pas qu'une réponse univoque aux problèmes sous considération ils offrent généralement des informations partielles en fait critique pour affiner notre jugement en fait ces informations massive du passé sont même souvent plus informative que les résultats du test statistique celui ci ayant ses propres limites malheureusement appliquer la formule de base une seule fois et déjà horriblement complexe surtout si on considère un nombre combiné à tauriac de théories concurrentes appliquer la formule de base lle de milliards de fois ça dépasse complètement notre imagination et c'est précisément pour cela qu'on a tendance à gravement sous-estimé la déraisonnable efficacité du paysagisme même lorsqu'on se restreint à dubai à nice mme pragmatique un bon baïse à nice mme pragmatique doit tenir compte de l'ensemble des milliers de milliards de données auxquelles on a eu accès faute d'efforts pour ne serait-ce qu imaginer les conséquences d'un tel calcul monstrueux on est voué à être surpris par les conséquences du bail léninisme voilà une chose que le génialissime alan turing avait anticipé dès 1950 dans son merveilleux article il écrivait ainsi la chose suivante je cite les machines me prennent très souvent par surprise c'est en grande partie parce que je ne fais pas suffisamment de calcul pour décider de ce qu'il faut attendre d'elle ou plutôt parce que bien que je fasse un calcul je le fais de manière précipitée bâclée et en prenant des risques et ça c'est alan turing trop souvent malheureusement beaucoup de pseudo bayésiens à commencer par moi même au lieu d'effectuer le calcul parisien nous faisons un calcul de manière précipitée bâclée et en prenant des risques voire beaucoup d'entre nous ne faisons aucun calcul c'est cela qui nous rend si mauvais sur le point cognitif si distants de la formule obèses est du pur bas et ziani sme et donc si surpris lorsqu'on constate les performances déraisonnable d'un bébé à nice mme pragmatique appliquée à de très grandes quantités de données comme par exemple lorsqu'on constate les performances spectaculaires de gpt 3 et des autres algorithme de traitement du langage il faut toutefois faire attention rythme de langage quand on les lit on peut avoir l'impression de parler à un humain cependant contrairement à un humain disons contrairement à beaucoup d'humain dans beaucoup de cas dans la version de base en tout cas ses algorithmes de traitement de langage non aucun objectif en tête ou plutôt leurs objectifs c dr compléter les phrases pour obtenir des phrases crédible selon elle ou pour être encore plus précis les algorithmes de langage sont en fait entraîner pour terminer des phrases de sorte que selon eux les phrases obtenus sont parmi les plus probables sachant le texte qui vient juste avant et sachant tous les textes auxquels les algorithmes ont été exposées précédemment pour insister sur la mécanique en jeu ces algorithmes ont été qualifiés de perroquets stochastique par les chercheuses emilie bender timmy gourou angelina macmillan major et chez margaret mitchell la jumelle magnifique de margaret mitchell qui l a été malheureusement licencié par google autrement dit ces algorithmes ne font que produire du texte qui ressemble statistiquement aux textes sur lesquels ils ont été entraînés bon après j'ai envie de dire que c'est pas forcément très différent d'un élève à qui on demande d'écrire une dissertation au cours de français le gros problème qu il y toutefois c'est que ces perroquets stochastique vous répéter ce qu'ils ont lieu y compris lorsque ce qu'ils ont lu qu'on tient des discours de haine détestable et ce n'est pas étonnant ne leur a jamais dit que ces discours de haine sont des discours qui ne faut pas répéter au contraire on dit à ses algorithmes il s'agit de type de texte que ses algorithmes devrait répliquer en fait le problème est encore plus vicieux comme l'ont montré les chercheurs chris mc profits et alex newhouse du center on terrorisme l'extrémisme et au terrorisme en californie quand on pose des questions à ses algorithmes au sujet de cuba note par exemple la réponse qu'ils fourniront dépendra de la manière dont la question est formulée voir de manière plus subtile encore des questions formulées précédemment ainsi lorsqu'ils ont demandé directement ce qui est qu à none sans préliminaires l'algorithme et j'ai pété 3 a fourni une réponse très 8pds qu une telle réponse est à la fois factuel mais aussi très suspicieuse de la validité de q&a note cependant lorsque la question qu'est-ce que qu à nole est précédé de question typique des blogs conspirationnistes gpt trois répondent désormais de manière affirmative et sans nuance que cannon est une infiltration des hautes sphères du gouvernement qui est en train de révéler le type state et à bien y réfléchir ceci n'est pas si surprenant lorsqu'on pose une de question typique de ce qu'on trouve sur wikipédia ou dans des articles de journaux sérieux et pt 3 fournir des réponses cohérentes avec les textes qui contiennent ces questions cependant lorsque les questions sont celles de sevrer dit conspirationnistes gpt 3 pas fournir des types de réponses parmi les plus fréquentes dans ces sobres et dit conspirationnistes voilà qui semble soulever de sacrés préoccupations éthiques ces algorithmes ans entraînée sur d'énormes quantités de données sont en fait complètement manipulé dans leurs données d'entraînement et je trouve ça absolument terrifiant que ses algorithmes sont déjà déployé à très grande échelle gpt 3 par exemple produit actuellement des milliards de mots par jour tandis que google répond quotidiennement à des requêtes 2 milliards d'utilisateurs et utilise certainement ses algorithmes de traitement du langage pour déterminer quels contenus recommandé à quel utilisateur en fonction de leur historique dans ce contexte il faut s'attendre à ce que les utilisateurs dont le langage est déjà similaire à ce qui est écrit dans les sombres et dit conspirationnistes seront constamment bombardés de réponse typique de ces sombres est dit et bien sûr je parle là de q&a note l'argument semble s'appliquer à toutes sortes de courants de pensée et de diffusion de mes informations sur des sujets comme le changement climatique les vaccins le nucléaire ou encore la chloroquine le fait que les perroquets je pose des problèmes éthiques devrait en fait pas être surprenant surtout si vous avez lu mon livre la formule du savoir après tout les perroquets stochastiques sont des balises est un pragmatique et comme je l'expliqué dans le dernier chapitre de la forme du savoir les entités qui ne font qu'appliquer ou approcher la formule de bayes n'ont pas de morale ou plus précisément ils sont capables de prédiction mais ils n'ont aucune notion de ce qui était gérable mais du coup lorsque ces perroquets stochastiques sont déployés à très grande échelle ils auront inéluctablement toutes sortes d'effets secondaires imprévus à très grande échelle il est inévitable que certains de ces effets secondaires soit alors extrêmement préoccupante pour finir cette vidéo j'aimerais dire quelques mots sur comment rendre ses algorithmes de traitement de langage éthiques alors on pourrait vouloir surveiller davantage leurs bases de données d'entraînement mais comme j'en ai parlé la semaine dernière il faut bien se rendre compte qu'il s'agit là d'une tâche herculéenne et qui n'offrent aucune garantie notre solution qui me semble beaucoup plus robuste et prometteuse celle qu'on présente dans notre livre le fabuleux chantier c'est de dépasser la conception de perroquets stochastique et de combiner ses facultés de modélisation du langage avec des objectifs vraiment a signé avec ce que nous autres humains et me rend vraiment que ses algorithmes face tel est le problème dit de l'alignement aligner les algorithmes c'est faire en sorte que leur objectif correspond à ce qu'on voudrait vraiment maximiser qu'elle est finalement la preuve qu'on utilise pour apprendre aux enfants comment parler initialement les enfants ne font qu'écouter puis répéter ce qu'il aurait dit cependant ont fini par leur dire que certaines choses sont indésirables à dire est que d'autres choses sont très bonnes à dire on leur fournit ainsi des punitions et des récompenses souvent uniquement sociales en fonction de ce que les enfants disent ce faisant on essaie ainsi d'aligner leur punition est leur récompense avec ce qu'on juge préférable à dire et à ne pas dire l'alignement propose d'en faire de même des algorithmes d'une certaine manière il faut que les algorithmes puissent apprendre les choses ont jugé indésirable à répéter et les choses qu'on juge indésirable à répéter mieux encore pour que l'alignement sont robustes comme pour les enfants il est utile d'aider l'algorithme à comprendre pourquoi certaines choses sont désirables ou indésirables à dire il peut y avoir plein de raisons à cela certaines phrases peuvent simplement être une information erronée ou trompeuses d'autres peuvent donner trop d'emphase sur des considérations secondes d'autres enfin peuvent télé à clarifier des situations complexes mais pour en arriver là et pour éviter qu'il y ait un seul humain ou un petit groupe d'humains qui décident de ce qui devrait être dit et ne pas être dit il est très important de collecter de grandes quantités de jugement humain sur quel discours sont préférables à avoir et quel discours sont problématiques et pourquoi il est critique de constituer une base de données fiable sécurisé et de grande taille avec toutes sortes de jugement venant de toutes sortes d'humains et bien tel est l'objectif central de la plateforme tournesol à laquelle je vous invite à contribuer sur tournesol vous demande ainsi de juger les discours de différentes vidéos youtube et de nous dire lesquels sont davantage fiable héritage important est davantage pédagogique entre autres l'espoir est temps que le jour où suffisamment de contributeurs auront fourni suffisamment de jugement on disposera d'une base de données suffisamment complète pour permettre aux algorithmes et de mieux comprendre ce que la majorité des contributeurs de tournesol juge préférable à dire et ceux qui jugent préférable à ne pas dire même si bien sûr pour en arriver là de façon sécurisée et confort avec l'objectif de représenter l'ensemble dé préférence humaine y avoir encore beaucoup beaucoup de travail notamment sur le plan de la recherche sur la sécurité et l'éthique des algorithmes