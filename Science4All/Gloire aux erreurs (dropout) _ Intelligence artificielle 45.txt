les réseaux de neurones artificiels sont en partie inspiré des réseaux de neurones biologique ce qui compose notamment nos cerveaux de primates sauf que bien sûr il ya des différences notables et même énormément de différences notables en fait les biologistes sont très souvent en mode vénère quand on commence à comparer l'un et l'autre l'une des différences importantes a priori c'est que les neurones biologiques sont des machines très imparfaite il aura arrive de dysfonctionner alors que les neurones artificiels a priori et au moins en principe ils sont capables de ne jamais faillir et ça ça donne l'impression que le réseau de neurones artificiels pourraient surpasser les réseaux de neurones biologique à moins que à moins que le dysfonctionnement des normes biologiques soit non pas une faiblesse mais au contraire c'est une force et bien en fait oui l'une des découvertes stupéfiants de la recherche une intelligence artificielle c'est qu'il est souhaitable de faire dysfonctionné de temps à autre les neurones artificiels nous encore introduite en 93 parquet relisant et valais l'idée d'ajouter artificiellement des dysfonctionnements des neurones artificiels aussi appelé drop out a été l'une des idées clés de l'article du jo farrington et de ses collaborateurs qui en 2012 allait bouleverser la recherche dans le domaine et a annoncé l'avènement du dip learning mais comment est-ce possible comment est ce possible d'ajouter des dysfonctionnements dans l'apprentissage de nos machines puisse leur permettre d'être plus performante et est-ce une règle générale alors pour commencer non ce n'est pas une règle générale si vous écrivez un code surtout en cryptographie ou pour des systèmes de sécurité ne fait pas exprès de rajouter des bugs et surtout n'allez pas dire à votre prof que votre copie contient des erreurs parce que les 200 vols a dit qu'il est souhaitable d'en créer artificiellement nom située plutôt d'aller voir lui-même les vidéos de ce forum et ne pas oublier de lâcher un pouce bleus et de s'abonner et et la cloche aussi en général faire d erreur dans un programme ça empêche le programme de bien fonctionner mais bizarrement si vous programmez réseaux de neurones alors il est en effet souhaitable de créer artificiellement des bugs aléatoire dont le fonctionnement des neurones du réseau en fait cette stratégie est même terriblement efficace et aujourd'hui on va essayer de voir pourquoi en particulier je vais vous proposer trois explications de ce succès de drop out qui toutes trois ont à voir avec le problème de la surinterprétation dont j'ai parlé dans les épisodes 11 à 18 souvenez-vous le problème de la surinterprétation aussi appelé over fitting c'est qu'on ne disposent souvent que d'un nombre restreint de données d'entraînement or un algorithme d'apprentissage cherche uniquement à expliquer ces données d'entraînement mais alors en se concentrant uniquement sur ces données il pourrait identifier des explications qui n'explique que ces données et qui pourrait très mal se généraliser à des données autres que les données d'entraînement et ça c'est un phénomène qu'on observe beaucoup en pratique surtout si le nombre de données n'est pas beaucoup plus grand que le nombre de paramètres de apprentissage et cette remarque d'ailleurs elle n'est pas restreinte aux machines landing du coup pu faire eh bien on a vu dans l'épisode 13 que pour combattre la surinterprétation on pouvait simplement cherché à avoir plus de données d'entraînement mais supposons que l'on est accès qu'aux données qui ont été collectés pour nous est bien là au lieu de chercher à collecter plus de données on pourrait chercher à créer des données à partir de cette que l'on a typiquement si on veut apprendre à reconnaître des chats à partir d'une image de chat on peut effectuer de légère rotation des zoom dézoom des petites modifications de la balance des blancs ou encore effectuer une image miroir de la photo pour ainsi obtenir de nouvelles données dont on peut encore raisonnablement penser qu'il s'agit d' images de chats on parle alors d'augmentation des données ou data augmentation cette technique permet ainsi d'avoir plus de données d'entraînement et de réduire le risque de sur interprétations et je vous invite d'ailleurs à l'utiliser vous même avec des textes que vous lisez n'hésitez pas à de temps en temps réécrire les textes que vous lisez ou moins dans votre tête nous en changeant les paramètres a priori non pertinent à l'apprentissage comme notamment les notations utiliser ce qui est particulièrement utile pour apprendre les mathématiques ou encore la connotation des mots utilisés ce qui est souvent très utile pour éviter de se laisser persuader pour des mauvaises raisons et bien en un sens ce drop out peut être vu comme une automatisation de ces techniques d'augmentation des données en effet les bugs et nourrit observable correspondent à apprendre des données légèrement corrompu et le réseau de neurones doit pouvoir fournir une réponse malgré cette petite corruption des données de façon plus intrigante encore les bugs des nouveaux intermédiaires peuvent correspondre à des déformations plus abstraite de l'image comme ces histoires de rotation de balance des blancs qu'on imagine plus tôt alors bien sûr cette remarque est plus intuitif qu'autre chose puisque il est très difficile de savoir ce qui se passe vraiment dans un réseau de neurones que celui ci soit artificielle ou biologique une autre explication du succès de drop out vient du fait que les données sont nécessairement que des mesures et quand cela elles sont souvent imparfaite typiquement nos appareils de mesure des formes légèrement les signaux qu'il mesure on dit que les données sont brutes et mais alors nous algorithmes d'apprentissage qui chercheraient à parfaitement expliquer les données brutes rond de bonnes chances d'apprendre la description rigoureuse du bruit plutôt que d'apprendre des données il risque donc de donner trop d'importance aux aléas de la mesure plutôt qu'à la nature du signal sous jacent pour éviter de tomber dans le piège de la surinterprétation du bruit il est alors utile de supposer que les données observées sont en fait des perturbations des vraies données et que ce sont ses vraies données inconnues que l'on cherche vraiment à décrire et à expliquer cependant on voit alors qu'il ya nécessairement une incertitude sur ses vraies données à décrire en effet si les données mesurées sont égales aux vraies données plus bruit alors on peut retrouver les vraies données en calculant données mesurées - bruit alors bien sûr le bruit demeure inconnue mais les bruits attendues vont nous permettre d'estimer les valeurs probable des vraies données et bien drop out peut être vu comme une manière d'explorer l'ensemble des valeurs possibles des vraies données ainsi plutôt que de chercher à expliquer les données mesurées notre algorithme d'apprentissage doté de drop out cherchera à expliquer les vraies données probable sachant les données mesurées notez que ce raisonnement est très proche de la notion d'ensemble d'incertitude de l'épisode 18 dont on a vu par ailleurs qu'elle était isomorphe a ajouté une régularisation c'est à dire une sorte de principe de parcimonie à notre algorithme d'apprentissage et bien cette isomorphisme montre ainsi qu'étrangement 3 août n'est pas si différent du rasoir d'occam qui dit que les modèles plus simples sont plus probables a priori à isomorphisme près drop out peut donc être interprétée comme une façon d'implémenter ce rasoir d'occam là encore cette remarque est intéressante pour la manière de réfléchir de nous autres humains quand on est confronté à certaines données il faut préférer les explications qui reste valide même si ces données sont en fait erronée on dit parfois que les explications avancées doivent être revues notamment par exemple à la découverte de biais dans les données mesurées autrement dit c'est quand même vraiment cool de sortir des arguments de la forme il me semble probable que x et alors on a clairement y mais de façon intrigante même sinon x alors y restent très probables ce genre d'argument est vraiment très convaincant pour amener quelqu'un croire y enfin il ya une dernière explication du succès de drop out qui est de loin ma préférée souvenez vous pour lutter contre la surinterprétation on pouvait soit appliqué le rasoir d'occam de l'épisode 18 soit faire grossir toute une forêt de modèles mutuellement incompatibles comme dans l'épisode 17 oui parce qu'une forêt de modèle est plus sage que n'importe quel arbre de la forêt hashtag proverbe baïse ayant en effet intuitivement même si un arbre se met à faire n'importe quoi la forêt dans son ensemble restera robuste plus formellement prendre la moyenne de modèles incompatibles est vraiment l'un des piliers du bail à nice mme on appelle ça la loi des probabilités total comme j'en parle très régulièrement dans mon livre du coup plutôt qu'à prendre une fonction calculé par réseaux de neurones on peut imaginer qu'on dispose en fait d'une forêt dont les arbres s'obtiennent en éliminant aléatoirement une fraction des neurones la prédiction de la forêt sera alors la moyenne des prédictions des arbres c'est à dire des prédictions faites par le réseau lorsqu un sous-ensemble aléatoire de neurones et mis hors service autrement dit au moment d'effectuer une prédiction on prendra bien suffisamment en compte l'avis des différents arbres de la forêt c'est à dire le cas où différents sous-ensembles des neurones bugs simultanément et cerise sur le gâteau l'apprentissage de cette forêt de réseaux de neurones est implémenté automatiquement par la descente de gradient stochastique où jacques pas de gradient stochastique est calculé pour une donnée prise au hasard et un arbre pris au hasard autrement dit drop out se marient à la perfection avec l'algorithme phare de l'exploration d'un très grand nombre de paramètres guidés par des données ainsi si je tulle cette technique toute simple qui est drop out qui consiste tout bêtement à programmer la vulnérabilité passagère des neurones artificiels est en fait une technique redoutablement efficace pour lutter contre la surinterprétation puisqu'elle permet à la fois de lutter contre le manque de données d'apprentissage contre les erreurs des données d'apprentissage et contre l'apprentissage d'un modèle unique plutôt que l'apprentissage de toute une forêt bref les boulettes des neurones ce sont en fait des erreurs très intelligente la nar fois on avait fait l'analogie entre la descente de gradient du machine learning et descendant une pente lorsque j'en ai en montagne on avait vu que ça pouvait poser problème parce que souvent quand on descend selon la pente en montagne va en finir au fond d'un lac plutôt que dans la fosse des mariannes donc dit comme ça c'est pas forcément problématique mais bon en machine learning ça pose problème et vous avez été nombreux à réagir notamment une dimension à l'idée de pourquoi pas commencé de façon un petit peu aléatoires pour essayer d'explorer l'espace des paramètres avant de commencer vraiment la descente de gradient et aussi des idées à base de commencer à différents points initiaux faire plusieurs descentes de gradient avec différents points initiaux tout ça ce sont des techniques qui sont en fait à implémenter très souvent dans les algorithmes de machine learning notamment cette seconde idée qu'on appelle aussi le multi start de façon équivalente le multi star ça leur revient a commencé avec plusieurs réseaux de neurones initiaux dont l'initialisation et aléatoire et ensuite à regarder ce cap à une ses différents réseaux de neurones et on peut vouloir du coup garder le meilleur de ses réseaux de neurones après avoir attrapé en tissage ou peut-être mieux son corps ce qu'on peut et devrait faire c'est prendre la moyenne de ces réseaux de neurones ce qui est vrai à une touche un peu plus bélizienne ceci étant dit il faut imaginer que dans les espaces de très grandes dimensions le nombre en gros de configuration est vraiment exponentielle dans la dimension et du coup pour avoir un réseau qui va tomber vers le minimum global faut vraiment un nombre exponentiel de réseaux de neurones dans la dimension nous passions concert des dimensions 1 milliards si on a un milliard de par m eh bien il faut à peu près de puissance un milliard de réseaux de neurones initiaux en ordre de grandeur en tout cas pour espérer atteindre un minimum global autant dire que c'est un peu complètement mort vraiment je pense que imaginer qu'on puisse obtenir un minimum global c'est c'est vraiment pas papa faisable en très grande dimension lorsque des problèmes donc complexe comme c'est le cas en pratique pour la plupart des problèmes de machinerie de fullmetal alchemist demande à ce qu'il ya une différence entre la descente de gradient ducatel proc programmés dans les réseaux de neurones et le fait d'avoir une balle qui tombe qui tombe l'apprendre qu'ils descendent la pente en retombant en fait ce sont deux équations très différentes puisque lorsque la balle tombe en fait la pente va effectuer va agir uniquement sur l'accélération de la balle et pas sur sa vitesse directement alors que la descente de gradient c'est vraiment sur la vitesse sur le déplacement et donc une balle en fête à va gagner de l'inertie et elle va par exemple elle peut ainsi traverser un lac et donc voilà un pays magique avec l'inertie elle peut remonter et traverser le lac et en s'inspirant soit cette idée du d'ailleurs il ya des gens qui ont proposé des variantes de descente de gradient ou pareil en fait le gradient va affecter l'accélération du déplacement dans l'espace des paramètres plutôt que la vitesse directement de façon générale comme vous pouvez l'imaginer énormément de recherches sur les différentes variantes qu'on peut faire autour de la descente de gradient est-ce qu'on peut pas faire de petites optimisations il ya beaucoup beaucoup d'idées il ya beaucoup de choses honnêtement je vais pas impression moi personnellement qui est un truc qui semble vraiment émergé plus tôt que les autres key a pas de très très bonne théorie assure qu'elle est la meilleure descente de gradient possible alors peut-être que du coup il faut un peu plus de recherche dessus si je pense qu'il ya eu pas mal de recherches dessus et de façon empirique également on n'a pas j'ai pas l'impression qu'on observe en tout cas une méthode dominés sur les autres en gros à présent c'est un peu au cas par cas voilà on fait une simulation de temps en temps ça ça marche mais tu entends ça ça marche mieux mais très difficile je pense qu'en tout cas ma connaissance a priori de savoir quelle variante de la descente de gradient va être la plus adaptée à un problème donné valérie on lui mentionne l'algorithme de recuit simuler qui inspirait de la physique et l'idée du recuit simuler qui est très proche en fait qu'une barre d'entre eux mêmes on peut dire d'un cas particulier même de des algorithmes par marco qui monte carlo dont je parle dans mon livre et en particulier de metropolis fasting désolé de faire c'est un peu de name dropping mais sont différentes idées qui sont assez similaires et à chaque fois l'idée de ces méthodes c'est d'avoir une exploration des paramètres où on peut s'autoriser des pâquis sont pas productifs d'une certaine manière des caïds et des papes qui nous font remonter et temps en temps en fait on va même avoir des phases où on va s'autoriser pas mal de ses pas de ces mauvais pas qu'ils me font remonter la pente histoire d'avoir un petit peu d'exploration et puis ton temps on va vraiment vouloir descendre on va rejeter un peu c'est pas qui remonte et on veut vraiment essayé de descendre ça finalement c'est quelque chose que l'on voit aussi apparaître dans la descente de gradient stochastique ou là le côté aléatoire possible remontée de la pente bien du fait que la super boussole dont on dispose donne des directions qui sont un peu aléatoire et vont globalement dans le bon sens mais parfois il nous faut un peu remonté la pente et on dit souvent informellement que ce genre de méthodes permettent de sortir des minima locaux alors moi personnellement je mettrai pas mal de réserves à ça parce que ça me paraît pas ultra convaincant je pense que l'intuition en dimensions 1 et une dimension de assez bonne on voit qu'on dit long sur l'évolution de sa marche mais j'ai quand même l'impression qu'en très grandes dimensions il se passe beaucoup des choses qui sont beaucoup plus subtil et beaucoup plus sophistiqué en particulier à un truc qu'on observe à lacets empiriquement c'est que ces méthodes notamment de metropolis casting on mettre beaucoup beaucoup plus de temps à converger parce qu'on fait en gros on fait beaucoup beaucoup de pas inutile ou de propositions de prades pas assez c'est peu productive et du coup dans des paysages qui sont vraiment très très grand et très long à explorer d'une certaine manière il me semble qu'à voir c'est une boussole de descente de gradient est déjà largement très très bonne idée quoi qu'elles soient un peu stochastique en plus peut-être peut aider mais je suis je mettrai pas vraiment un scoop et là dessus je pense que la décence de grayan stochastique est vraiment intéressant dans le sens où elle est adaptée à l'album devrait être aux populations notamment et en plus la chance d'être adaptée à ces idées des drop out donc j'avais plus que c'est ça le côté vraiment avantageux d'avoir cette descente qui ne sont pas exactement de gradient et qui ont un côté un peu aléatoire et il ya une autre raison pour laquelle le côté stochastique est intéressant c'est que ça a empêché la convergence vers un unique modèle et sa force à explorer un petit peu différents modèles et si on vend à ce moment là on regarde les prédictions du réseau de neurones lorsqu'on moyenne par rapport à 10 ces différentes variations des paramètres du modèle là on obtient un truc un peu plus bas et bien et je pense que c'est plutôt ses idées qui font que l'idée de gradient stochastique est meilleur que des centres de gradient mais je suis pas tout à fait convaincu par l'idée mais peut-être que si vous avez des références à suggérer ça pourrait marcher sur l'intérêt serait de savoir si en grande dimension dans des paysages non convexe le fait d'avoir un truc qui n'est pas vraiment une descente de gradient permet vraiment d'avoir une meilleure meilleurs algorithmes d'apprentissage moi personnellement pour l'instant je ne suis pas convaincu mais jamais tout à fait aussi l'étendue de mon ignorance et si vous avez des trucs intéressants à proposer je serai ravi de lire ça me permet aussi de revenir sur une histoire que j'ai l'impression d'avoir mal à expliquer j'ai pas été très convaincantes que l'anr fois sur le fait que en très grande dimension à ce qu'on a un point celle on a de très bonnes chances de rester bloqué sur ce pont celle surtout aussi 99 % des directions plus exactement des espaces propres de la matrice et ciel 6 99% de cette direction vont vers le haut un bond vers l'océan correspondent à une auto gratuit donc une ligne de creux sicile 99% de cette direction correspond en gros à des mauvaises directions en fait une très bonne chance de rester bloqué sur ce point un argument qui serait plus convaincant pour voir ça c'est imaginer que lorsqu'on prend une direction aléatoire c'est un correspondant à une combinaison linéaire des différentes directions et du coup on peut imaginer que la contribution de chaque direction correspondent une variable aléatoire duo entre - rien disons que les valeurs propres des maîtres et sociales ne sont pas très différents les uns des autres et le truc c'est qu'il ya beaucoup plus de ces directions qui sont des lignes de crête c'est à dire si les valeurs propres d'amatrices sont plus de souvent positives que négatives en fait ça fait une somme dont on sait que l'espérance est extrêmement positif et du coup la probabilité d'avoir un truc négative lorsqu'on une somme de beaucoup de variables aléatoires qui ont tendance à être plutôt positive bah typiquement être exponentiellement faible en le nombre de variables victoire c'est à dire la dimension de l'espace bref tout ça pour dire que si 99 % des directions correspondent à des lignes de crête alors en grande dimension on a une probabilité exponentiellement faibles de réussir à trouver une direction qui correspond à descendre de ceux de ce col ça ça me fait dire que en très grandes dimensions la plupart des pensées elles sont des points sur lequel on risque fort d'être coincé et d'où il est très très improbable de réussir à échapper de façon plus générale il faut bien voir que c'est cette analogie avec la topographie à cette limite en cela que la topographie que l'on connaît en tout cas c'est une topographie de dimensions de qui a des propriétés très différente des espaces de très grandes dimensions comme on a déjà parlé avec la ligier et j'espère que vous avez aimé cette vidéo à partir de la prochaine fois on va parler de différentes architectures de réseaux de neurones en particulier les données que l'on étudie avec les réseaux de neurones ne sont pas arbitre à raison souvent des propriétés et du coup à cause des propriétés que l'on connaît des données observées il peut être adéquat d'ajuster la architecture du réseau de neurones pour qu'elle soit un petit peu optimisé pour les symétries naturelles notamment des données que l'on cherche à étudier ces centres on parlera dans les quatre prochaines vidéos en particulier prochaine fois on parlait des réseaux de neurones récurrente si vous avez aimez cette vidéo pensez à la likouala commentaire partagé parce qu'elle vous abonner pour ne pas manquer les futurs épisodes neigeux et hip hop awards et j'espère que vous serez là la prochaine fois des fois dans la vie on aura détruit et des fois on fait des choses qui devraient très bien fonctionné mais qui foire sans qu'on comprenne forcément pourquoi ça arrive plus ou moins souvent c'est plus ou moins graves mais de toute façon il faut apprendre à faire avec ce qu'on fait