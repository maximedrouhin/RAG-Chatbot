l'une des principales applications d'une machine learning et l'analyse d'image et ce n'est pas étonnant pour nous autres humains aussi les images sont une source d'information majeure la vue est l'indécence est plus important pour une corde primates sauf que l'analyse d'image pose un problème assez intimidant chaque image correspond à une dawn est de très grande taille comme vous le savez peut-être la taille des images prises par vos téléphones et de l'ordre du méga pixels ce qui correspond à des millions de 0 et de 1 pour un humain train que lire ces données au rythme d'une donnée par seconde prendrait des semaines pire que cela en termes mathématiques une image correspond à un objet dans un espace de très grandes dimensions d'une dimension 1 million typiquement or comme on l'a vu avec lg fis ces espaces de très grandes dimensions sont extrêmement vaste et contre intuitif pour bien analyser ces monstres il va nous falloir réfléchir à comment exploiter les nombreuses propriétés très spécifique des images mais plutôt que d'y aller à coude invariants mathématiques il se trouve qu historiquement les informaticiens en général notamment à certains viennent car en particulier se sont tournés vers la biologie en effet les biologistes ont découvert que le cortex visuel de nombreuses espèces animales et en particulier des mammifères avait une structure distinctive celui ci se décompose en particulier en cinq régions successives appelé très originalement v1 v2 v3 v4 et white flight des cinq du coût de la même manière le cun et ses collaborateurs ont proposé d'effectuer une analyse d'image qui passait par un certain nombre de couches de neurones intuitivement chaque couche correspond aux calculs de représentation de plus en plus abstraite du contenu de l'image typiquement la couche observable mesure uniquement la luminosité et la couleur de pixels la couche qui suit va étudier différentes sortes de corrélation entre des pixels voisins puis la course suivante peut être essayer de mettre à bout ces corrélations pour identifier des lignes directrices puis la course suivante va regrouper ses lignes directrices pour identifier des structures puis on cherchera à identifier des composants comme des yeux ou des oreilles puis on combinera la présence d'yeux et d'oreilles que pour un ferrer les concepts plus abstraits comme la présence ou l'absence de chat le réseau de neurones calcule ainsi une abstraction croissante des données qui lui sont représentés ce qui lui permet de calculer en bout de ligne une présentation sémantique de l'image avec toutes les applications dont on a déjà parlé dans les vidéos 20 à 23 ok ça c'est pour le principe général de superposition de couches de réseaux de neurones et ce n'est en fait pas du tout spécifique aux réseaux de convulsions l'idée importante des réseaux de convolution c'est surtout que chaque neurone de chaque couche intermédiaire n'était exposée car un champ récepteur particuliers ainsi que le fait que l'analyse de ce champ récepteur est la même que l'analyse qui effectue un autre neurones de la même couche avec son propre champ récepteur pour illustrer cela prenons le cas d'un neurone de la première couche après l'accouchement serviable ce neurones va typiquement analyser une toute petite région du champ visuel prenons le cas simpliste où cette région ne contient que neuf pixels avec différents niveaux de luminosité le neurone va alors multiplier la luminosité de chaque pixel par un poids synaptique possiblement négatif et calculer la somme des luminosités pondérer et puis ensuite ce qui est souvent face et de ne retenir que la partie positive du résultat ce qui correspond à la fonction d'activation relu dont on a parlé dans l'épisode 41 lespwa synaptique forme alors une matrice dit de convolution on parle parfois aussi de filtres ou de carnelle l'idée générale des réseaux de convulsions c'est d'utiliser la même matrice de convolution pour tous les champs récepteur de l'image autrement dit si je prends un autre neurones de cette première couche il sera exposé à une autre partie de l'image mais sept autres neurones effectuera en fait les mêmes calculs que le premier neurones en particulier et surtout il utilisera la même matrice de convolution tout se passe exactement comme si entre l'accouchée observable première couche il y avait tout un tas de synapses qui étaient exactement les mêmes mais trans latter pour être appliqué à d'autres régions de l'image et d'autres neurones de la première couche en particulier ses synapses ont toujours le même poids synaptic dans le langage d'une machine learning on parle de partage des poids synaptique ou juste weight schering et c'est une excellente idée puisque ça permet de réduire drastiquement le nombre de paramètres de nos réseaux de neurones autrement dit au lieu d'apprendre les poids de toutes les synapses entre la couche observables et la première couche il suffit alors de n'apprendre que les quelques paramètres de la matrice de convolution ça fait beaucoup moins deux paramètres alors en fait ce n'est pas tout à fait exact il faut imaginer que dans la première couche pour un même show un récepteur il existe en fait plusieurs neurones différents qui vont analyser le même choix recept et chacun de ces neurones va utiliser une matrice de convolution différentes ainsi chaque chant récepteur sera traduit par un certain nombre de nombre qui résume le contenu de ce champ un récepteur on dit que chaque chant récepteur est résumée par un certain nombre de canaux et ça c'est vrai pour la première couche pour la seconde couche et ainsi de suite mais c'est aussi vrai pour la couche observable oui parce que chaque pixel d'une image ne correspond pas en fait à un ce nombre il correspond en fait à 3 nombre pour chacune des couleurs primaires rouge vert et bleu donc si je résume pour la première couche pour chaque matrice de convolution et chaque chant récepteur on trouve un neurone qui calcule la matrice de convolution appliquée au chant récepteur lui applique une transformation non linéaire et pendant l'apprentissage du réseau de neurones se sont directement les matrices de convolution que l'on cherchera à améliorer et ce qui est vraiment cool ce faisant c'est que notre réseau de neurones va être parfaitement en accord avec une symétrie très naturelle du réseau appelé l'invariance par translation ainsi quand j'observe une cuillère si je tourne la tête la cuillère subi une translation dans mon champ visuel il semblerait souhaitable que mon cerveau analyse cette cuillère de la même manière qu'il analysait avant translation et bien en utilisant les mêmes matrice de convulsions pour tous les champs réceptif c'est justement ce que garantit automatiquement notre réseau de neurones de convolution dit autrement les réseaux de convolution ont une architecture pré programmés pour l'invariance par translation des images on y est presque il reste tout de même un petit problème en fait ce faisant on ne diminue absolument pas la taille des données à analyser pierre si on a plus de 3 canaux dans la première couche du réseau on en vient en fait à augmenter la dimension des données or on a justement dit que le gros problème des données comme des images c'était la malédiction de la dimensionnalité qui fait que les vecteurs de dimensions 1 million ou plus sont en fait très difficile à analyser c'est pour cela qu'on a ajouté généralement ensuite une phase de réduction de la dimensionnalité qui revient résumé l'information de plusieurs neurones voisins en une seule information typiquement pour chaque canal ou considère tous les neurones associés à ce canal et dont les chants réceptif sont inclus dans une certaine région de lima je calcule alors la valeur moyenne des excitations de ses neurones on parle alors d'extraction de la moyenne ou aveugles pauline en anglais mais alors je ne sais pas pourquoi mais il se trouve qu'en pierrick mans il semble que ça marche mieux si on utilise l'extraction du maximum ou max pulling qui consiste à ne retenir que la plus grande excitation de l'ensemble des neurones considéré quoi qu'il en soit ce faisant on obtient une représentation moins précises de l'image mais qui a aussi le bon goût d'être deux dimensions moindre d'une certaine manière après pauline on a alors décrit différentes petites sous région de l'image à l'aide de résumer composé de quelques canaux qu'est-ce qui vient ensuite eh bien il s'agit tout bêtement ensuite et de recommencer des opérations similaires non pas à l'image initiale mais à la première couche obtenu après l'opération convolution pulling on répète ainsi convulsions pulling puis à nouveau convolution pulling puis à nouveau convolution bowling et ainsi de suite généralement en bout de ligne on obtient un vecteur de bien plus petite dimension et qui a une bonne représentation abstraite des contenus de différentes régions de l'image et on peut alors mieux décortiquer ce vecteur avec un réseau de neurones fit for world classic ou même avec un réseau de neurones récurrents typiquement si on veut faire de l'analyse de vidéos enfin dernière chose à dire étrangement les réseaux de convolution ont des applications bien au delà de l'analyse d'image de façon étrange ils ont trouvé des applications dans la compréhension de textes en médecine et également pour le jeu de go en fait c'est de façon générale les réseaux de convolution semble utile dès qu'il semble y avoir ne serait ce qu'une partiel corrélation local plutôt que global et une relative invariance spatiale ou temporel mais une fois j'avais déjà fait des liens entre les réseaux de neurones récurrents et des raisonnements cognitif qu'on pourrait avoir au niveau du cerveau biologique notamment et bozon de geek demande si ce genre de recherche en intelligence artificielle et en réseaux de neurones artificiels en particulier pourrait donner plus d'une série de plus d'informations sur la nature d'une pensée humaine qu'elle est comme ça alors là je sens qu'il ya quelque biologiste qui sont en train de rebondir dès à dire mais c'est impossible les réseaux non artificielle n'ont rien à voir avec les réseaux de neurones belgique en particulier neurones artificiels n'ont rien à voir avec neurologique en effet il ya un peu un gap entre les deux cependant si on croit notamment à la tête de turc featuring dont elle est parlée dans l'épisode 4 de cette série on a tenté de penser que le cerveau humain n'est rien d'autre qu'une machine à calculer et que tout ce qui se passe dans le cerveau n'est que le résultat d'un très grand nombre d'opérations et il y a un problème de son père rivers engine ring à ces pratiques qu'elle est de savoir étant donné un truc qui marche comment est-ce qu'on peut inférer son fonctionnement interne pour reconstituer son fonctionnement est finalement essayé de construire nous mêmes un truc qui résonne aussi bien que le cerveau humain va forcément nous amener à des questions qui se pose également pour le cerveau humain et du coup forcément on va apprendre un peu plus sur n'ont pas nécessairement ce qu'est exactement le cerveau humain mais au moins des fonctionnements un peu macroscopique du cerveau à lui un peu l'idée du fonctionnement global du cerveau et peut-être même de façon plus générale m manière assez globale de réfléchir et en fait dans mon livre c'est une perspective encore un peu différente que j'ai prise puisque je me suis demandée sur la forme du savoir quelle était la meilleure façon de savoir et si on en croit le bail à nice mme la meilleure façon de savoir c'est d'appliquer la formule deux pays le problème de la formule d'obèses est très au courant c'est le fait que cette formule de belz demande des calculs qui sont irréalistes en pratique et du coup pour pouvoir bien réfléchir on est obligé de se contenter d'approximations de la formule de beille du coup pouvoir bayésiens savoir comment bien réfléchir et en particulier vu que le cerveau humain quand même réfléchir bien mieux que des algorithmes capables d'écrire quand même à le penser c'est que d'une certaine manière le cerveau humain arrive à calculer de meilleure approximation de la forme de pays ce que ce dont on est capable de faire nous avec nos algorithmes que l'on écrit du coup vous envie de dire que forcément si on réfléchit mieux à comme à ce que le cerveau parvient à faire les calculs qu'il fait et a abouti en conclusion qu'il fallait surtout quand ces conclusions semblent en fait très très bonne et qu'on n'arrive pas à l'expliquer c'est une façon de nous pousser dans leurs retranchements et d'essayer de faire mieux en intelligence artificielle et al'inverse qu'on intelligence artificielle on trouve des méthodes qui fonctionne tout à coup beaucoup mieux que les méthodes précédentes qu'on avait inventé en influence artificielle ça laisse parfois même à penser que ce genre de méthode convient de découvrir révèle quelque chose ça c'est fondamental sur quelles sont les meilleures façons d'approximations formule de bèze de façon pragmatique et si tel est le cas on peut vraiment s'attendre au fait que le cerveau humain applique ce genre de calcul en savoir plus sur ces merveilleuses relations entre les neurosciences d'un côté et les raisonnements plus algorithmique de l'autre côté je recommande très très très vivement les cours de stanislas de haine au collège de france je crois que c'est les années 2012 à 2014 si vous vous trompez pas 2012 1013 du xiii et louis xiv ou sané 6 2 n a donné des cours intitulé le cerveau statisticiens et le bébé statisticiens qui sont vraiment des découvertes assez fantastique sur des modèles très utile pour point c'est la condition humaine et j'espère que âgées mais cette vidéo la pression on va continuer notre exploration des différentes architectures utile des réseaux de neurones qui sont souvent également inspiré des cerveaux humains puisque on parlera des réseaux très profond notamment des réseaux résiduelles si vous aimez cet épisode spécial de l'ikea le portail de partage et pour ça il ya donné pour l'épisode leur seul titre ford et j'espère que vous serez la prochaine commission reebok pour au pied qui pourrait la fin des années 80 quand j'étais malade dans new jersey donc laboratoire laitiers ainsi la compagnie téléphone américaine j'ai en fait travailler sur des premiers réseaux collectifs qui était d un petit peu inspirée du nous connaîtrons de fukushima mais utilisant ses ago rides des centres de gradient d'avoir supervisé de rétro propagation