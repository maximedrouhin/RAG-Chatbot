dans les épisodes précédents on a vu que le machine learning moderne consiste est souvent être guidé dans l'exploration des paramètres de nos algorithmes à l'aide de gradient c'est gradient nous disent comment ajuster nos paramètres pour mieux coller aux données d'entraînement cependant j'ai pu malencontreusement donner l'impression que cette exploration des paramètres allaient nécessairement bien se passer et nous conduira à déterminer des paramètres optimaux pour nos machines ce n'est malheureusement absolument pas le cas l'exploration par descente de gradient c'est une exploration bien plus intelligente qu'une exploration aléatoire cependant il reste difficile de garantir quoi que ce soit vis-à-vis des performances de la descente de gradient et pour le comprendre il est utile de considérer l'analogie avec la topographie des topographies c'est l'étude des reliefs typiquement pour dessiner des cartes qui rende compte que des altitudes des paysages naturels et en machine learning ou même typiquement considérer que les coordonnées gps d'un individu sur terre sont l'équivalent des paramètres de nos algorithmes alors que l'altitude correspond à la performance de l'algorithme typiquement une faible altitude correspond à un algorithme performant lors qu'une haute altitude correspond à un algorithme qui commet beaucoup d'erreurs de prédiction le problème d'une machine learning correspondait alors à identifier des points de très faible altitude cependant il faut imaginer que le paysage global est très difficile à observer étant donné deux boutons il est difficile de visualiser les performances de la machine pour différents réglages de ce bouton souvenez-vous du cadre de l'épisode 42 tout ce que l'on a ce sont des boutons à tourner et donc on est limité dans ce qu'on peut faire pour une configuration donnée on peut calculer la performance de la machine mais aussi et surtout dans le cadre des réseaux de neurones notamment pour chaque configuration de la machine on a aussi un gradient qui nous suggère comment tourner les boutons pour aller dans le bon sens en topographie ça correspond à dire que l'on à un capteur gps qui nous dit exactement où on est mais on n'a pas de carte pour savoir où il faut aller qui plus est on est aveugle ce qui veut dire qu'on ne peut pas non plus voir le paysage autour de nous on dispose toutefois d'un altimètre pour que m l'altitude et enfin d'une espèce de super boussole qui nous dit vers où aller pour descendre autant que possible cette direction de plus forte descente est justement ce qu'on appelle le gradient de la pente appliquez la descente de gradient sar vient se placer quelque part dans le paysage puis à chaque instant on utilise la super boussole pour déterminer la direction de plus forte descente et les yeux fermés on effectue un pas dans cette direction généralement proportionnelle à la pente de cette direction de plus forte descente et puis une fois ce pas effectué on recommence alors très rapidement dans l'épisode 42 j'ai mentionné une variante de la descente auto gradient ap les stochastiques ça consiste à avoir une super boussole imprécise qui en moyenne marche mais qui donne des indications qui fluctue d'une mesure à l'autre ça ne change pas énormément de chose au problème bref on a donc cette technique de descente de gradient qui pas à pas et c'est de nous amener vers de basse altitude le succès du machine learning dépend alors de la faculté de cette descente de gradient à y parvenir qu'est-ce qui peut mal aller alors tout d'abord on peut parler du cas où tout ira forcément bien c'est typiquement le cas lorsque le paysage est entièrement une énorme cuve est si cette cuvette et bien comme il faut et si nos pas à sont de plus en plus petit on sent qu'on finira par atteindre le fond de la cuvette eh bien il ya une façon de formaliser cette intuition une bonne cuvée correspond à ce que l'on appelle une fonction de coo convexe intuitivement si on remplissait la cuvette de goudron alors le goudron prendrait alors la forme d'un solide sens creux intel solide est appelée convexe et ça a bien sûr des liens avec la géométrie comme par exemple la possibilité de pavage pentagonaux de façon un peu plus formel une cuvette sera convexe si lorsque je prends deux points de la cuvette et si je construis une tyrolienne qui relie ces deux points alors la tyrolienne ne touchera jamais le sol dit encore autrement tout moyenne pondérée des altitudes des deux points sera supérieur à l'altitude de la moyenne pondérée des deux poings bref en optimisation on s'est rendu compte encore et encore que cette propriété de convexité rendait les problèmes simples et résolument efficacement ainsi si un informaticien vous dit qu'un problème est complexe vous pouvez généralement comprendre qu'il vous dit qu'il est facile et en effet typiquement ses métriques de performance à minimiser son complexe alors on peut démontrer que la descente de gradient va bien marcher malheureusement à l'instar de la topographie les performances des réseaux de neurones de forme généralement pas une gentille cuvette globale pour commencer on peut avoir des cuvettes locale typiquement le relief peut être miné de petit creux à l'instar de la surface de la lune disons la descente de gradient valeurs nous faire tomber dans l'un de ces creux mais probablement pas dans le plus bas de ces creux or il est tout à fait possible que la plupart des creux soit en haute altitude par opposition à un maxi cros qui serait l'équivalent de la fosse des mariannes en fait si on appliquait la descente de gradient sur terre il serait improbable d'atteindre une altitude comparable à la plus basse altitude possible on finirait bien plus probablement dans des creux en altitude peut-être même au dessus du niveau de la mer comme le fond d'un lac dans le jargon on dit qu'on attendra des minimales locaux par opposition à la fosse des mariannes qui serait un minimum global et il faut bien dire que machine learning on a fini par accepter cette défaillance de la descente de gradient il est même considéré illusoire aujourd'hui d'espérer atteindre des minimes à globo on se satisfait généralement bien aisément de minima locaux après tout s'il y a atteint des performances surhumaine à sa tâche c'est déjà pas mal et d'ailleurs on peut invoquer des arguments de complexité en temps de calcul pour conclure que chercher des minimes à globo est désespérée en effet en s'appuyant par exemple sur le problème du voyageur de commerce il est aisé de construire des familles de fonctions telles que trouver les minima globaux de ses fonctions soit un problème np dur et npd or ça veut dire ça veut dire super dur go bref le mieux qu'on puisse espérer en pratique c'est sans doute les minimales locaux mais du coup si on laisse tourner la descente de gradient on finira au moins par atteindre des minimales locaux non eh bien ce n'est pas gagné en effet une autre chose qui peut arriver et de se retrouver bloqué au niveau d'un col en montagne un col correspond à l'intersection entre une ligne de crête et une ligne de creux typiquement en randonnée au monde souvent le long de ligne de creux pour rejoindre d'école et c'est souvent génial parce qu'en arrivant au niveau du col on découvre tout à coup la vue de l'autre côté du col en descente de gradient on aura plus tendance à descendre plus ou moins le long d'une ligne de crête avant d'arriver au col et le problème c'est qu'au niveau du col le gradient s'annulent ce qui fait que les pas du gradient sont très petit et peu indicatif de la bonne direction à suivre pour descendre alors pour les cols de montagne ça ne paraît pas si problématique puisqu'il semble qu'on finira bien par tomber d'un côté ou de l'autre du col mais cette intuition que l'on finit assez vite par tomber d'un côté ou de l'autre est assez spécifique des petites dimensions oui parce que les coordonnées gps correspondent à de nombreux la latitude et la longitude est donc la topographie terrestre correspond à uniquement de boutons d'une machine que l'on chercherait à paramétrer en pratiquant machine learning on a souvent des millions de boutons et il se peut que l'on est alors un col qui soit une ligne de crête selon 990 mille direction et une ligne de creux selon les 10000 direction restantes mais alors il faut imaginer que notre boussole surtout si elle est randomisés va souvent nous amener vers une combinaison d un million de direction qui va alors avoir de bonnes chances d'aller globalement dans l'es9 190000 mauvaise direction plutôt que dans les dix mille direction restantes et le truc c'est une telle direction bonne faire remonter plutôt que des cendres trouver l'une des directions qui descendent et bien les prendre peut être redoutablement difficile car en grande dimension l'angle admissible pour descendre peut-être ridiculement petit bref il se trouve que en pratique l'école de très grandes dimensions sont des obstacles très difficile à franchir pour la descente de gradient et du coup oui il faut parfois se contenter des cols plutôt que des cuvettes ce qui nous amène à la question suivante ces histoires de cul est non globale et de colle au final est ce si grave et bien pas nécessairement en 2015 certains chercheurs ont étudié les propriétés de certaines ipea surface aléatoire et l'ont montré qu'en un certain sens les cuvettes écoles étaient généralement pas si mal autrement dit en général tombé dans une cuvette ce n'est pas une garantie d'avoir bien appris mais c'est un signe qu'on a bien progressé et c'est sans doute comme ça qu'il vaut mieux interpréter le machine learning d'ailleurs il est bon de garder en tête que même le minimum global n'est globalement meilleures que vis-à-vis de l'ensemble des données collectées et ne le restera probablement pas si on collecte toujours plus de données surtout si ces autres données correspondent à quelque chose d'un peu différent ainsi selon cette vision du machine learning il faut surtout cherché à beaucoup progressé plutôt que de chercher à être optimal vis-à-vis de ce qui a été observé jusque là enfin une dernière particularité géologique qui peut avoir des conséquences étonnantes en apprentissage et la présence de falaise ainsi il est possible qu'un apprentissage soit bloqué sur la taupe avant de soudain tombé d'une falaise bon et irl vous déconseille de tomber d'une falaise maison machine learning cela correspondrait à tout à coup à atteindre des performances nettement meilleures et je me demande si ce n'est pas ça qui m'arrive de temps en temps dans l'apprentissage des mathématiques parfois je bloque des heures des jours parfois même des mois sur un concept mais tout à coup en l'espace parfois d'une fraction de seconde tout s'éclaire bon ça arrive pas aussi souvent que j'aimerais mais quand ça arrive c'est l'une des sensations les plus cultes issoire bref globalement il me semble que l'analogie entre la descente de gradient l apprenti c'est avec les humains ne soient pas si farfelue dans les deux cas d'une certaine manière il s'agit d'explorer l'espace des idées en étant guidé par des données empiriques deux sortes n'ont pas à chercher à apprendre l'idée optimale mais plutôt à simplement constamment cherché à mieux faire l'anr fin on a parlé de l'algorithme devrait être aux populations il avait notamment donné un exemple où on essayait de détecter des chars ou pas et on mesurait la performance de l'algorithme avec le carré de l'erreur entre paix et la valeur attendue qui étaient soit 0 ou soit un pour la présence ou l'absence de chats et j'avais invité aussi une remarque comme quoi en fait entre l2 et l1 ça changeait pas énormément de choses bon en fait c'est pas tout et faudra ça change pas mal de choses parce que changer des choses en tout cas et c'est ce que fait remarquer nicolas aborderont notamment la métrique l1 est souvent quelque chose de beaucoup plus robuste elle sera beaucoup moins sensible à notamment des des erreurs d'étiquetage si on a une image de chats mais elle a été étiqueté pas achats en fait du coup la fonction objectifs qu'on a programmé d'intelligence artificielle sur à quelque chose qui ne sera pas en fait la bonne réponse attendue parce qu'on avait fait une erreur d'étiquetage et et lorsqu'on utilise une erreur quadratique ça donne beaucoup plus importance à ces erreurs d'étiquetage et on a déjà parlé notamment avec midi en parlant de robustesse notamment vis-à-vis des attaques contre les ya ce genre de différence entre le carré et la valeur absolue et en fait assez important pour garantir toujours plus de robustesse du coup pour des raisons de robustesse il peut être souhaitable de préférer la valeur absolue plutôt que le carré de l'erreur bon bien sûr la gamme des histoires de très neuf dans parce que si on prend le car et de l'erreur à ce moment là on est capable avoir en fait une meilleure cuve est en fait qu a été beaucoup plus plu lis beaucoup plus plus ronde et beaucoup plus complexes en un sens en ce sens qui peut être même vendu assez précis et ça c'est du coup plus souhaitable c'est nous oriente mieux dans la descente de gradient tout cas ça permet à descendre de gratteurs de converger plus rapidement vers des bonnes solutions peut-être qu'une bonne idée dans certaines applications sa dépendance à des applications mais dans un premier temps il peut être souhaitable de regarder d'abord dépénalisation quadratique comme ça ça va nous permettre de bien explorer l'espace des idées de pression assez globale et ensuite une fois qu'on a des bonnes performances on peut chercher à avoir plutôt des erreurs en valeur absolue qui vont nous éviter d'être sensibles à des attaques notamment ou à des données corrompues et à ce moment là on gagnera en robustesse et tout en continuant à ça à progresser dans notre apprentissage ap qui voit lui demande bas sachant que la descente de grayan stochastique et l'algrave doit être prise à sion ont déjà été découverts avec eux bongo ça fait pas mal de temps et qu'on a rien su découvrir de nouveaux et de meilleurs depuis cela depuis au moins trois décennies est-ce que c'est à dire que les frais qu'est-ce qu'on peut faire maintenant en termes de recharge artificielle est bien et ce qui encore énormément énormément de recherches sur l'intelligence artificielle à faire et on a déjà des repères parler pas mal dans cette série notamment en termes de robustesse des intelligences artificielles il ya des tout ce pan de recherche il ya aussi plein de problèmes de comment extraire la sémantique de données et à ce moment là en fait on va voir en tant que l'on parlera dégénérative à dire sur les networks mais l'idée d'utiliser juste la rétrogration ne suffit pas parce qu'on n'aura pas des données étiquetés et du coup va falloir trouver des façons de combiner notamment plusieurs architectures de réseaux de neurones et des algorithmes d'asie aidée et de rétro progression mais avec des fonctions objectif bien choisi pour que on arrive quand même à faire des trucs intelligents tout en exploitant et sgd descendre de gradient stochastique et la rétrogration vient en parlera un peu plus tard dans cette série et aussi peut-être des architectures de réseaux neurones à optimiser ses aussi on en parlera par exemple notamment les idées de réseaux de neurones récurrents avec des boucles qui sont capables d'oublier l'information de ses boucles ou encore des histoires de convolution avec le choix de certains liens plutôt que d'autres le partage de serre pour moi bref y'a pas mal le plein de petites idées comme ça pour améliorer les architectures de réseaux de neurones d'aujourd'hui pour faire des performances qui sont toujours un peu meilleur d'un point de vue théorique est encore à des questions qui se posent après la clé pour faire de la recherche dans ces domaines c'est vraiment ne pas être trop ambitieux de pas vouloir réinventer complètement la route voilà faut se dire que les progrès que l'on va faire de toi sinon c'est vrai dans la recherche en général quand un domaine qui concept bien établi que faire de physique aujourd'hui cherchait pas à réinventer toute la physique est ce qu'on les travaux de physique théorique sont plus des contributions des pompiers a rajouté à tout un édifice est assez grand comme ça qu'il faut penser la recherche pas espérer tout résoudre mais il faut espérer tenez débit fournir des vies qui vont permettre à d'autres chercheurs de les réutiliser de créer d'autres billes pour ajouter des modules important à un système plus complexe et je pense que c'est un petit peu comme ça que fonctionne aujourd'hui la recherche en intelligence artificielle à moins que vous voulez faire des trucs plus révolutionnaire avec un plus grand impact et là il y aura plus de risques d'échec également mais par exemple les problèmes de sécurité en intelligence artificielle en particulier le problème de la programmation des valeurs morales dans une ia et là je vous renvoie vers l'article que j'ai écrits ou à identifier pas mal de problèmes plus ou moins théoriques et je pense qu'il ya beaucoup de boulot à faire sur chacun de ces domaines mais encore une fois ce sera dès ce seront des pierres a ajouté un édifice il faut pas espérer tout résoudre dans le cadre d'un travail de recherche elle espère qu'ils avaient aimé cette vidéo la pression de voix on va parler des bêtises des neurones à la fois biologique et artificiels et de façon très contrainte ils vont avoir que les bêtises des neurones sont une propriété importante et désirable des neurones si vous avez aimez cette vidéo pensez à la lica les commentaires et partager pense à vous avez question épisode merci au départ bourdon et j'espère que vous serez là la prochaine fois [Musique]