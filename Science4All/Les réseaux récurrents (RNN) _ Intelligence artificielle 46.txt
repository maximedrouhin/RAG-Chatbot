on a vu que les réseaux de neurones reposer sur l'exploration de leurs paramètres guidée par les données en principe ceux ci pourraient suffire à créer toutes sortes d'intelligence artificielle mais en pratique cette approche pourrait nécessiter des temps de calcul déraisonnable pour améliorer les performances d'une machine learning il est alors pertinent d'ajuster larchitecture de nos réseaux de neurones en fonction des données que l'on cherche à étudier en effet les données que nous machines sont amenés à étudier sont tout sauf arbitraire elles contiennent généralement des symétries naturel il serait bon d'exploiter notre connaissance de ces symétrie pour optimiser l'apprentissage de nos machines parmi ces symétrie on trouve les symétries naturel de translation des images typiquement un objet reste le même lorsqu'on se déplace de gauche à droite mais ça on en reparle la semaine prochaine pour aujourd'hui j'aimerais m'attarder davantage sur les symétries naturel des données séquentielles c'est à dire des données qui sont des successions de signaux élémentaire c'est typiquement le cas du texte du son d'une vidéo et ce qui est intéressant avec ces données séquentielles c'est qu'il est souvent une forte corrélation locales dans le temps mais pas nécessairement de fortes corrélations global je m'explique dans un texte la lettre q est très souvent suivi de la lettre u il ya une donc une très forte corrélation entre la lettre q et la lettre qui le suit cependant si je vous demande de deviner la lettre qu'ils se trouvent 1000 caractères après un q le fait que vous sachiez qu'il y ait là un cul nour a sans doute que peu d'influencé sur votre prédiction d'ailleurs j'en profite pour recommander l'excellente vidéo de sciences étonnante sur les chaînes de markov qui exploite de telle corrélation locales pour inventer des mots passible biscornus et improbable de la langue française de même comme on l'a vu dans l'épisode 21 lits à world check de google a su exploiter les propriétés de corrélation local des mots pour un ferrer une représentation sémantique des mots et on déduire des manipulation sémantique stupéfiante tels que broie - sommes plus femme est égal à rennes bref nombre de nos données séquentielles où de telles propriétés de corrélation locale qu'il serait bon d'exploiter un sirh bon modèle pour analyser ces corrélations devrait faire interférer ces corrélations locales pour un désir le sens des mots et des phrases pour ensuite mieux prédire par exemple des mots à venir d'où l'idée des réseaux de neurones récurrents l'idée fondamentale des réseaux récurrence et d'introduire dans le réseau de neurones des synapses qui boucle pour ainsi récupérer l'information des mots précédent pour mieux comprendre le mot présent un point de vue d'architecturé du réseau de neurones se revient tout bêtement à dire que les signaux ne vont plus nécessairement dans un seul sens certains signaux reviennent vers l'arrière du réseau de neurones de façon amusante ce genre de boucles récurrente est en fait équivalent un réseau de neurones plus large en effet en un sens on peut déplier ses boucles pour voir qu'un réseau récurrent fait en fait le même calcul care réseaux fit for words plus grand pour comprendre cela prenons le cas du réseau récurrent le plus simple qui capte les données extérieures avec un neurone observables et conclu immédiatement son calcul avec un neurone de sortie et supposons qu'il existe une boucle qui renvoie le signal de sortie vers le neurone de sortie lui même le premier signal de la données séquentielles va être lu par le neurone observable qui va ensuite transmettre l'information aux neurones de sortie qui conclut avec une réponse puis le second signal élu par le neurone observable qui va transmettre l'information aux neurones de sortie mais de façon intéressante ce neurones de sortie va également prendre en compte la conclusion du neurone de sortie pour le premier signal il va donc combiner sa conclusion pour le premier signal avec l'information pour le second signal pour en venir à une conclusion pour le second signal puis il va combiner cette conclusion avec l'information qu'il aura été transmise pour le troisième signal pour en venir à une conclusion pour le troisième signal et ainsi de suite pour y voir plus clair on peut copier coller le réseau est considéré que chaque traitement d'informations de chaque neurone correspond à une version différente de ceux neurones on voit ainsi que le premier signal est traitée par la première version de deux neurones le deuxième signal est traité par deuxième version des deux neurones avec deux façons intrigante une synapse qui va de la première version du neurone de sortie vers la seconde version du neurone de sortie puis le troisième signal est traité par les troisième version des neurones avec là encore une synapse de la seconde version du neurone de sortie vers sa troisième version et ainsi de suite on voit alors qu on obtient un réseau fit for world c'est à dire un réseau dont tous les signaux vont de la gauche vers la droite tous vos réseaux récurrent est équivalent à un réseau fit for world ceci étant dit on peut noter qu'un réseau récurrents ainsi déplier n'est pas n'importe quel réseau fit for words en particulier les neurones les synapses sont des copies les uns des autres voilà qui montre qu'à chaque instant ce sont en fait les mêmes opérations qui sont utilisés même si ces opérations s'applique à des signaux différents voilà qui montre qu'un réseau récurrents implémente automatiquement une forme de symétrie de translation à travers le temps en plus d'exploiter la localité des corrélations il exploite aussi ainsi la symétrie par translation des données séquentielles que l'on retrouve très souvent dans beaucoup de données séquentielles bien sûr j'ai pris là l'exemple d'un réseau récurrents ultra simple mais de façon intrigante il est possible d'avoir des boucles de récurrence qui remonte plus ou moins loin dans le passé ces boucles ont une interprétation immédiates en termes de mémoire à plus ou moins long terme en effet une boucle très courte correspond à une mémoire à très court terme l'information est réutilisé immédiatement et est potentiellement immédiatement détruite une boucle à plus long terme permet de garder l'information d'un signal en mémoire du réseau de neurones plus longtemps ce qui permet d'exploiter des corrélations à plus long terme à l'intérieur des données séquentielles notre conséquence intrigante de tout cela c'est l' influence cruciale de la vitesse de lecture en effet on a su poser ainsi que la lecture d'un signal prenait autant de temps qu'une transmission d'informations à travers une synapse cependant si on lit les données séquentielles plus lentement alors on pourrait lire un signal toutes les deux transmission synaptique vola qu'il correspondrait un des pliages différent du réseau récurrents les ça semble s'appliquer à nos cerveaux de primates aussi en pratique quand on lit un texte on peut le lire plus ou moins vite ce qui correspond à plus ou moins exploité les corrélations entre les mots qu'on lit typiquement si on lit un texte très vite on pourra y déceler une structure globale mais les détails des arguments nous échappe rang al'inverse si on lit un texte avec beaucoup d'attention il y un risque de se perdre dans des arguments locaux et de ne pas voir la logique globale texte voilà qui suggère fortement qu'il est utile de lire et relire un texte à différentes vitesses et avec différents niveaux d'attention aux détails ou de faire de même pour aux hasards une vidéo samsung bref tous ces arguments montre que les réseaux récurrents semble vraiment être l' architecture naturel pour traiter des données séquentielles cependant les réseaux récurrents sont aussi horriblement dur a entraîné à cause de l'instabilité des boucles de récurrence en effet à chaque fois qu'un signal passe par une boucle il est multiplié par le poids des synapses qui traverse il ya donc un phénomène multiplicatif au niveau des boucles de récurrence cependant les phénomènes multiplicatif répéter donne inéluctablement lieu à des croissances ou d'aidés croissance exponentielle si le signal est constamment x nombre supérieur à 1 alors le signal explosera après quelques boucles si les x nombre inférieur à 1 alors on observera le phénomène inverse le signal disparaîtra exponentiellement vite ce phénomène est encore plus problématique au niveau de l'apprentissage lorsque l'on applique la rétro propagation où on a souvent tendance à observer une disparition des signaux de rétro propagation on parle alors du gradient évanescent ou vanishing agent en anglais pour éviter ce problème les informaticiens scène rock writer et jurgen chimiques ou beurs ou introduit en 1997 l' architecture des longues sorties memory plus connu sous le nom de l estime qu'ils possèdent également la capacité d'effacer tous les signaux de rétro propagation voilà qui permet d'éviter de trop longue dépendance entre différents signaux qui conduirait à des évanescence des gradients au moment de l'apprentissage dit autrement quand il faut apprendre une longue suite d'arguments il est bon de séquences et l'apprentissage et d'apprendre chaque argument de façon indépendante ce n'est qu'une fois que chaque argument sera bien compris que l'on pourra tous les apprendre tous ensemble typiquement les lisant vite et en évitant ainsi de se perdre dans les détails des arguments me semble ainsi très probable que nous propre cerveau possède des modules semblable aux architectural estime en effet personnellement il m'arrive souvent d'être interrompus dans un raisonnement de toutacoup oublier ce que je pensais avant cette interruption comme simon module l est on avait tous supprimés et le fait que l stm soit encoder par le cerveau humain suggère qu'il s'agit en effet d'un outil indispensable pour se confronter à la grande complexité de données séquentielles enfin une dernière remarque amusante et que pour comprendre des données séquentielles les données du passé ne sont généralement pas les seuls à être utile pour comprendre le présent surtout quand il s'agit de blagues de publicité ou de films la fin de la blague est un publicité ou du film peuvent être également très utile pour comprendre le présent voilà qui a amené barde cosco à introduire les réseaux dits bidirectionnelle c'est à dire des réseaux qui utilise des données futur pour interpréter le présent et bien sûr ceci implique que le réseau devra attendre le futur pour comprendre le présent voilà qui semble nécessiter un délai de compréhension des blagues des publicités et des films ou à ceux qui te paraît un délai de compréhension des vidéos cnce warhol est enfin on a parlé de drop out on avait déjà fait des liens entre les réseaux de neurones artificiels et les réseaux de neurones biologique on continuera en voit notamment dans les vidéos à venir puisqu'ils sont finalement les bonnes idées pour interpréter les données sensorielles qui ont été sélectionnés par la nature semble du coup très performante et du coup c'est sont une bonne idée que de haut - s'inspirer de certains de ces mécanismes naturels pour pouvoir programmer des intelligences artificielles et je chausse de saint-yvi fait la remarque que pour les réseaux de neurones naturel il ya à la fois des signaux électriques qui sont envoyées entre les neurones mais également des signaux chimiques notamment des molécules nourault transmettez aux qui peuvent par exemple faciliter les signaux ou les réduire en neurones à l'autre et jose de 7,8 demande du coup est ce que les informaticiens se sont inspirés de ça et est ce que c'est quelque chose a été programmé dans les antilles ou artificiel aujourd'hui à ma connaissance il n'y a pas vraiment de chose qui corresponde vraiment à ça mais c'est très très très possible que juste j'ai pas encore tombé sur ce genre d'article c'est très possible que cela soit déjà largement explorée mais à ma connaissance ça ne fait pas ce n'est pas un ingrédient indispensable en tout cas d importants d intelligence artificielle et plus performante d'aujourd'hui donc encore une fois je me trompe peut-être et si c'est le cas dites le dans les commentaires je serai ravi de lire un peu plus à ce sujet mais globalement l'impression que j'ai aujourd'hui et qui a amené possiblement à évoluer dans le futur c'est que ses idées de base des réseaux de neurones artificiels fondée sur les fonctions de l'activation et la rétrogradation des gradients est quelque chose qui me semblent incroyablement efficace d'un point de vue purement algorithmique en tout cas il me semble vraiment que ces notions de poids synaptique de signaux qui vont d'un neurone à l'autre et de fonctions d'activation sont vraiment des notions fondamentales ainsi que la rétro propagation j'ai points sans aujourd'hui du mal à voir des alternatives crédibles à cela pour entraîner des réseaux ou des modèles de façon très générale de très très très grande complexité comme les réseaux de drones avec des millions ou des milliards de paramètres j'ai l'impression aujourd'hui que ça c'est vraiment l'ingrédient de base après il ya plein d'ingrédients à mettre par dessus et peut-être qu'il ya d'autres paramètres sur lesquels on peut jouer smooth d'hyper paramètres sur lesquels on peut jouer comme par exemple il ya une discussion ce très récurrente sur les appels des learning waits c'est à dire une direction de gradient mais à quel point on suit cette direction de gradient est ce qu'on fait un pas de gradient ou 6 où est-ce qu'on peut se permettre de faire sans pas du gradient tout à coup donc ça ça correspond vraiment à la question du learning wake est peut-être que variés le learning wright au cours de l'apprentissage notamment peut-être avoir tu entends des sceaux brusque du lord ingrid qui augmente ça pourrait correspondre à avoir des espèces de neurotransmetteurs qui tout à coup facilite la transmission de signaux très spéculatif ce que je dis est sans doute qu'il faut beaucoup plus de recherches à ce sujet wen date de demande quelle est la probabilité selon moi qu'il existerait un mur technologie qui empêcherait les intelligences artificielles d'aujourd'hui d'atteindre le niveau humain et de surpasser alors c'est si tu vois un peu compliqué cette question parce que j'aime est très clair ce qu'on entend par les ja d'aujourd'hui sont plein d'améliorations au niveau des architectures plein de petites astuces comme ça à m on va parler aussi dans de futurs épisodes de mettre plusieurs modules les uns après les autres des réseaux qui avaient des feedbacks un terreau midi bref il ya beaucoup de progrès dans ce genre à faire peut-être même combinez cela avec d'autres idées algorithmique comme je sais pas moi-même cmc ou monte-carlo frissen pour dans lequel trois un peu dingo il ya plein de combinaisons possibles est-ce que ça appelle ça de l'intelligence artificielle d'aujourd'hui ou pas ma réponse bien sûr à cette question dépend de ce qu'on entend par quelles sont les technologies d'intelligence artificielle d'aujourd'hui mais globalement je pense qu'aujourd'hui on a déjà quand même fait pas mal le tour de pas mal des techniques fondamentales de base en tout cas de l'intelligence artificielle et j'aurais tendance à pareille qu'il n'y aura pas tout cas besoin de de révolution conceptuelle majeurs que sont plus des réflexions sur l'architecturé des réseaux de neurones comment publier plusieurs choses pour que ça se passe bien - est-ce qu'on peut exploiter au mieux les données et est la rétrogradation des réseaux de neurones pour faire de meilleures impressions de la forme lebesque quelle fonction d'activation utilisez plutôt que quelle autre est-ce qu'il faut pas réduire la connectivité des oiseaux neurones et surtout l'impression que j'ai celle que pour que nos réseaux de neurones d'aujourd'hui progressent beaucoup plus il faut surtout qu'ils grossissent en taille et en profondeur j'envoie notamment l'argument de turing nom apparaît dans l'épisode 7 sur la taille nécessaire des réseaux de neurones pour atteindre le niveau humain moi j'aurais tendance à parier avec ses fortes crédence que tout ça ça suffira pour atteindre une audience artificielle de niveau il faut simplement bien combiner tous les différentes idées que l'on a aujourd'hui et faire des algorithmes de plus en plus gros qui analyse de plus en plus de données pour y parvenir à la question un petit peu floue du coup c'est un peu difficile de progresser d'être assez confiant notamment sur l'importance de dessins de grignan stochastiques donc je mettrais quelque chose comme 95% mais encore une fois ça dépend de ce qu'on entend par la question qui était posée ensuite notre question qu'on peut se poser c'est quelle est la probabilité ou moins lié un mur technologique entre l'intelligence artificielle et l'intelligence humaine et là par contre la probabilité qu'il existe un mur technologique nous semble vraiment vraiment épisode la questionne de très faible activité ou un jeu mettra peut-être ypsilon de l'ordre 1 sur 10 puissance 20 disques ans 30 tiennent vraiment vraiment très très très très faible pourquoi parce que j'en parle notamment dans mon livre sur la formule de bise il semble que l'intelligence correspond essentiellement à calculer les probabilités de bisous des approximations de la forme de belize et optimiser les prises de décisions pour maximiser les conséquences en fonction de notre fonction d'objectifs et si on croit en ce formalisme qui me semble vraiment très très très loin quand j'en parle très longuement dans mon livre l'intelligence ne s'agirait vraiment que de calculs et à ce moment là s'il ne s'agit que de calculs j'ai bien peur que les technologies sont capables de faire des calculs plus efficace plus performants que nous peut-être pas aujourd'hui mais dans le futur au fur à mesure qu'il y a ce progrès technologique et surtout si on se dirige vers du hardware spécialisé pour les calculs de descendre de gradient ce gaz est typiquement dans des réseaux de neurones par l'elysée tout ce qu'il faut là j'ai bien l'impression que les performances calculatoire des machines seront largement supérieur au nôtre si c'est le cas et ça me paraît très très très très probable que ce sera le cas il me semble très très très improbable qu'il y ait un mur technologique entre l'intelligence des machines et l'intelligence humaine et j'espère que j'avais aimé cette vidéo la projection on va parler de l'autre grande architecture des réseaux de neurones qui est très très utilisés à savoir les réseaux de convulsions si vous avez aimé cet épisode dont celle de l'ikea leconte et à les partager et j'espère que vous serez là [Musique]