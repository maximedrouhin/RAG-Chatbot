[Musique] [Musique] en 1950 pour tester les capacités cognitives des machines qui aurait une propose de tester leur capacité à avoir des comportements indiscernable des comportements humains c'est ce que turing a appelé le jeu de limitations que l'on appelle désormais le test de turing alors on pourrait rétorquer qu imiter ce n'est que faire de limitation et qu'il ne s'agit donc que d'un test d'escroquerie certes reste que pour devenir un bon escroc il faut tout de même acquérir deux sacrés compétences je l' ai appris tant bien que mal en essayant d'imiter les nordmann cyprien et autres miquel launay pour devenir youtuber ou pour essayer de danse est assez correctement quoi qu'il en soit ce principe d'imitation est aujourd'hui au coeur de l'architecturé du plus spectaculaire des réseaux de neurones à savoir les réseaux de neurones générateur adverse à rioz aussi connu sous le nom de générative agresseurs networks ou guys et les gagnent c'est vraiment un truc de fou kelis c'est un truc de malade [Musique] en particulier il ya un an nvidia a demandé à son gagnent d'imiter des images photoréalistes de célébrités et voici les performances du gan pendant des heures et des heures le gan créa de toutes pièces des imitations d'image de célébrités sauf que ses imitations représenter du coût des célébrités fictive inventé de toutes pièces voici deux images de célébrités l'une d'elles a été fabriquée de toutes pièces par le gan de nvidia laquelle est ce selon vous est-ce l'image du mec ou de la fille je vous laisse répondre dans le sondage à l'écran et que ceux qui connaissent la bonne réponse se taisent pas de spoil dans les commentaires ok mais comment l' architecture des gannes permet tel d'entraîner les gan affaires de limitation et bien l'idée c'est comme le nom l'indiqué d'introduire une sorte d' adversaire aussi appelé discriminant plus précisément on aura un réseau de neurones générateurs en charge de créer des fake imitant les images photoréalistes et un réseau de neurones adverse arial en charge d'aider l'imitateur à déterminer ce qui fait bien et ce qui fait mal donc en fait l'adversaire n'est pas vraiment un adversaire on pourrait voir cela comme un enseignant ou en critique bienveillant quoi qu'il en soit générateur et adversaires forment ensemble un réseau générateur adverse arial google mais bien sûr pour que l'adversaire puisse émettre des critiques sens et il lui faut apprendre la différence entre les images authentiques et les imitations l'apprentissage du gan se décompose donc en deux phases d'abord montre plein de données authentiques et fake pour aider l'adversaire à distinguer les deux puis l'adversaire dit aux générateurs comment ces images doivent être modifiées pour davantage ressembler aux authentiques et l'astuce technique pour permettre cet enseignement personnalisé du générateur par l'adversaire c'est l'algorithme de rétro propagation oui parce qu'au bout du réseau adverse arial il y a en gros une note qu'il dit à quel point le fait qu du générateur ressemble à une donnée authentique et c'est cette note pour ces faits que le générateur à vouloir augmenter la rétro propagation permet de remonter les gradients est de savoir comment les activations des neurones observable de l'adversaire doivent être modifiées pour améliorer au mieux la note que vraisemblance en interconnectant les réseaux adversaire you est générateur on peut alors remonter les gradients au niveau du générateur qui pourra alors mieux déterminer comment modifier ces paramètres et notamment ses pois synaptique pour que les images qu'ils génèrent soit aussi indiscernable des authentiques que possible donc si je récapitule l'idée dé gagnent c'est d'entraîner un imitateur en lui fournissant des gradients calculé par un adversaire qui a appris à noter le niveau de ressemblance entre un fake est un authentique plus précisément l'adversaire calcul une probabilité que l'image qu'on lui donne soit un fake et on l'entraînent à donner des probabilités adéquate en pénalisant la perte logarithmique moyenne quand on lui expose aléatoirement des fake et des authentiques c'est lié à la divergence calme et je garde ça pour une prochaine série bon ça c'est l'idée globale mais bien sûr il faut s'attendre à ce que le générateur s'améliore et à ce que l'adversaire devra ensuite ré apprendre à distinguer les authentiques des nouveau fake en fait le gan consiste à alterner des phases où l'adversaire apprend à distinguer les authentiques défèque du générateur et où le générateur apprend ensuite grâce au feedback de l'adversaire comment rendre les fake plus indiscernable des authentiques et de façon stupéfiante ce procédé tout simple a permis de concevoir des réseaux générateur capable de créer des images photoréalistes de célébrités que les réseaux adversaries auront bien du mal à discerner des authentiques que ces réseaux adversaires reçoit des réseaux de neurones artificiels ou des cerveaux de primates et tout ça ça marche déjà super bien les applications sont à la fois nombreuses et stupéfiante en exploitant la sémantique profonde que les réseaux générateurs en viennent à créer à la suite de l'apprentissage lagan dispose ainsi d'une sorte de représentation adéquate de l'ensemble des données qui leur sont exposés vue-là qui leur permet par exemple d'augmenter de manière spectaculaire la résolution d'image très pixélisé on peut également appliquer les ghana l'édition automatique d'image qui consisterait par exemple à gommer certaines parties de l'image que l'on trouve moche par exemple est demandée hogan de reconstituer les images sans ces trucs moches [Musique] les gan permettent également d'imaginer à quoi ressemblerait un paysage données observées dans d'autres conditions météorologiques et puis de façon plus stupéfiante encore légales peuvent nous faire danser et pas juste danser comme un idiot mais même dansé avec la même posture et le même rythme que les danseurs professionnels bon malheureusement je n'ai pas réussi à trouver le code source du guen qui fait ça et c'est bien dommage car j'aurais bien aimé une image de moi qui danse de façon non ridicule bref avec les gagne de nombreuses applications stupéfiante inimaginable il ya quelques années semble désormais à portée de main comme la création de vidéos fabriqué de toutes pièces qui peuvent faire faire et faire dire n'importe quoi à n'importe qui [Musique] obama yes we can reduce dynamique et oui tous ces trucs trop cool pose également de sérieux problèmes d'éthique mais bon je pense que vous commencez à comprendre à quel point ces problèmes mouscron du coup je vais laisser ça de côté pour aujourd'hui pour le reste de cette vidéo la question qui va nous intéresser c'est la suivante est ce que cette idée de réseau adversaire you est vraiment la bonne façon de faire du machine learning après tout ce que je vous ai présenté jusque là ça ressemble davantage à une liste d'astuces d'un générique à une théorie solide est ce qu'il est possible d'affirmer qu'en un sens assez fort les gagnent sont la bonne architecture voir l'architecturé ultime pour faire du machine learning alors tout d'abord il est bon de signaler qu'une propriété remarquable des gan qui est différent si d'autres architectures notamment c'est le fait que les gagnent sont entièrement non supervisées autrement dit pas besoin d'étiqueter les données pour permettre au gan de bien fonctionner il suffit de données hoogan une énorme banque d'imagés authentique vous pourrez trouver sur des tas de sites comme pique sa belle les gars ne se chargeront ensuite du reste toute seule ok les gars ne font de l'apprentissage non supervisées mais est-ce à la bonne manière de faire de l'apprentissage non supervisées qu'est ce qu'on veut vraiment faire au final quand on effectue de l'apprentissage nous superviser et même de façon plus générale encore qu est-ce qu à prendre aux caisses qu'à prendre aujourd'hui cette session fascine toutes sortes de chercheurs les informaticiens devraient rendre leur algorithme intelligent les philosophes aimera garantir la fiabilité des sciences et les étudiants aimerait valider les examens sans aller en amphi et là je ne peux m'empêcher de citer l'un de mes grands héros le géant reis l'aumône of je cite en sciences mon intérêt concerner davantage la manière dont les choses étaient découvertes que les contenus des découvertes l'oeuf doré n'était pas aussi excitant que loi qui les condamne les contenus de découverte saignant mais effectué aurait effectué des découvertes c'est tellement plus floue et le génie de solomonoff fut de découvrir le moteur des découvertes en combinant l'algorithmique avec une étrange philosophie appelé le bail à nice mme pour les adeptes du bayer zanis mme apprendre c'est ni plus ni moins appliqué la formule de bise en particulier cette version de la formule de bayes qui nous invite à calculer la probabilité des théories sachant les données et là c'est le moment historique que vous êtes sortir pour la première fois du youtube internationale ma casquette bayésienne sauf bien sûr si vous avez respecté mon livre vous savez sans doute aussi que cette merveilleuse équation est horriblement dur à calculer elles mêmes en un sens précis incalculables du coup il nous faut nous contenter d'approximations de cette équation pour commencer la première approximation que l'on va faire ça va être de ne se restreindre qu'un an ensemble de théories qu'ils sont gentils en un sens c'est à dire des théories qui seront faciles à manipuler algorithmique morts et comme vous l'aurez compris il ya beaucoup de bonnes raisons de penser que parmi les théories les plus gentils on trouve les réseaux de neurones notamment parce qu'il se prête très bien à la descente de gradient via la rétro propagation en fait plus précisément on va considérer toutes les théories qui correspondent à une architecture de neurones fixé mais dont les paramètres peuvent varier il reste alors à trouver les bons paramètres et pour cela ce qu'on va faire c'est en fait chercher les paramètres qui sont les plus crédibles au vu des données c'est à dire celle qui maximise la probabilité de la théorie sachant les données on parle de maximum a posteriori hockey et donc pour déterminer les paramètres et le plus crédible d'après la formule de bèze il nous faut déterminer celles qui maximise le produit de la probabilité des données sachant les paramètres par la probabilité a priori des paramètres bon je vais pas énormément rentrer dans les détails mais la probabilité a priori des paramètres ça correspond à la régularisation de l'épisode 18 est bon on va un peu l'ignorer pour aujourd'hui il nous reste alors plus qu un terme à considérer qui correspond vraiment à la capacité de la théorie ici le réseau générateur a expliqué les données c'est-à-dire dans notre cas la banque de données d'image ou de vidéos ou autres et bien dans le cas des réseaux générateur la probabilité en particulier d'une image ça correspond à la probabilité que le réseau génère exactement cette image pour comprendre cela il faut que je vous dise que pour générer une image le réseau générateur prend une graine aléatoire qui correspond à une activation aléatoire de certains de ces neurones initiaux et applique ensuite les calculs basique des réseaux de neurones dans un pareil dans l'épisode 41 pour générer une image et bien si on répète le processus générateur avec plein de graines aléatoire le réseau générateur finira par générer exactement l'image donnée et bien le temps que ça prendra six ans grohl inverse de la probabilité de l'image donnée selon le réseau de neurones générateurs on peut ensuite déterminer la probabilité de la banque de données d'image au multipliant ainsi les probabilités obtenu pour les différentes images et ça c'est en principe ce qu'il faudrait faire pour calculer la probabilité des données sachant une configuration des paramètres du réseau générateurs malheureusement c'est un calcul un faisable en pratique surtout si images sont de très haute résolution en effet si on a un million de pixels alors le temps qu'il faudra pour reconstituer exactement une image donnée sera de l'ordre de 10 puissance 1 million ce qui est bien pire qu'un nombre cryptographiques petit aparté les nombreux cryptographique on va bientôt en parler sur la chaîne string theory bref il nous faut faire encore une approximation du calcul bayésiens et bien l'idée dé gagnent c'est d'utiliser un réseau adverse arial pour calculer une approximation de cette probabilité impossible à calculer autrement en cela les gars ne font partie d'une famille plus large d'inférence bayésienne dit sans vraisemblance pour des raisons un peu obscur qui agace un peu le buzz est en moi cette famille contient aussi par exemple les à proximité jeune compétition et les paramétrique belge en direct la croûte et ces derniers ont d'ailleurs a été utilisé dans la thèse un excellent youtuber scientifique après c'est pas parce qu'une thèse contient le mot b jeunes que c'est forcément une bonne thèse mais bon dans le cas de sébastien ça parle aussi de ses créateurs donc si tu montes ceci dit ce qui distingue les gagnent des autres inférences bas et viennent sans vraisemblance c'est que parce qu'il s'agit de réseaux de neurones segan se prête très bien à l'apprentissage des paramètres guidés par des gradients autrement dit illégal semble bien se tenir à l'intersection entre approximations de la formule deux pays'' et optimisation des paramètres par descente de gradient et ça ça fait que je parie centraux flipper que les gan joueront un rôle central dans les yeux du futur [Musique] [Applaudissements] dernière fois en a pas réseau très profond et comment réussir à les entraîner malgré notamment l'évanescence du gradient qui est le phénomène observé de façon empirique et qu'il anticipe également très facilement avec la zone dans tes oreilles tout simple qui vient du fait que il ya un système multiplicatif que l'on observe notamment au niveau du gradient lorsque l'on à des réseaux profond et d'une succession de beaucoup de multiplication conduit à une croissance exponentielle qui peut conduire un explosion exponentielle ou une évanescence exponentielle et benoît quand on nous demande quel est l'effet des fonctions de l'activation sur cette évanescence exponentielle du gradient et bien si vous faites un petit peu le calcul vous verrez que lorsqu'on remonte le gradient et bien ce qui va compter ça va être la dérivée de la fonction d'activation à son niveau d'activation données et du coup on pourrait se dire que des fonctions de l'activation ou les gradients sont importants vont conduire plus une explosion exponentielle du gradient alors que les cas où la dérivée du gradient est faible qu à ce monde bus à une évanescence exponentielle du gratin du coup à partir de là on peut se dire qu'avoir fait des fonctions d'activation qui sont très plateau en certains endroits typiquement lorsque je prends des sigmoïde en fait elles sont très plate dès qu'on s'écarte un petit peu du centre ou quand je prends des fonctions directif ailleurs ligne henry unit c'est complètement plat à gauche pour éviter ce genre de cas on pourra remplacer les rectifier leur ligne 8 par des fonctions valeur absolue je crois que c'est un nom spéciale en terminologie machine learning mais bon c'est juste la fonction valeur absolue et équipements et cette fonction valeur absolue - sa dérive est toujours égale à 1 ou qu'on se place et du coup on peut dire qu'il aura jamais un gardien qui deviendra 0 après avoir traversé avoir peut-être remonter cette fonction d'activation ccdh il ya également des défauts avoir des fonctions de l'activation comme ça qui sont en fait sans borne qui peuvent prendre des valeurs aussi grande que l'on veut parce que du coup les signaux qui sortent de ces motions d'activation sont également un bon guide sont sans borne goût une synapse pourrait avoir tout le poids du gradient et le reste pour avoir un effet négligeable et du coup l'apprentissage pourrait être synapse par synapse et aucun tweet ivement c'est un truc qui paraît très insatisfaisant puisque ça ferait beaucoup de libération pour un apprenti pour l'apprentissage d'ailleurs pour régler ce genre de problème il ya des techniques qui sont utilisés comme la baci normalisation ou la normalisation du bac je sais pas trop pourquoi s'appelle battu d'ailleurs mais l'idée c'est vraiment que la sortie le signal de sortie dans une synapse qui traverse une synapse ce signal de sortie doit jamais prendre des valeurs trop grande ni trop petite idéalement en fait ce genre de signaux qui sortent doit être toujours à peu près du même ordre de grandeur et ça en fait c'est vrai du signal à travers assignables c'est aussi vrai des poids à synaptique c'est aussi vrai des fonctions de l'activation donc plus généralement en fait il semble que les réseaux de neurones qui apprennent bien ce sont des réseaux de neurones ou tout l'autre réseau apprend à peu près au même rythme tous les paramètres sont a appris à peu près au même rythme et en particulier l'évanescence du gradient est un vrai problème puisque une certaine manière les couches profondes apprennent beaucoup plus vite mende et des normes du gradient beaucoup plus grandes que les couches initial et donc il semble en tout cas empiriquement et a sans doute ça va pouvoir se justifier théoriquement même si j'ai pas vu passer de papier qui ont paraît il semble qu'un bon apprentissage en apprentissage ou tout le réseau apprend ensemble à peu près au même rythme et où toutes les informations sont à peu près du même monde même ordre de grandeur partout dans le réseau ça pour dire que les fonctions qu'il devienne pas un moment pose problème parce qu'elle donne plus un risque de débats naissance du gradient mais les fonctions qui sont sans borne pose également problème puisqu'elle risque de causer des signaux qui sont par endroits beaucoup plus fort que dans d'autres endroits ce qui est il sera également néfaste à l'apprentissage du réseau d'une certaine manière il ya un peu un équilibre à trouver dans l' architecture globale du réseau pour que tout le réseau apprennent ensemble et à ma connaissance à part cette idée un peu intuitif il n'y a pas vraiment de bonnes recherches en tout cas théoriques sur cette question adrien de cause lui demande s'il ne serait pas possible de créer une machine ou un algorithme qui générerait des algorithmes capables de bien apprendre de résoudre les premiers apprentissages ça c'est un truc qui j'ai pas beaucoup étudié déjà désireux en gros qui sont capables de générer des murs rire ou du moins des meilleurs architecture de réseaux de neurones qu'un humain ne serait capable de le faire typiquement une des idées qui est d'ailleurs été proposée par turing en 1950 c'est d'effectuer un apprentissage génétiques sur les meilleurs architecture de réseaux de neurones donc avec des idées des algorithmes génétiques j'en ai déjà parlé plusieurs fois dans les commentaires des vidéos malheureusement j'ai pas préparé de vidéos séries sur l'île telle qu'elle est mais l'idée de base c'est en gros d'avoir plusieurs formes d'architecturé plusieurs paramètres de ces architectures et d'essayer de mélanger temps en temps certains paramètres donc ça revient un petit peu à faire des croisements génétiques entre certains à certaines architectures de réseau à regarder un petit peu comment elles performent et puis ton temps à supprimer de l'ensemble de la population de l'architecturé de réseaux de neurones les architectures qui sont moins performantes que d'autres vraiment ça revient un peu à simuler finalement la manière dont le cerveau tellement humain a fini par acquérir les capacités qu'il a acquise finalement c'est cet algorithme génétique qui a permis à la sélection naturelle et à la nature de créer des machines à penser aussi performante que le cerveau non j'espère que vous avez aimé cet épisode c'était le dernier sur les architectures de réseaux de neurones c'est aussi je consulte plus intéressant est le loin que cette idée l'architecturé de réseau adverse à rioz dans les deux prochains épisodes on va revenir sur des questions plus théorique on va se demander notamment qu'est ce qui fait qu'ils le succès des réseaux de neurones profond qu est ce qu il ya de façon théorique de justifier le fait que plus un réseau et profond plus il est adapté pour apprendre et pour bien comprendre le monde qui l'entouré en particulier le prochain épisode est un épisode qui va explorer les relations entre le diplôme ni et les fractales voilà le truc vraiment trop classe du machine learning électrique d'armes entre classes de la géométrie on verra ça la semaine prochaine si vous aimez cette vidéo pensez à le lac est le commentaire de partager frontalière vous abonner pour perfusion d'épisodes merci au type colorant et j'espère que vous serez là la prochaine foire du carré des jalles rights watch vous êtes