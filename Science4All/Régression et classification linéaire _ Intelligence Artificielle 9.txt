la dernière fois on a introduit la notion d'apprentissage superviser rapidement en gros l'apprentissage superviser consiste à analyser un jeu de données qui consiste à un ensemble de perrefitte chose les boys ou reconstruire une fonction de prédiction c'est à dire une fonction qui calcule délai bol y pour des figures os x alors présenté comme ça ça peut donner l'impression de quelque chose de très machine learning by jiss on entre tout ça sauf qu'en fait l'apprentissage superviser date de moins de 250 ans à l'époque où les premiers chercheurs en intelligence artificielle et en machine learning inventaire la régression linéaire en ce sens le premier chercheur en il ya de l'histoire pourrait être un certain roger joseph bosco vic autour de 1755 pour interpréter des mesures géographique les détails importe peu toujours est il que parce que richie s'est retrouvé avec une dimension du free jazz une dimension de l'ibs et plein de points dans cet espace sauf que boscovich savait que les les buzz de ces données étaient des mesures empirique et était donc nécessairement imprécises et postulats alors que ces mesures étaient erronées il chercha à décrire les données comme devant être sur une droite avec des erreurs de mesures verticales qui éloigne les données de la droite à la fin du xviiie siècle le géant pierre simon laplace formalisera les intuitions géométrique de boscovich définissant ainsi la méthode moindre déviation absolue ou liste absolute déviation pour trouver une régularité dans les données de façon cruciale comme klm dont on a parlé dans l'épisode précédent l'algorithme de machine learning de buzz kovic la place permet d'exploiter la droite calculé pour effectuer des prédictions pour les features encore non observé si je récapitule l'algorithme de boscovich la place est incroyablement simple on a un nuage de points il suffit de trouver une droite qu'il colle assez bien à ce nuage de points et on utilise ensuite cette droite pour effectuer des prédictions nous parle alors de régression linéaire puisque on ramène la complexité d'un nuage de points à la simplicité d'une droite or une droite en fait c'est un ensemble de deux paramètres décrivent d'un côté l'inclinaison de la droite que l'on appelle sa pente et de l'autre côté sa hauteur que l'on appelle sont ordonnés à l'origine oui ce sont les a et b de l'équation y est égal à ax suspect ainsi la régression linéaire c'est résumer tout un jeu de données à seulement deux nombres pas mal alors dit comme ça avec un séminaire ça paraît simple et pourtant déjà avec ce cas simpliste ce n'est pas si simple au début des années 1800 legendre ygos proposèrent une approche alternative pour calculer une bonne droite qui traverse un nuage de points ils montrèrent que leur approche était optimale si l'on cherche à minimiser la somme des carrés des erreurs et c'est pour ça que l'algorithme de legendre gosse est aujourd'hui davantage connue sous le nom de méthode des moindres carrés mais du coup on a deux façons différentes de tracer une droite à travers un nuage de points d'un côté il ya les moindre déviation de boscovich la place et de l'autre n'a les moindres carrés de legendre gosse laquelle des deux approches la meilleure est-ce qu'il vaut mieux minimiser la somme des erreurs ou la somme des carrés des erreurs j'aime bien cette question parce qu'elle en fait incroyablement profonde il y aurait tellement de choses à dire ce sujet mais comme j'ai déjà beaucoup de choses à vous dire aujourd'hui je vais laisser cette question en suspens et je vais vous l'essayez réfléchir sachant que personnellement je pense que la bonne réponse à izel toujours est il qu'aujourd'hui à l'école ou à l'université on enseigne qu'une seule de ces deux méthodes à savoir la méthode des moindres carrés de l'eau gendre gosse et on a tendance à complètement ignoré les moindre déviation de boscovich la place et tout ça de façon amusante surtout à cause de la place et son théorème centrale limite bon je vais laisser ça de côté en fait aujourd'hui la quasi totalité de la communauté scientifique utilise la méthode des moindres carrés de le joindre gosse pour déterminer des relations entre des variables comme pv légal nrt ou disons la manière dont l'ancienneté et la productivité d'un youtuber scientifique affecté son nombre d'abonnés les moindres carrés de legendre gosses sont particulièrement utilisés pour calculer la corrélation entre des features aider les boys de façon grossière la corrélation mesure à quel point la régression linéaire colo donner cette corrélation est plus un quand la régression linéaire monde et contient les données elle est égale à -1 quand elle descend et contient les données et elle est proche de zéro si la droite ne passe pas franchement par les données et d'ailleurs ça ça montre les limites du concept de corrélation vous connaissez sans doute le dicton qui dit que corrélation n'implique pas causalité si ce n'est pas le cas je vous recommande vivement cette vidéo de la statistique explique et amoucha mais indique t on au moins connu est celui qui dit que causalité n'implique pas corrélation par exemple imaginons le label y est égale à e puissance x alors y est clairement causé par x puisque on l'obtient vient à calcul déterministe mais pour x entre 0 et 100 la corrélation est inférieure à 30% et je ne parle pas du chaos x implique y de façon non monotone bref la corrélation a beaucoup de limites et on le fait souvent dire des choses qu'il ne dit pas ce qu'elle mesure vraiment c'est la performance de la régression linéaire pour décrire un nuage de points et c'est déjà pas si mal ça permet de donner une idée d'à quel point on peut faire confiance à la régression linéaire pour prédire des labels étant donné des piqûres en effet il ya une autre façon intéressante est souvent utilisée d'interpréter la corrélation plus tôt le carré de la corrélation errecart est aussi appelée coefficient de détermination en effet regardons les erreurs de la régression linéaire et comparons les aux variations des les bowls indépendamment de tout lien avec les phishers on peut montrer que les erreurs de la règle et soul in her sont toujours moins grande que les variations des les boys ça revient à dire qu'avec la régression linéaire on a diminué notre incertitudes quant aux valeurs des levels et bien vers carey est essentiellement cela il s'agit de 1 - le rapport des erreurs moyenne par les variations des levels ce que l'on interprète souvent comme étant la proportion de la variance des levels que l'on parvient à expliquer à l'aide de la régression linéaire en gros quand air carré est égal à 1 alors la régression linéaire explique entièrement le lien entre les futurs hôtels et les bosses mais si air car est à peu près égale 1 0 alors la récréation linéaire apporte rien ainsi air carré est un bon indicateur de la validité de la régression linéaire alors jusque là je n'ai parlé que de droite et ça c'est parce que l'espace des phisheurs ce que j'ai considéré était de dimension 1 mais contrairement à cannes aime d'ailleurs la régression linéaire se généralise en fait très bien aux dimensions supérieures ainsi l'espace des phisheurs et de dimensions 2 c'est à dire si on cherche à prédire le label y à partir de deux fincher x1 et x2 alors on pourra représenter les données par des points dans un espace de dimension 3 et la régression linéaire consiste à trouver le plan qui colle le mieux au nuage de points de cet espace plus généralement si l'espace des phisheurs et de dimensions d alors la régression linéaire sera le calcul d'un hyper plan de dimension des à l'intérieur d'un espace de dimension des plus sains ou là des puces 1e dimensions et celle des levels alors ça peut faire peur dit comme ça ni père plan dimension d mais de la même manière qu'on peut représenter une droite part de nombre on peut représenter un plan par trois nombres et un hyper plan de dimension des parts des puces un nombre d'un point de vue calculatoire la régression linéaire pour un espèce de features des dimensions des correspond simplement a résumé tout un jeu de données par le calcul de des puces un nombre et le truc génial c'est que le jeu entre gosses nous ont décrit un algorithme c'est à dire qu'ils ont décrit les étapes de calcul nécessaire pour calculer ces des puces un nombre bref la régression linéaire c'est un outil de calcul puissant qui est très utilisée aujourd'hui et là vous allez me dire mais du coup pourquoi est ce qu'on a besoin de réseaux de neurones et bien le problème de la régression linéaire c'est qu'elle ne décris que des relations lumière sauf que beaucoup de relations entre fishers et les boys ne sont pas linéaires à commencer par le cas où les levels ne sont pas des nombres mais par exemple des like dislike ou par exemple des spams pas spam ou par exemple pour faire de la reconnaissance d'objets à ce moment là pour quand même exploiter les structures linéaire qui ont le bon goût d'être simple à conceptualiser et à calculer on peut chercher à essayer de coller un peu à la régression linéaire par exemple au lieu de considérer juste été like dislike on peut créer artificiellement des échelles plus ou moins like this light malheureusement les données seront toujours tout en haut où tout en bas de cette échelle et du coup faire passer une droite par ce nuage de points ça risque fort de ne pas franchement le faire mais ce que l'on peut faire c'est une presque droite ou plus exactement on va calculer un point de rupture et broder autour de ce point de rupture soit à l'aide d'une jolie courbe en s soit l'aide de morceaux de droite prenons le cas où l'espace défi job est de dimension 2 pour mieux visualiser ce point de rupture dans ce cas le point de rupture ne sera pas un point mais une droite la couture se fera alors dans la direction perpendiculaire à la droite et pour des features de dimension des le point de rupture sera maintenant un hyper plan de dimension des moyens avec là encore une couture dans la direction perpendiculaire ce qu'on a construit là ce sont des classiques year linéaire alors en fonction de la couture utilisé pour coller les différents bout du tice différents noms pour la jolie couture en forme de s on parle de régression logistique et dans le cas des morceaux de droite on parle de svm pour se portent vector machine mais le détail importe peu ce qui est intéressant de voir c'est que un peu comme pour la régression linéaire déclassifier or linéaire pour un espace des features de dimension des peuvent être décrits par des non pas mieux encore il existe des algorithmes efficace à qui vous pouvez donner tout un jeu de données et qui calculeront automatiquement et rapidement les des nombres de la classification linéaire et de nos jours ces classifications linéaires sont très utilisés qui vit quand l'une des premières très gros application de la classification l'ine était la classification des emails en spam pas spam mais ces classifications linéaires ont aussi leurs limites et je vais finir avec une petite image à l'écran ça c'est ce que l'on appelle le ou exclusifs ou xor et pendant longtemps c'était l'argument conclusive contre les méthodes lean et y compris la classification linéaire ou pas je vous laisse y réfléchir la dernière fois on a parlé de notre premier algorithmes de machine learning qui a été calme et ça me semble encore eric au fait remarquer qui en fait des hypothèses que j'ai oublié d'expliciter pour que le théorème de cover ehrhart s'applique en particulier faut que la loi de probabilité des données soit une combinaison de d'irak et de distribution absolument continue par rapport à la métrique de l'espace en particulier il faut que seul m finissent par être vérifiée à savoir que si vous avez une donnée qui est tirée selon la loi de probabilité alors avec probabilité 1 en tirant d'eau plein d'autres données vous finirez par avoir une suite de données qui vont se rapprocher de la donne et que vous aviez tiré initialement prono giraud sauveur se demande s'il n'est pas possible d'améliorer la complexité en temps de l'algorithme canet ce qui est possible de faire une prédiction finalement très rapide on entend qu'il ne dépend pas énormément de la taille des données collectées c'est une question très intéressant d'ailleurs je me la suis posée en préparant notamment la vidéo de la semaine dernière il se trouve que si la dimension des featurettes égal à 1 ou à deux la réponse est oui on est capable d'avoir des algues au rythme très rapide pour faire la prédiction de cayenne malheureusement en dimensions supérieures il semble que la réponse soit non ma connaissance encore une question ouverte mais d'après ce poste de sur stock exchange il semblerait que résoudre ce problème en transe ou linéaires correspondrait à résoudre le problème de satisfaits habilité en temps inférieur à deux puissances n aujourd'hui à la plupart des pharmaciens pense que c'est pas possible bref on n'a pas prouvé que c'était impossible de voir un algorithme vacances ou linéaires pour appliquer caennais non mais j'ai envie de dire que a priori les réponses viennent parce que si vous arrivez à prouver que la réponse est oui pour toutes dimensions alors je pense qu'un prix turing vous attend du prix turing pour ceux qui connaissent pas c'est l'équivalent du prix nobel pour l'informatique un vidéo précédentes on avait vu que le gros problème de knn était le fait que les complexités d'échantillonnage de cannes n était très très grande en fait la complexité échantillonnage semble grandir de façon exponentielle avec la dimension de ça c'est très mauvais comme l'expliquent sauf ça aux âges et emmanuel jacob c'est ce que l'on appelle la malédiction de la vie mention ou encore le fléau de la dimension c'est le fait que les espaces de grandes dimensions se comporte pas forcément comme les espaces de dimensions plus petites que l'on connaît bien pour ça qu'il faut faire vraiment très gaffe machine learning a souvent des idées qu'on peut avoir qui marchent en petites dimensions mais il faut absolument voir comment ça marche quand la dimension de l'espace grandi et en particulier si la complexité du problème grandi de façon exponentielle c'est pas bon alors amis de deux riches se demande du coup s'il n'est pas possible de réduire à la main des dimensions de l'espace en enlevant certaines dimensions peu pertinentes comme ça à la main c'est vraiment pas facile et a priori en devançant général si vous écartez comme ça des dimensions bas après vous perdez de l'information est possible que l'information utile pour effectuer les prédictions disparaissent lorsqu on aurait des dimensions hanoi a fait alors remarquer qu'il ya une méthode qui s'appelle l'analyse par composante principale principales composantes analysis en anglais on appelle cipi sièges il ya aussi une variante d'officiels qui s'appelle single of value 10 composition ou svd en gros l'idée de ces approches c2c d'enlever les dimensions qui semblent pas pertinentes de façon automatique de détecter automatiquement les dimensions dans lequel on fait un peu de variation des données pour essayer d'orienter l'espace pour regarder uniquement les dimensions sur lesquels il ya une forte variation dans les données alors en effet ces méthodes sont vraiment génial marche rentrée très bien mais elles ont aussi leurs limites et on en reparlera dans des prochaines vidéos je vous ai compris se demandant s'il serait pas possible de choisir un petit peu plus à la main de choisir quels sont les features qu'ils ont faites sont pertinents à la prédiction et ça pendant longtemps c'était ce qui était fait en machine learning mais aujourd'hui le paradigme notamment du diplôme ning cessé de faire en sorte que toute la sélection tout tout ce travail en fait ce prêt mâchage des données finalement soit au entièrement automatisés et qu'il n'est pas besoin d'une intervention humaine pour pouvoir ensuite faire des prédictions en particulier du coup aujourd'hui le machine learning notamment le diplôme ou qui cherchent à voir directement des données brutes analyser ce machine en ingla sera obligé d'avoir des espaces de très très grande dimension et tous ces espaces de grandes dimensions ça fait forcément appel à de la gemme linéaire donc atomix se demande si pour apprendre par exemple le diplôme ning c'est vraiment nécessaire de maîtriser ces notions deuxième lumière la réponse est vraiment oui malheureusement l'aja binaire est vraiment vraiment centrale aux techniques de machine learning d'aujourd'hui tout comme certaines notions de base de l'analyse et de la théorie des probabilités donc ça fait évidemment beaucoup de choses à apprendre pour la dja binaire en particulier fait une série de vidéos sur when did a et je recommande aussi très vivement la série de vidéos de football bat pour l'analyse aussi j'ai fait quelques vidéos sur you wanted à effet brandon a aussi une excellente série sur l'analyse souvent c'est sont pas forcément des vidéos qui sont ultra scolaire donc c'est très utile d'avoir un manuel scolaire pour pouvoir vraiment apprendre ces choses je pense aussi que ces vidéos aideront beaucoup à avoir une intuition de vraiment ce qui se passe dans ces espaces aux dimensions de trélon dimensions ou dans ses calculs infinitésimaux bref de façon peut-être à signer attendue du match de prépa sont en fait souvent très très très utile pour pouvoir faire notamment des machines à cormont 79 lâché un petit spoiler apparemment un trigone peppers parle pas de pentagone je crois je vais pas aller le voir du coup par coup qu'il lixon du dos qui est sorti sur nes de sciences où il parle de pentagone et la vidéo est plutôt cool romain miller et mon compte le tubulaire remarquer que nous code génétique dépendent aussi de notre environnement et se demande du coup ci ça remet pas en cause la faible complexité de solomonoff du cerveau enfant en fait j'ai envie de dire que d'une certaine manière la modification du code génétique par des méthodes de des pigeons tic notamment c'est en fait un apprentissage de l'environnement c'est une façon dont l'environnement influence le code génétique et donc la structure du cerveau de l'enfant donc là encore ça correspond d'une certaine manière a volé de la complicité de solomonoff utile de l'environnement pour pouvoir améliorer l'intelligence du cerveau d'équipement 55 se demande si du coup si on regarde toute l'évolution de l'espèce humaine vu qu'à l'origine il ya sans doute une bataille très simple avec une boîte génétique très très simple est-ce qu'on pourrait pas dire finalement que la complicité de seulement 9,2 m en fait se réduit à cette complexité très simple initiale et raconter non parce qu'en fait dans ce processus de sélection au cours de l'évolution il ya eu beaucoup d'influencé de l'environnement et du cumul certaines aires beaucoup de machine learning sur les codes génétiques et des espèces vivantes qui a eu lieu et d'ailleurs ce processus de sélection naturelle c'est quelque chose qui avait été proposé par turing dans son papier de 1950 pour pouvoir sélectionner les meilleurs learning machine initiale c'est à dire les meilleurs cerveaux enfants plus généralement les parallèles entre l'informatique théorique et la biologie en fait sont très très très nombreux et d'ailleurs j'en avais parlé à tlx à clé et la vidéo 2 x anglais est sorti récemment moule concédé le voir c'est sur les algorithmes du vivant et notamment sa parle des grandes idées d'alan turing de john von neumann et de solomonoff qui ont des conséquences majeures sur la biologie enfin je vais finir avec un petit bémol par rapport à mes explications de dar ya un argument que j'ai sauté et qui est très important puisque je suis tombé en fait dans le sophisme cas swing et tearing qui le fait que juste parce que machine est compréhensible théoriquement ne veut pas dire qu'elle peut être compressible de façon pratique en particulièrement si on pense à intelligence artificielle alpha 0 alpha 06 une intelligence artificielle qui a battu toutes les autres intelligence artificielle aux échecs au jeu de go un autre jeux du même genre cette année ces évidences artificielle elle n'a pas utilisé de machine learning en français l'a pas utilisé des données qui venaient de l'extérieur est juste parti de quelques règles simplistes que ces différents jeux et elle a ensuite fait beaucoup de calculs donc c'est une dernière c'est une intelligence artificielle sans machine ne renie en tout cas sans apprentissage des données qui viendrait du monde extérieur et on a envie de dire que après tout ces calculs toutes ces simulations de plusieurs millions de parties de jeux de go et d'échecs et d'autre on a envie de dire que alpha 0 a gagné en complexité pour en fait elle n'a pas gagné en complexité de solomonoff puisque son état futur peut-être essentiellement prédit de son état initial donc activement la complexité sur le monop d'alpha 0 après toutes ces procédures de simulation et de parti est en fait pas plus grande que sa combi sida selon l'ofs initial ça techniquement c'est vrai sauf que pour passer de l'état initial à l'état où elle est aujourd'hui en fait le temps de calcul est absolument énorme et du coup en particulier en pratique si on veut une machine qui joue très bien audio aux échecs alors on pourra lui faire faire partir de la petite machine lui faire faire plein de calculs pour atteindre l'état de la grosse machine mais bien sûr ce processus serait très long et du coup jouer un coup des chèques pour cette machine serait très très très bref tout ça pour dire que le raisonnement sur la complicité de saumoneaux de la vidéo précédent il est parfaitement juste d'un point de vue théorique mais il a tendance à ignorer le fait que certains calculs sont nécessairement long et les intelligences artificielles que l'on veut en pratique sont des objets qui sont capables de prendre des décisions rapidement du coup si on prend en compte la théorie de la complexité algorithmique en temps on se rend compte que en fait il est quand même possible pour des intelligences artificielles de gagner en complexité de solomonoff effective une fois qu'on a pris en compte le temps de tout sans avoir de données extérieures ouais c'est un petit point un peu technique que je trouve très très fascinant et comme c'est un petit peu un enfant de ce que j'ai dit en vidéo précédente je pense je pense que ça vaut le coup que je le signale à lescar qui j'avais des jeux vidéo que vous allez réfléchir à ce problème du où les exclusifs jeu fait que ou exclusifs ne peut pas être appris par la qualification jr mais du coup comment est ce qu'on peut faire pour vraiment apprendre ou exclusif des pressions et bien la prochaine fois c'est à cette question qu'ont essayé de répondre si j'avais mais cette vidéo pensez à la ligue et à la commenter à la partager pensez à vous abonner pour l'éternité et futurs épisodes merci au tibet repos leurs dons et j'espère que vous serez là la prochaine fois mais il existe encore une autre possibilité pour expliquer la corrélation entre deux phénomènes a et b il se peut que n'y a ni paix ne soit la conséquence de l'autre mais bien qu'un troisième phénomène appelons le sobrement c'est soit la cause à la fois 2a et 2b de filosofia svr eastgate crown system ligne classe praia da maximiles distance reaching ces pratiques populaires et des réponses