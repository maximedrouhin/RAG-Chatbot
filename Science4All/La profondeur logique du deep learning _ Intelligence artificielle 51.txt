[Musique] oh [Musique] la dernière fois on a déjà parlé de la déraisonnable efficacité du dip learning et on avait relié ça à la notion de fractal en mathématiques qui est d'ailleurs parfaitement illustré par cette étonnante expérience de chimie sur laquelle on reviendra plus tard aujourd'hui on va aller plus loin dans notre raisonnement remplaçant les fractales par une notion qui me semble encore plus fondamentale à savoir la profondeur logique de bennett oui parce que s'il ya un truc spectaculaire dans notre univers un truc vraiment officines qui rend notre univers merveilleux un truc plus incroyable encore que la relativité générale que la mécanique quantique et que les vidéos de chats sur youtube à la fois pour l'informaticien que je suis en tout cas c'est bien l'incroyable profondeur logique de bennett de l'univers tel qu'il est aujourd'hui ainsi que son énorme sophistication solomonoff mais on va s'arrêter à sur la notion de profondeur logique pour aujourd'hui oui parce qu'en plus d'être une source d'émerveillement sans fin cette profondeur logique stupéfiante pourra bien être également la raison principale de la déraisonnable efficacité du dip leur nid en tout cas c'est ce que rachid diguer raul et moi avons suggéré dans cet article de recherche plein de conjectures qui me semble vrai quoi qu'elles soient très largement indémontrable bref aujourd'hui on parle de l'une des notions les plus fascinantes des algorithmique et pour commencer on va parler un peu de cosmologie en particulier de l'histoire et que futur de l'univers ce qui remarquable avec le début et la fin de notre univers c'est que ces deux états sont remarquablement pas remarquable ils sont même incroyablement ennuyeux dans les deux cas il s'agit d'un univers où rien d'intéressant de ce produit car la seule description macroscopique pertinence de ces univers aussi c'est à dire qu'il s'agit d'un mélange homogène de matière et d'énergie dans ces deux états il n'ya pas d'être vivant pas d'être vivant pas de cerveau pas de cerveau pas de découvertes mathématiques et pas de découvertes mathématiques pas de découvertes mathématiques et c'est quand même triste mais étrangement entre ces deux extrêmes ennuyeux à mourir l'univers est passé par une phase transitoire absolument fascinante avec des filaments cosmique des galaxies autour de trous noirs supermassifs des systèmes solaires quasi stable et même des planètes qui abrite des écosystèmes où interagissent constamment des êtres vivants si divers et variés et sur certaines planètes au moins une ya même des individus pensant à l'aide de cerveau d'une complexité formidable capable d'appliquer inconsciemment des approximations de la formule de pays et même de réfléchir consciemment aux conséquences de la forme et le buzz nous vivons dans une période fantastique de notre univers une période d'une complexité astronomique banques rendent est astronomique rare une période d'une sophistication spectaculaire fnr est passagère [Musique] ok je pense qu'un tweet ivement on est tous d'accord pour dire que notre univers actuel est particulièrement complexe mais est ce qu'on pourrait dire cela de façon rigoureuse il y aurait-il une quantification possible de la complexité intuitive de l'univers actuel quand il s'agit de mesurer l'ordre et le désordre de systèmes physiques les physiciens ont tendance à rapidement évoqué la notion d'entropie l'entropie d'un système dit on souvent mesures sont des ordres sauf qu'en fait ce n'est pas tout à fait le cas techniquement ce que l'entropie mesure vraiment c'est l'incertitude microscopiques qui persistent sachant une description macroscopique et d'un système physique et ça ne correspond pas vraiment à ce que longtemps usuellement par la notion de complexité d'un système physique en effet vous avez sans doute déjà entendu parler de la seconde loi de la thermodynamique qui dit que l'entropie de tout système fermé ne peut qu'augmenter et ça on peut l'appliquer à l'univers entier ce qui implique alors que l'univers a commencé à faible entropie et va atteindre un état de très grandes entre opi voilà qui ne colle pas du tout avec l'intuition selon laquelle la complexité de l'univers a émergé et va disparaître le physicien sean carole et l'informaticien scott aronson ont décidé de collaborer ensemble pour formaliser et révéler les mystères de la notion intuitive de complexité de l'univers actuel et dans un merveilleux article initialement et rené et corrigé par lauren ouellette qui a du coup rejoint la bande aronsson carole et donc où elle est on proposait 4 formalisation de la notion de complexité intuitive la complexité du cône de lumière la sophistication de kolmogorov la complexité apparente et donc la profondeur logique de bennett bon alors la complexité du cône de lumière est un peu intrigué physicien donc je vais laisser ça de côté pour aujourd'hui à la sophistication de kolmogorov c'est vraiment un truc génial mais dans mon livre je montre qu'il s'agit d'un cas particulier d'un truc qui me semble plus fondamental est que j'ai appelé la sophistication de solomonoff et malheureusement ou pas il me faudrait au moins un épisode dédié pour expliquer tout ça la complexité apparente elle c'est un peu une approximation facile à calculer de la sophistication de kolmogorov que aronson carole et ouellette ont utilisé pour leur simulation enfin et surtout la profondeur logique de bennett c'est la notion qui va nous intéresser aujourd'hui dernière chose avant de vous en parler les trois auteurs ont montré que les quatre notion est est intimement liées les unes aux autres et donc on peut imaginer qu'il suffit à peu près d'être complexe selon l'un des quatre critères pour être complexe selon tous les critères même si ce n'est pas techniquement toujours le cas alors cette profondeur logique de beineix c'est quoi et bien la profondeur logique de bennett c'est une sorte de mesure de la subtilité algorithmique d'un état physique détaillons pour commencer comme souvent en informatique ou commence par tout ramener à des données c'est-à-dire une suite de 0 et de 1 donc typiquement si on été exhaustif on pourrait décrire les positions et les vitesses de toutes les particules de l'univers à l'aide d'une suite de 0 et de 1 ou si on est un peu plus paresseux on peut prendre toutes sortes de photos de son et de vidéos de nous fait divers et les a enregistrées au format jpeg mp3 ou mp4 bref on obtient alors une longue suite de 0 et de 1 la profondeur logique de bennett sera alors une mesure de la subtilité algorithmique de cette suite de 0 et de 1 et pour mesurer cette subtilité algorithmique on va d'abord chercher des descriptions plus succincte de la suite de 0 et de 1 typiquement si on a une suite de deux millions 2 0 on pourra efficacement la résumer en disant qu'il s'agit d'une suite de deux millions 2 0 la description de la suite est ici beaucoup plus courte que la suite elle même ok juste pour vérifier que vous avez bien compris si j'ai maintenant un million 2 0 suivi de un million de 1 page envie de dire même combat que se passe-t-il maintenant si je prends une suite aléatoire 2 0 et 2 hors bien cette fois je vais être bien embêté en effet les suit et aléatoires ont justement la propriété de n'avoir aucune description succincte en fait la seule façon de décrire une suite aléatoire c'est de la décrire chiffres par chiffres on voit d'ailleurs avec cet exemple d'où vient la notion d'entropie l'entropie de la suite aléatoire et très grande car elle ne dispose pas de description macroscopique précise but de long une suite aléatoire de zéro et de rats ne peut pas en dire grand chose à part le fait qu'il s'agisse d'une suite 2 0 et de rats il subsistera alors une énorme incertitude microscopique sur les valeurs précises des éléments de la suite d'où la très grande entropie des suites aléatoire ok prenons un quatrième et dernier exemple qui va être les 2 millions premiers chiffres du nombre de g ce nombre gargantuesque dont on a parlé dans le premier épisode de la série sur l'infini cette fois la suite obtenu sera facile à décrire exactement ok on a donc vu que quatre exemples alors intuitivement selon vous lesquels de ces exemples sont complexes lesquels ont des subtilités algorithmique et bien l'idée de cars planète c'est de considérer que la subtilité algorithmique d'une suite de 0 et de 1 et le temps de calcul de l'algorithme qui génère cette suite de 0 et de 1 le temps de calcul de cet algorithme est ce qu'on appelle la profondeur logique de bennett ça correspond donc au nombre d'étapes de raisonnement logique nécessaire pour passer d'une description complète de la suite aux calculs complet des éléments de la suite dans le cas du premier exemple cette profondeur est très faible il suffit d'écrire deux millions 2 0 à la suite dans le second exemple elle est évidemment encore très faible de façon plus intrigante dans le troisième exemple la profondeur logique de bennett est encore très faible en effet puisque la description exacte d'une suite aléatoire et la suite aléatoire elle même il suffit de copier coller cette description de la suite pour déterminer les chiffres de la suite mais le quatrième exemple est plus étrange encore en effet bien que la description de la suite et très succincte le temps de calcul nécessaire pour calculer cette suite est gargantuesque il semble en effet qu'il faille faire une grosse partie du calcul du nombre de grammes pour arriver à calculer cette suite or ce calcul prendra un temps qui sera probablement de l'ordre de grandeur du nombre de grammes lui même et même si on arrivait à diviser ce temps de calcul par un googleplex vu qu'un googleplex et beaucoup beaucoup beaucoup beaucoup plus petit que le nombre de grammes c'est comme s'ils restaient encore en gros un nombre de grammes de calculs à effectuer ainsi notre quatrième exemple à une grande profondeur logique de bennett car il y a beaucoup de calculs algorithmique nécessaire pour passer de la meilleure description de la suite aux valeurs des éléments de la suite ok mais qu'est ce que tout ça a à voir avec notre univers est bien la thèse que défendent aronson carole et ouellette c'est que l'univers a commencé avec une très faible profondeur logique de bennett et qu'il finira avec une très faible profondeur logique de bennett mais entre les deux aujourd'hui il possède la propriété extrêmement rare et précieuse d'avoir une énorme profondeur le de bennett en effet tellement de phénomènes de notre côté on peut être compris à condition de requérir à des normes puissance de calcul en fait l'importance même des technologies de l'information dans nos sociétés modernes bien de cette importance du calcul qui semble venir elle même de l'énorme profondeur logique de bennett de notre univers actuel aronsson carole et ouellette vous même plus loin en suggérant qu'il s'agit là d'une propriété récurrente dans l'évolution de nombreux systèmes dont l'entropie et initialement faibles et pour s'en convaincre dans leur article il propose d'étudier le mélange de lait temps du café mais on peut revenir à l'expérience de chimie a réalisé par bruzy coeur à la geek touch il y a quelques mois pour profiter de ces belles images alors il s'agit d'une réaction chronomètre alliod et bien sûr le truc qui fascine c'est que la réaction n'a pas lieu instantanément même si elle a tout de même mieux rapidement bon je passe les détails chimiques sous silence mais vraiment à les suivre pour les écarts sur twitter si c'est vous fascine moi ce qui m'intéresse particulièrement dans cette vidéo c'est le moment là vous voyez alors qu'initialement le mélange son bep très simple à décrire un liquide homogène transparent et qu'après il est tout aussi simple à décrire liquide homogène bleu foncé il y à une phase intermédiaire plein de subtilités algorithmique avec notamment plaints de figure fractale et bien ce que les simulations d'harrison carole et ouellette suggère c'est que ces figure fractale intermédiaires dont une grande complexité apparente ce qui suggère qu'elles ont aussi une grande profondeur logique de bennett mollahs qui boucle la boucle et suggère que la particularité fondamentale de l'état physique de notre univers actuel le truc qui le rend absolument fantastique est plein de merveilles c'est cette fameuse profondeur logique de bennett et ça ça expliquerait la déraisonnable efficacité du diplôme ning oui parce que du coup ça suggère clé bonne description de notre environnement sont des descriptions plein de subtilités algorithmique qui requiert un grand nombre d'étapes de calcul pour être transformés en prédictions or seule la profondeur d'un réseau de neurones permet cette succession d'étapes de calcul que requiert tout bon modèle de notre univers notre univers semble donc nécessité du diplôme ning pour être compris et ça c'est une réflexion qui fait réfléchir tant les semble profonde la lymphe en avait déjà essayé d'expliquer la déraisonnable efficacité du diplo ning en parlant notamment des fractales et antoine ged demande si le cerveau humain est plutôt large ou profonds et bien il a l'air déjà d'être pas mal profond puisque c'est précisément la profondeur du réseau de nos humain qui a inspiré les chercheurs en intelligence artificielle et qui les a amenés à considérer des réseaux de neurones artificiels profond alors il ya même encore un truc un peu plus intéressant à noter qui est fait que la profondeur si on mesure ça en nombre d'étapes de calcul et vient potentiellement avec des réseaux récurrents il est infini puisque le calcul peut boucler à l'infini bon c'est de bouc à l'infini comme on a déjà parlé dans un épisode précédent ça pose beaucoup de problèmes au niveau de l'apprentissage mais il a forte apparaît que tout bon algorithme doivent avoir ce genre de boucles de rétroaction qui correspond si vous faites le choix du piton à une boucle wilde ou à une boucle fort ce genre de boucler vraiment en fait on y présente dans les logiciels que l'on utilise aujourd'hui et ce qui est intéressant c'est qu'il est possible que ce genre de boucles qui peut être également programmées par des algorithmes dit récursif ce genre de recursion en particulier sans quelque chose d'absolument fondamentales y compris pour le cerveau humain et c'est quelque chose dont stanislas dehaene parlement dans un de ses cours au collège de france sur le cerveau barisien on parle notamment de travaux de georges tenenbaum un des experts en neurosciences notamment tout ce qui nous rend science proche du machine learning est autrement des idées béziers notamment dans son article avec d'autres co auteur thénot mam suggère que apprendre à compter c'est apprendre un algorithme récursif préfet quand on a un compte à terme ans on compte sur les doigts ou rompre avec les doigts voire 1 2 3 4 comme ça en fait ce qu'on a appris à faire c'est un faire en sorte que chaque mouvement du doigt soit associée à nos tapis louisiens qui consiste à prononcer le prochain mot de la suite des nombres et à passer le doigt vers les choses suivantes que l'on compte et cet algorithme qu'on apprend est un algorithme récursif puisqu'il consiste à dire prononce le nom du numéro auquel tu es lorsque tu prends un objet est ensuite passez au suivant il répète l'opération et cet aspect est très récursif et le fait que le cerveau maîtrise joueurs de pression montre qui en fait il est capable d'algorithmes très très très long potentiellement très très long après le cerveau finit par fatiguer et ses boucles estiment qu'ils font que son outil gloubi il se force à oublier à certains moments raisonnement qu'il est en train de faire mais tout cela suggère que le cerveau et au moins capable de faire pas mal d'opérations de calcul à la suite mais vu notre faible attention à notre faible capacité de concentration notamment via forte appareil que ce nombre d'opérations successives dont le cerveau humain est capable est en fait assez limité en fait c'est très dur de suivre tout une preuve mathématique point par point de a à z de typiquement a tendance à s'intéresser un bout validé en bout puis regard d'un autre nouvelle terre notre bout ça ça vient sans doute du fait que le cerveau humain est limitée dans sa capacité à enchaîner les étapes de calcul bruno moreau lui fait la remarque que ces notions de frags cas d espèce de très grandes dimensions sont des choses qu'il a beaucoup de mal à visualiser bienvenue au club à ces objets très très très difficile à visualiser possible de visualiser intuitivement et du coup il faut se restreindre à des opérations mathématiques et à des descriptions mathématiques de ces différents objets fractals pétrole et que c'est que justement même ses descriptions mathématiques sont extrêmement compliqué et ça ça revient à dire qu'en fait la complexité de ce groupe dont on avait déjà parlé dans un épisode précédent notamment de ce qu'est un chat ou de ce qu'est différents objets et l'ensemble des chats typiquement ce sont des objets qui sont impossible à décrire en peu de mots et tout ça avec elle c'est très très difficile à visualiser tout ça mais juste pour expliquer un petit peu plus le rôle du diplôme ning et son apport avec les spectacles l'idée que j'ai essayé de défendre dans la vidéo précédente c'est que le sous-ensemble des chats vraiment un sous-ensemble des images qui est sans doute un nouveau un ensemble assez ouverte au sens où il va ti visant à contenir des petites boules vertes ce qu'ils voient ce moment à dire qu'en fait le concept de chats et robuste à des attaques par pixel ou par combinaison de de pixels du coup c'est vraiment un sous-ensemble a priori et avec une certaine frontières et l'idée que j'essaie de défendre dans la vidéo précédente c'est que cette frontière est sans doute une structure très fractales qui bouge beaucoup dans tous les sens et l'objectif du réseau de neurones c'est un peut projeter cette frontière pour la rendre aussi pour la des fractales ise on rôde d'avoir une fonction qui sera donc le réseau de neurones qui transforme cette frontière fractales bizarroïde en quelque chose il est allemand qui ressemblerait à une droite donc vraiment complètement là des fractales is et l'on vit d'ailleurs puisque la droite c'est vraiment le truc le moins fréquemment on envie de dire fait même mieux que ça plutôt qu'une droite pourrait que ce soit plus un espèce d'hyper plan dans un certain espace vectoriel de sémantique l'idée c'est que cette hyper plan en fonction de là on est du côté de cette hyper plan du côté chats ou pacha donc cette hyper plan par rapport au réseau de zones ça correspondrait au synap 130r est fonction de l'activation du dernier haut rhône qui est le neurone responsable de la détection de chat ou de pacha dans le réseau de neurones bref tout ça pour suggérer le fait que un réseau de neurones doit être profond pour réussir à sa manière des fractales is et les frontières fractales qui nous sont intéressants c'est une utilisée en pratique pour par exemple pas de parler des chats à supposer bien sûr que c'est ensemble des chats à une frontière fractale j'espère que vous avez aimé cet épisode qui clôt toute la partie sur les réseaux de neurones on en a bientôt fini avec cette série sur intelligence artificielle ne reste plus que trois épisodes et les trois derniers épisodes de parleront pas mal de l'article que j'ai écrit sur le problème du bayou loading en particulier la prochaine fois vous faire un titre très pratique puisque je vous présenterai ma solution contre le changement climatique si vous avez aimé cet épisode sans sel l'ikea le commentaire de partage et pensez à vous abonner pour une planque et futurs épisodes merci aux ti peu à peu leurs dons j'espère que vous serez là la prochaine fois peut-être que la notion complexité structurelle de complexité organisé ça serait lié à la quantité de calcul présente dans un objet plus un objet va être restructurée plus il va contenir une sorte de traces qu'il est le résultat d'un calcul charles bennett a proposé une définition qui semble toucher le but est donc ils provoquent la chance il entier pas du tout évidente c'est pas du tout évident c'est pour ça qu'on a mis tant de temps la définition était proposée qu'en 1977 donc qu'on appelle la profondeur logique de bennett et qui est une tentative pour exprimer cette notion de mesures de complexité organisée c'est le temps de calcul du programme minimum minimale à ses toiles du produit