en 2010 les historiens des sciences Naomi aurasquez et Eric konouey publie marchand de doutes un ouvrage qui expose l'étendue de la désinformation scientifique qui vise à nier l'ampleur du changement climatique et ses conséquences probables sur la sécurité des sociétés humaines ainsi certains instituts et expertes du domaine ont visiblement délibérément biaisé leur recherche pour nourrir la controverse et banaliser le statut quo dans leur livre très documenté aurait caisse et Conway traçant particulier des parallèles saisissants avec d'autres cases et verrues précédemment comme la désinformation scientifique sur le tabagisme les plus acides et le trou d'un couche d'ozone dans tous ces cas la communauté scientifique a été empoisonnée par des membres malveillants qui maintenaient le doute sur la dangerosité des produits industriels et normalisés l'inaction malgré cette dangerosité si une énorme partie des sciences est épargnée de tels trahison notamment les sciences fondamentales qui ont que peu d'applications la présence des normes enjeux industriels ou géopolitiques dans certains secteurs doit absolument appeler à la vigilance il nous faut ainsi peut pas perdre de vue que la science est menée par des humains qui peuvent être mis sous pression notamment sous pression de publication voire être corrompus ou plus simplement biaisé par l'influence parfois malsaine de certaines stars dans leur domaine or malheureusement dans mon domaine de recherche à savoir l'intelligence artificielle et en particulier la sécurité du machine learning les enjeux industriels et géopolitiques sont absolument énormes et le phénomène de scarification est très répandu après tout on parle là d'entreprise plus puissante encore que des industries pétrolières rien qu'en termes de chiffres d'affaires à cela s'ajoute la pression pour publier avec les salaires murobolants à la clé pour ceux qui parviennent voilà qui me semble avoir dangereusement poussé quelques-uns de mes paires notamment quelques-uns qui bossent avec ou dans des entreprises privées à produire diffuser et accepter des désinformations scientifiques extrêmement dangereuses des désinformations qui justifient notamment le déploiement précipité et massif d'algorithmes beaucoup trop peu sécurisé et qui sont d'ores et déjà hackés par des campagnes de désinformation depuis plus d'une décennie conduisant ainsi la montée de l'autoritarisme à travers tout le monde et même à des génocides au Myanmar en Éthiopie et en Ukraine de façon plus dérangeante encore l'écrasante majorité de la communauté scientifique me semble avoir fini par normaliser voir à encourager l'inattention à cette désinformation scientifique très préoccupante et à ses conséquences sociales désastreuses sur des millions de vie humaine aujourd'hui je vais vous parler de quatre articles scientifiques publiés dans des conférences prestigieuses avec comité de lecture qui me semble être très clairement de la désinformation scientifique dangereuse et pour être clair je ne prétends pas que le contenu de ces articles est faux mais plutôt que la présentation de ces articles en particulier les titres résumés et introductions tout ça est volontairement et dangereusement trompeur à l'instar d'une publication qui signalerait le refroidissement d'une région de la planète et dont le titre serait tout le globe ne se réchauffe pas étrangement tous les articles les plus problématiques que j'ai trouvé sont tous publiés avec au moins un employé de Google parmi les auteurs une entreprise qui s'est précédemment opposée à la publication d'articles scientifiques par ses chercheurs et qui a explicitement exigé deux qu'ils adoptent un ton positif dans leur publication scientifique mais surtout tous ces articles prétendent à tort résoudre les problèmes de sécurité du machine learning ou au moins il vise à nier ses problèmes à l'instar de lobby du pétrole qui cherche à nier la gravité du changement climatique ou affirmer que des solutions techniques finiront par être trouvées mais surtout on va voir que cette désinformation est jusque-là terriblement efficace en dévalorisant certaines lignes de recherche sur la sécurité ou en produisant des mes informations très répandues dans la communauté scientifique enfin je précise que bien entendu la désinformation dont on va parler et restreinte uniquement à ce que j'ai trouvé lu et pris le temps de déboguer avec mon temps en très limité et très occupé par beaucoup d'autres priorités dans cette vidéo je vous parlerai aussi de l'énorme travail de débunking scientifique que mes fantastiques collaborateurs et moi avons effectués en publiant des travaux qui contredisent cette désinformation dont les revues scientifiques les plus prestigieuses en machine learning notamment ne rips et acml même si je préciserai aussi que nos articles reçoivent beaucoup moins d'attention et de citations de la part des autres académiques que la désinformation produite par Google et célébrée à tort par cette communauté scientifique si vous êtes chercheur en informatique je vous invite à exiger beaucoup plus de rigueur de vous-même et de vos collègues surtout vis-à-vis des affirmations selon lesquelles les problèmes de sécurité seraient résolus ou peu crédible la publication scientifique de ces affirmations aura des conséquences majeures sur les algorithmes déployés à très grande échelle par Google et compagnie et risque d’ailleurs d'amplifier des tensions géopoétiques déjà très préoccupantes je vous invite à être particulièrement rigoureux au moment de faire la review des articles de vos pères de sélectionner les conférenciers à inviter et de choisir qui recruter ou promouvoir en plus de potentiellement gober et propagé de la désinformation scientifique un manque de vigilance peut aussi et surtout conduire à davantage de pro d'algorithmes opaque mal sécurisé et potentiellement très dangereux sous prétexte que leur principe ont été scientifiquement validés si vous n'êtes pas chercheur en informatique vous pouvez toujours aider à alerter la communauté scientifique sur sa vulnérabilité à la désinformation scientifique de Google en partageant massivement cette vidéo notamment avec des chercheurs en informatique mais aussi en interpellant des journalistes et des politiciens ou encore en participant à des projets de recherche participative et éthiques en machine learning comme le projet tournesol je reviendrai plus longuement sur les actions possibles en fin de vidéo alors la vidéo est un peu longue donc juste pour vous y retrouver on va d'abord parler de désinformation autour de la protection des informations sensibles par des algorithmes d'apprentissage avant de parler de la sécurité de ses algorithmes notamment vis-à-vis de campagne de désinformation et puis enfin il y aura une conclusion chaque section est ensuite soudée en sous section dont vous trouverez les time code en description allez on est parti les désinformations scientifiques de Google la plus simple à comprendre et celle de la protection des informations sensibles jusque-là des régulations comme rgpd le règlement général pour la protection des données personnelles ce sont concentrés sur la protection des données des utilisateurs avec l'hypothèse sous-jacente selon laquelle cela suffirait à protéger nos sociétés contre la révélation publique d'informations sensibles cependant le lien entre protection des données personnelles par exemple les données sur votre téléphone ou les données enregistrées dans votre compte Google le lien entre ces données et la protection des informations sensibles indirectement extraite de ces données personnelles est en fait plus complexe que ce qu'on pourrait croire surtout dès lors que les algorithmes utilisent les données personnelles pour répondre à des requêtes d'autres utilisateurs du web et notez que ce problème peut survenir même avec des algorithmes ultra simples lorsque WhatsApp vous dit que votre interlocuteur a bien reçu votre message il révèle une information que cette interlocuteur aurait potentiellement préféré ne pas révéler l'information sensible c'est à dire l'information qu'une personne ne souhaiterait pas avoir diffusé est alors enregistré dans les données personnelles d'une autre personne ceci dit le problème de l'exploitation algorithmique des données personnelles et bien entendu beaucoup plus important dans le cas d'une machine learning après tout ces algorithmes d'apprentissage sont littéralement conçus pour apprendre des données si ces données contiennent des informations sensibles alors l'algorithme aura littéralement appris ces informations sensibles hors de façon très problématique et dangereuse l'algorithme qui contient potentiellement les informations sensibles mais généralement pas considéré être une donnée personnelle et rgpd est alors très flou vis-à-vis des droits et des devoirs de cet algorithme pourtant si l'algorithme a appris des informations sensibles les actions ensuite entreprises par l'algorithme peuvent largement trahir ces informations sensibles par exemple il suffit de regarder les recommandations faites par l'algorithme de YouTube en utilisateur pour connaître les habitudes de consommation de l'utilisateur bref tout objet obtenu à partir d'une exploitation des données personnelles y compris un objet aussi simple qu'une recommandation est un vecteur de fuite essentiel d'informations sensibles pire encore les algorithmes de langage sont souvent conçus pour littéralement recraché ce qu'ils ont appris avec presque toujours très peu sinon aucune distinction entre les informations publiques et les informations sensibles en demandant à ces algorithmes de remplir des formulaires semi-remplies des chercheurs ont ainsi montré qu'il était parfois possible de récupérer des informations personnelles de vrais individus humains tandis que d'autres utilisateurs d'autres algorithmes de langage informatique notamment ont montré que l'algorithme exposait des clés secrètes privées de façon plus préoccupante encore au fur et à mesure que les algorithmes de langage se développent il faut s’attendre à ce que nos téléphones disposent petit à petit d'algorithmes de langage très sophistiqué pour faire l'auto-complétion de nos messages sur nos claviers intelligents dès lors il pourra un jour être possible de taper monsieur fille a dit hier soir à sa femme que et de tout simplement lire l'autocomplétion par Google jboard pour ensuite extraire des informations potentiellement très sensibles le déploiement massif de telles algorithmes sera alors très clairement dangereux devant ses risques évidents Google a bien dû s'attaquer à la sécurisation de ces algorithmes et donc comment s'y prennent-ils l'ont-ils fait avec sérieux rigueur et transparence ont-ils été à la hauteur des énormes enjeux sociaux en jeu en 2015 Google a proposé un algorithme d'apprentissage distribué qu'ils ont appelé l'apprentissage fédéré ou fédérité de learning en anglais ce système est capable d'apprendre des données des utilisateurs sans jamais que ces données ne soient transférées au serveur de Google en gros au lieu d'envoyer les données chaque téléphone des utilisateurs va utiliser des algorithmes liés au fameux réseau de neurones pour calculer ce qu'on appelle un modèle ou un gradient et c'est cet objet qui est ensuite transféré au serveur de Google les chercheurs de Google ont ensuite été déployé leur système d'apprentissage fédérés sur les téléphones de millions d'utilisateurs pour apprendre ce que ces utilisateurs tapent sur leur clavier intelligent duboard or ces données contiennent des informations potentiellement incroyablesment sensibles réfléchissez-y il n'y a peut-être rien de plus intime que ceux que vous tapez sur le clavier de votre téléphone surtout sur des messager privés comme signal ou WhatsApp qui plus est l'information dans ces données peut être aussi sensible pour votre interlocuteur que pour vous elle peut même être sensible pour un tiers si vous parlez de lui et bien sûr tout cela est fait sans le consentement éclairé des utilisateurs en tout cas les articles ne parlent pas du tout de consentement je cite pour cette étude des entrées ont été collectées de la population anglophone d'utilisateurs de j-bord aux États-Unis en fait si vous utilisez un clavier jboard par défaut tous ceux que vous tapez est très certainement appris par un algorithme de Google pensez-y notamment si vous utilisez des messageries sécurisés comme signal avant que vos messages ne soient chiffrés vous les tapez en clair sur un clavier qui apprend très probablement de ce que vous tapez vu le manque d’attention à la sécurité par jboard une gaffe de cette IA pourrait un jour faire fuer vos communications bref je vous invite à envisager l'utilisation d'alternative open source comme openboard ou unisoft keyboard tout en restant vigilant rien ne garantit que le code exécuté sur votre téléphone soit celui publié sur GitHub en fait pour les trucs ultra sensibles vous pouvez carrément désactiver le côté intelligent du clavier et surtout utilisez une machine moins vulnérable que le téléphone comme un ordinateur mais ce n'est pas là le plus grave ce qui est extrêmement dangereux c'est que Google est parvenu à propager la désinformation scientifique selon laquelle l'apprentissage fédéré protège les données des utilisateurs je cite un article de blog de Google l'apprentissage fédéré permet des modèles plus intelligents avec moins de latence et moins de consommation énergétique tout en garantissant la priveci tout en garantissant la privacy quel désinformation cette désinformation s'est retrouvée publiée dans la littérature scientifique dans cet article scientifique accepté à publication à CCS 2017 au titre incroyablement trompeur agrégation sécurisée pratique pour du machine learning qui préserve la privacy le résumé de cet article propage à son tour des informations trompeuses comme je cite nous prouvons la sécurité de notre protocole dans des contextes d'utilisation honnêtes mais curieux et activement adversario alors je n'ai pas traduit le mot privacy parce qu'il y a des faciliter légale dans le choix de traduction mais pour le chercheur machine learning qui connaît mal le sujet ou pour le législateur qu'il y en diagonale cette phrase donne l'impression que l'apprentissage fédéré garantit la protection des informations sensibles il s'agit là de l'une des plus claires et des plus graves des informations scientifiques d'aujourd'hui et pourtant cette désinformation est triviale à des bonnes quais en effet l'apprentissage fédéré est tout simplement une procédure plus sophistiquée pour conduire au même résultat que l'apprentissage centralisé classique dès lors un algorithme fédéré aura appris la même chose que ce Cora a pris un algorithme centralisé y compris si des mesures de chiffrement à base de cryptographie au moment ou de secrets partagés sont utilisés au milieu comme c'est le cas dans l'article de Google or depuis environ 5 ans de nombreuses recherches théoriques et empiriques n'ont cessé de montrer que les algorithmes d'apprentissage moderne obtiennent leur meilleur performances lorsqu'il mémorise les données d’apprentissage un phénomène parfois appelé double disent pour ceux qui dès lors l'apprentissage fédéré à son meilleur et qui aujourd'hui en machine learning ne cherche pas à optimiser son apprentissage cet apprentissage fédéré produira infini un algorithme qui aura mémorisé toutes les données des utilisateurs Taylor il pourrait être possible pour n'importe qui de décortiquer cet algorithme pour récupérer des informations sensibles pire encore s'il s'agit d'un algorithme de traitement du langage il pourrait suffire de poser des questions à l'algorithme pour que celui-ci réponde en révélant les informations sensibles et ça les auteurs de l'article de désinformation sont sont bien rendu compte mais il ne le signalent que dans le coeur de l'article où ils écrivent je cite quand le protocole est exécuté avec un seuil t la vue conjointe du serveur et de n'importe quel sous-ensembles de télé utilisateurs honnête ne fait fuiter aucune information à propos des utilisateurs à part ce qui peut être inféré du résultat du calcul cet aveu de faiblesse dissimulé dans une précision après une virgule au milieu de l'article est en fait énorme particulier le dispositif dans son entièreté n'offre finalement aucune garantie de protection des informations sensibles mais ça les académiques les journalistes et les législateurs paresseux qui se contentent du titre du résumé et de l'introduction comme c'est aussi mon cas trop souvent on ne se rendra pas compte or les conséquences d'une propagation massive d'information sensible par les algothmes d'apprentissage pourrait être catastrophique imaginez le bouleversement social que cela représenterait si tout le monde pouvait tout à coup espionner n'importe qui d'autre la révélation soudaine de nombreuses trahisons pourraient exacerber des tensions et sérieusement augmenter les risques de violence armées par ailleurs les secrets d'affaires et d'état pourraient être exploités par des entités malveillantes pire encore des mots de passe de système critiques notamment pour prendre le contrôle de données médicales de centrales nucléaires ou de systèmes bancaire ou encore des informations sensibles à propos des humains en charge de la sécurité de ces systèmes d'information tout ça pourrait être exploité par des acteurs malveillants on parle là d'un enjeu de sécurité nationale voire de paix dans le monde bref l'apprentissage fédéré même si les combinés avaient des affaires de chiffrement au mot mort ou de secrets partagés n'offre aucune garantie de protection des informations sensibles au mieux il faut y voir un hack technique pour faire du machine learning en son conformant à une version faible et très maladroite du rgpd le règlement général sur la protection des données personnelles au pire il faut aussi voir une désinformation que Google a voulu faire avaler à la communauté scientifique avec malheureusement beaucoup de succès pour légitimer le déploiement massif de technologie de surveillance qui permettront une publicité toujours mieux ciblée et pourtant de façon très décevante ça a été désinformation selon laquelle l'apprentissage fédéré protège les informations sensibles a été reprise amplifiée et normalisée par beaucoup trop de publications scientifiques l'article scientifique très dangereux de Google de 2017 a déjà été cité près de 1600 fois par les publications d'autres chercheurs académiques d'après Google Scholar 1600 citations en 5 ans dans le monde de la recherche c'est vraiment énorme pire encore en recherchant learning is a privacy on se rend compte que la désinformation de Google est reprise telle qu'elle est sans justification dans un nombre ahurissant d'articles scientifiques publiés dans des revues scientifiques prestigieuses comme Highway security and prevessy Highway John all of aded incommunication computing nature communication Edge 6 et SPML entre autres vraiment le pire review en machine learning n'est pas une garantie de qualité et de façon terrifiante cette scientifiques a été utilisée pour justifier toutes sortes de systèmes d'apprentissage fédérés de la prédiction des cas cliniques de covid 19 à la détection de problèmes de santé mentale le tout en s'appuyant sur des informations hautement sensibles qui si elles étaient connues des proches ou des employeurs pourraient bien bouleverser la vie des personnes souffrant de ces maladies et j'insiste sur le fait que les cas que je cite ici sont publiés dans des revues scientifiques à comité de relecture autrement dit les auteurs et les revues scientifiques sont fiers d'avoir reproduit les systèmes d'information proposés par Google et de les avoir appliqués au traitement de données personnelles y compris des données personnelles hautement sensibles pire encore en célébrant ses travaux ce qui conduira des promotions pour les auteurs au détriment d'autres en célébrant ces travaux la culture et la mésinformation au sein de la communauté scientifique valide encourage et normalisent le déploiement précipité et maljustifié de technologie dangereuses voire le mépris pour la recherche rigoureuse sur la sécurité mais tout pendant ce temps Google peut tranquillement déployer des algorithmes de langage beaucoup plus sophistiqués encore avec beaucoup plus de mémorisation en s'appuyant sur les données des utilisateurs et le jour où on les poursuivra pour mise en danger des utilisateurs ils pourront répondre à juste titre que les garanties de protection des informations sensibles de leurs algorithmes ont été scientifiquesment prouvés et validés par la communauté scientifique qui n'a cessé d'affirmer que l'apprentissage fédéré garantit la protection des informations sensibles tout ça c'est extrêmement dangereux et malheureusement je peux témoigner à titre personnel de l'ampleur de la tâche qui consiste à des bunker cette désinformation au sein du milieu scientifique j'ai eu l'occasion d'être reviewer dernièrement à acml et norrips deux des trois conférences les plus prestigieuses en machine learning et de façon très troublante un article sur trois en apprentissage fédéré dont on m'a demandé de faire la revue propager à son tour cette désinformation j'ai dû ainsi prendre le temps de faire le devunking et de cette désinformation moi-même et c'est quand même très troublant que de devoir devunker une chose aussi triviale à des experts du domaine mais surtout avec six collègues notamment elle m'a dit lui-même chercheur chez Google on a écrit un article qui synthétise les théorèmes mathématiques d'impossibilité sur la sécurité et la privacy des algorithmes très sophistiqués de machines learning en gros on en vient à la conclusion que combiner sécurité et performance est tout simplement impossible et donc si on valorise vraiment la sécurité des milliards d'humains sur la planète il faut urgemment sacrifier la performance malheureusement l’accueil de notre article de recherche par Google d'un côté et par la communauté scientifique de l'autre a été compliqué pour commencer Google dont il nous fallait l'approbation pour ajouter l'affiliation Google de Mehdi a mis le papier en suspens pendant plusieurs mois avant de s'opposer à sa soumission une affaire a priori inédite un an après le licenciement d'intimité d'ailleurs on avait prévu le coup et on a finalement soumis le papier avec l'autre affiliation de Mehdi qui se trouve être professeur à l’école plus technique en France mais de l'autre côté il y a eu la revue par les Pères plus décevante encore le principal critique qui s'opposait fortement à la publication à justifier le rejet de notre article en s'appuyant justement sur la désinformation selon laquelle l'absence d'enregistrement des données personnelles suffisait à garantir la protection des données personnelles et le tout dans un message horriblement mal écrit et difficilement compréhensible une revue absolument scandaleuse à mon goût la revue site aussi l'utilisation majoritaire de Wikipédia ce qui est une autre en tout cas si on considère les algorithmes les plus spectaculaires comme GPT 3 ou panne qui s'appuie beaucoup plus sur les conversations sur les réseaux sociaux vachement rassurant ou si on considère les algorithmes les plus influents du monde moderne comme les algorithmes de recommandation Siri ou encore les claviers intelligents dont l'opacité permet même aux entreprises d'empiéter encore plus sur les données personnelles si la version alors soumise de notre article était par ailleurs critiquéable et critiqué par les autres revueurs la décision finale de rejet de notre article aura clairement été influencé par le principal critique qui au mieux propager la désinformation au pire vous voulez empêcher la publication de critiques scientifiques de la sécurité du Deep learning d'expérience certains chercheurs en machine learning comme finalement beaucoup de monde détestent qu’on exige deux qui mesure les véritables conséquences sociales de leurs travaux et de ceux de leurs collègues et peuvent abuser de raisonnement motivés pour justifier l'éthique de leurs travaux payés par le contribuable ou par des entreprises privées en affirmant typiquement qui ne font que des travaux théoriques directe ou qu'il existe aussi des applications positives de leurs recherches ou encore qui ne sont pas responsables des mauvaises utilisations de leur publications ainsi pour défendre l'éthique de leurs travaux et pour ne pas avoir à questionner leur carrière certains académies préfèrent fermer l'oeil sur les millions de vies qu'il contribue à mettre en danger depuis notre article a été réécrit et ressouni en mode système autorisation of noge ça veut dire en gros mettez analyse avec beaucoup d'ajouts de références et beaucoup plus de pédagogie pour comprendre la portée des travaux en sécurité du machine learning aujourd'hui encore beaucoup trop difficile à publier dans les conférences prestigieuses et beaucoup trop souvent ignoré par les chercheurs en machine learning et bon on verra avec curiosité ce que les reviewer diront de cette nouvelle version alors s'il y a des experts en privacy parmi vous vous écrivez peut-être que je n'ai toujours pas mentionné le concept dominant de la recherche académique en protection des informations sensibles à savoir la notion de confidentialité différentielle bon la vidéo est déjà très longue donc je ne vais pas pouvoir prendre le temps de bien expliquer de quoi il s'agit mais en gros l'idée de la confidentialité différentielle c'est d'envoyer des versions brutées des objets calculés à partir des données personnelles au serveur de Google de sorte à réduire la capacité de Google à connaître ces données personnelles et en effet on a là une solution dont si on fait les choses bien on peut prouver qu'elle protège en partie les données personnelles j'insiste sur le fait bien trop méconnu des chercheurs du domaine que la confidentialité différentielle protège les données personnelles c'est-à-dire les données explicitement associées à une personne comme les données sur le téléphone de celle-ci ou celle enregistrée dans son compte Google or il y a une distinction cruciale à faire entre ces données personnelles et des informations sensibles c'est à dire des informations que quelqu'un ne veut pas que les algorithmes diffusent surtout dans le cas du langage des activités sur le web ou de la santé où les données d'une personne contiennent des informations sensibles à propos d'autres personnes la différence entre données personnelles et informations sensibles et majeures par exemple ma mère peut très bien envoyer mon adresse personnelle à mon père l'information sensible qui est mon adresse serait alors dans les données personnelles de mes parents et ne seraient pas protégés en tout cas pas protégé comme la confidentialité différentielle prévoit de la protéger de la même manière les données personnelles génomiques de mes parents trahissent inéluctablement des informations sensibles à mon sujet notamment à propos des maladies que je risque d'avoir puisque mon ADN est une combinaison des ADN de mes parents dès lors surtout si beaucoup de gens parlent d'informations sensibles à mon sujet mes informations sensibles risquent d'être fuité même si mes données personnelles sont parfaitement protégées pour bien se rendre compte de l'ampleur de la fuite et d'informations via ces canaux indirects je vous invite à voir cette vidéo ou à lire l'histoire du Golden State killer qui a été trahi par les données personnelles de cousins éloignés bref la notion de confidentialité différentielle est en fait trompeuse pour la plupart des cas d'application à grand enjeu ceci étant dit même à supposer que la confidentialité différentielle est un bon concept sa mise en pratique est absolument catastrophique comme en parle cette excellent article je ne rentre pas dans les détails c'est encore une fois très long à expliquer mais en gros les algorithmes des géants de latex n'ont pas du tout les garanties suffisantes même lorsque ces derniers prétendent garantir la confidentialité différentielle et je vais ne pas rentrer non plus dans les détails des réutilisations malsaines et dangereuses de la notion de confidentialité différentielle par les gens de la tech notamment en termes d'asxwashing début de pouvoir dominante et de silenciation d'autres problèmes plus graves malgré toutes ces critiques aujourd'hui la confidentialité différentielle est vue comme le Saint Graal de la protection des informations sensibles par une écrasante majorité de la communauté scientifique si bien qu'un algorithme qui garantit cette propriété est souvent qualifiée de privacy plus élevé en 2021 un article accepté à iclr la troisième des plus prestigieuses conférences en machine learning à côté de la CML et norrips avec un titre qui affirme les grands modèles de langage peuvent être des systèmes d'apprentissage fortement différenciés et encore une fois l'un des auteurs de cette désinformation scientifique boss chez Google alors si on cherche l'interprétation charitable de ce titre en un sens ok oui ces algorithmes de langage peuvent être des apprentis fortement différencié dans le sens où après avoir été conçu et entraîné dans un premier temps ils peuvent dans un second temps continuer à apprendre en respectant la privacy des données exploitées pendant ce second temps et c'est justement ce que fait le papier sauf que sauf que la performance dans ce second temps est en gros trivial par rapport à ceux qui étaient initialement appris de plus le pas à pied ne parle absolument pas de protection d'information sensible dans la première phase d'apprentissage qui est bien la phase critique car nécessite beaucoup plus de données et donc contient potentiellement beaucoup plus d'informations sensibles mais surtout le titre plus difficilement des bonnes câbles de désinformation me semble d'ailleurs beaucoup plus dangereuse mais soyons clairs non les très gros modèle de machine learning ne peuvent pas atteindre leurs performances tout en protégeant les informations sensibles en tout cas avec les algorithmes d'aujourd'hui pire de nombreux travaux théoriques suggèrent au contraire l'impossibilité d'une telle protection des informations sensibles sans dégradation majeur des performances comme on en parle dans notre article la course à la performance dans laquelle nous sommes complètement engagés à coups de communiquer de presse sur Dali et lambda et qui est massivement relayée par les médias et les utilisateurs des réseaux sociaux nuira inéluctablement grandement à la privacy [Musique] les algorithmes de machine learning sont donc une grave menace pour la protection des informations sensibles et pourtant cette menace me semble presque dérisoire par rapport à ceux qui me préoccupe vraiment ce qui me terrifie c'est surtout que ces algorithmes amplifient encourage et normalisent la désinformation le cyber-harcèlement et les appels à la haine comme on l'a vu en Chine en Inde en Éthiopie au Royaume-Uni au Brésil ou encore aux États-Unis cette désinformation et cette haine peuvent conduire à des politiques dangereuses à des émeutes meurtrières voire à des génocides de population entière après l'ONU anesti internationale a reconnu le rôle criminel de Facebook dont l'amplification de la haine au Myanmar contre la communauté des Rohingyas exige réparation les Rohingya ont d'ailleurs entamé des procédures de justice qui exigent 150 milliards de dollars de réparation ce qui avec la chute du cours de facebook représente près de la moitié de la valeur de l'entreprise mais pourquoi les algorithmes d'apprentissage favorisent-ils la désinformation et la haine sont-ils part nature malveillant leur concepteur cherche-t-il à propager la haine et la désinformation non sans doute pas en tout cas pas intentionnellement quoique il faudrait très bien que ça aussi augmente le revenu et leur bonus le problème c'est surtout que les algorithmes d'apprentissage moderne sont généralement conçues pour apprendre et généraliser toutes les données dont ils apprennent si leur donner contiennent beaucoup de désinformations de harcèlement et de haine alors ces algorithmes vont répéter généraliser et amplifier la désinformation le harcèlement et la haine comme on en a déjà parlé les données manipules les algorithmes d'apprentissage ok mais ne serait-il pas possible de se protéger contre les contes malveillants qui produisent ou amplifient la désinformation et bien en 2021 une réponse positive a été fournie par quatre chercheurs dont l'un travaille à Google oui vous l'aurez compris cet article accepté par les Pères académiques dans la très prestigieuse conférence économics Computing et encore une fois je pense une énorme désinformation dans la dernière phase de son résumé l'article affirme nos résultats indiquent que les plateformes en ligne peuvent efficacement combattre les utilisateurs frauduleux à faible coût en consommant de nouveaux algorithmes d'apprentissage qui garantissent la convergence efficace même lorsque la plateforme est complètement ignorante du nombre et de l'identité des faux utilisateurs l'analogie avec la désinformation des entreprises pétrolières qui prétendent que les problèmes climatiques peuvent être efficacement résolus par de nouvelles technologies est tellement frappante que j'ai vraiment du mal à ne pas y voir de malveillance encore une fois il suffit de se plonger dans l'article pour se rendre compte cette phrase n’est en fait valide que sous des hypothèses complètement irréalistes mais pour s'en rendre compte vu comment le titre et le résumé ont été rédigés les malheureusement nécessaires de se plonger dans l'article et de prendre le temps d'analyser les théorèmes ce qui clairement demande une certaine aisance mathématique mais une fois ce travail effectué on se rend compte l'affirmation n'est valide que si le nombre de fauconstes est exponentiellement plus faible que le nombre de comptes authentiques exponentiellement plus faible autrement dit il y a 30 faux comptes alors vous pourrez combattre ces faux comptes à faible coût avec de nouveaux algorithmes uniquement si votre plateforme possède disons 2 puissances 30 comptes authentiques soit 1 milliard d'utilisateurs humains et pour peu que les attaquants créent trois faux comptes de plus il vous faudra plus de vrais comptes qu'il n'y a d'humains sur terre mais bien sûr l'hypothèse selon laquelle Google ne disposerait que de 33 faux comptes et elle-même complètement irréaliste souvenez-vous de cette statistique chaque année Facebook retire autour de 7 milliards de faux comptes dans ce contexte non seulement l'affirmation principale de l'article est complètement trompeuse mais l'article dans sa globalité est lui-même complètement inutile et pourtant ce papier a été accepté à la conférence économique cette computing cette même conférence qui a rejeté un de nos papiers en 2022 malgré un review or enthousiaste tout ça parce que l'autre revueur juge que le sujet qu'on aborde dans cet article je cite malgré les affirmations de liens avec le machine learning ne semble pas important qui me permet de souligner le fait que l'acceptation à publication scientifique est beaucoup plus liée à l'attractivité voire aux sensationsnalisme que ce qu'on pourrait croire en tout cas ça n'a absolument rien de complètement objectif neutre ou un partiel et ça ça rend d'ailleurs le processus scientifique de revue par les paires très vulnérables des informations qui est une excuse toute faite pour rejeter des articles critiques en disant bah c'est pas important d'autant que en machine learning beaucoup d'herviewers sont eux-mêmes des employés de Google ou des chercheurs indirectement financés par Google la même année avec des collègues notamment l'excellence Sadek farad Kani on a publié à neurips l'une des tops conférences en machine learning un papier qui au contraire démontre l'impossibilité mathématique de se défendre efficacement face à des faux comptes indiscernables des vrais comptes surtout lorsqu'il y a beaucoup d'hétérogénéité parmi les comptes authentiques même s'il n'a pas du tout été écrit avec ce but je pense qu'on ignorait l'existence même de l'autre papier à ce moment-là notre article est un des bonnes Kings de la désinformation scientifique citée plus haut mais malheureusement jusque là le devunking fait moins de citations scientifiques c'est à dire que la désinformation qui en a fait 14 la communauté scientifique propage beaucoup plus l'affirmation complètement trompeuse selon laquelle la résilience au faux comptes peut se régler facilement à coup de nouveaux algorithmes que les arguments beaucoup plus solides sur la vulnérabilité inéluctable des algorithmes à grande échelle et encore une fois l'enjeu est majeur puisqu'il permet à Google de prétendre qu'il maîtrise parfaitement leur technologie à l'instar de l'entreprise privée du film dans le CUP qui prétend pouvoir miner et détourner seul un astéroïde de plus fort de cet article accepté et célébré par la communauté scientifique Google peut alors affirmer au législateur que des audits ou des régulations en cybersécurité sont superflues voire contre-productif pour la société civile puisqu'il est scientifiquement établi qui leur suffit de développer de nouveaux algorithmes comme eux seuls savent le faire et pourtant pendant ce temps les algorithmes de recommandation sont hackés en permanence car le besoin et la capacité à y arriver est une évidence parmi beaucoup d'entre vous notamment ceux qui commentent mes vidéos mais du coup si c'est évident pour vous il faut s'attendre à ce que ça soit une évidence encore plus pour les entités puissantes et malveillantes comme des gouvernements autoritaires ou des multinationales qui ont des profits à protéger il faut s'attendre à ce que ces entités exploitent elles aussi la vulnérabilité du machine learning pour biaiser les algorithmes de recommandation les plus influents du web mais comme ils ont beaucoup plus de moyens pour y parvenir que ceux qui militent pour une information de qualité il faut s'attendre à ce que les algorithmes de machine learning soient beaucoup plus influencés par cette désinformation que par l'information de qualité c'est en tout cas ce que ne cesse de montrer la recherche mais aussi le journalisme et les licks des géants de la tech malheureusement les articles qui envisagent la possibilité de manipulation des algorithmes sont encore incroyablement rares dans les conférences en machine learning de manière absolument terrifiante la quasi-totalité de la recherche en machine learning fait l’hypothèse dangereusement irréaliste que les algorithmes d'apprentissage devraient avoir pour but d'apprendre et de généraliser toutes les données auxquelles ils ont accès avec second derrière la fameuse hypothèse iid sur laquelle la quasi entièreté de la théorie du machine learning s'appuie et qui suppose que toutes les données sont tirées indépendamment d'une mystérieuse vrai loi de probabilité qu'il faut absolument apprendre une sorte de main invisible de l'intelligence artificielle qu'on m'a déjà dit qu'il faut que j'invoque dans mes articles si je veux augmenter la chance qu'il soit acceptées par les reviewer et oui j'ai consigné des articles qui contiennent cette hypothèse alors que vraiment cette hypothèse ok ça permet de faire des super jolies mathématiques ça permet aux chercheurs de briller par leur ingéniosité technique mais au final c'est un peu comme faire l'hypothèse des marchés efficients et de l'appliquer à des marchés comme celui de l'information de l'électricité ou des ressources naturelles ok le chercheur peut utiliser cet article pour se démarquer par sa virtuose technique et ça va l'aider à gagner des prix à avoir un job de rêve et parfois même à gagner des millions de dollars siffleur avec Google notamment mais ce faisant le chercheur en dehors et propage aussi des désinformations et des habitudes extrêmement dangereuses pour les sociétés humaines ce problème est d'ailleurs pire encore dans le cadre de l'apprentissage fédéré puisque les participants au système peuvent plus facilement envoyer n'importe quoi au serveur de Google et ces serveurs pourront plus difficilement déterminer si c'est gradions correspondent à des données plausibles de plus ces participants pourront plus facilement optimiser leurs attaques pour encore mieux biaiser les algorithmes que tout le monde utilisera dans les directions que eux souhaitent l'écrasante majorité des algorithmes d'apprentissage fédérés publiés dans la littérature scientifique est ainsi en fait extrêmement vulnérable à la première attaque dite byzantine depuis 2017 pour enfin considérer la sécurité du machine learning gravement a prioriser les problèmes de sécurité du traitement et de la communication de l'information et Google n'y est pas pour rien dans cette des priorisation clairement la recherche qui révèle les vulnérabilités de sécurité de l'apprentissage fédéré est très gênante pour des entreprises comme Google qui voyaient l'avenir de leur business de publicité ciblée et de recommandations de contenu et c'est sans doute ça qui les a motivés à encourager et financer une recherche qui vise clairement à dénigrer la recherche sur la sécurité et à ma grande déception l'article de désinformation ainsi produit dont l'un des auteurs est un chercheur de Google a été accepté par les communautés scientifiques dans la revue ice Tripoli security and prevessy je cite le résumé de l'article nous présentons une analyse critique des attaques dans des environnements d'apprentissage fédérés pratique et de production nos résultats sont plutôt surprenants contrairement à la croyance établie nous montrons que l'apprentissage fédéré est très robuste en pratique même en utilisant des défenses simples et peu coûteuses vu les nombreuses formulations trompeuses de ces phrases ne fait essentiellement aucun doute à mes yeux qu'il s'agit là de désinformation intentionnelle en bref le point le plus important à retenir pour des bonnes CACES des informations c'est que l'environnement pratique et de production considérée par cet article n'est absolument pas l'environnement pratique et de production des algorithmes déployés à très grande échelle et avec des enjeux géopolitiques majeurs en effet l’article étudie exclusivement la classification d'image pour deux jeux de données standard à savoir liste et si Fardi typiquement reconnaître qu'il y a un 3 dans cette image et que ceci est un camion ce qu'il faut savoir c'est que ces tâches sont extrêmement faciles surtout en comparaison de ce qu'on exige des algorithmes de langage ou de recommandations mais alors la robustesse dans ces cas simples ne dit en fait presque rien de la robustesse dans des vraies cases d'applications beaucoup plus complexes en particulier il me semble dangereusement mensonger de prétendre que l'article en question correspond à la pratique ou en l'environnement de production c'est exactement comme si pour valider une voiture autonome on l'a testé sur un circuit fermé et on affirmait contrairement à la croyance établie nous montrons que la voiture autonome est très robuste en pratique même en utilisant des algorithmes simples et peu coûteux affirmer cela pour des applications aussi dangereuses que les algorithmes utilisés à grande et donc en fait beaucoup plus dangereux que des voitures autonomes puisqu’il mettaient déjà des millions de vie en danger en Inde au Brésil ou encore en Éthiopie ça me semble criminel contrairement à ce que fait cet article la sécurité ne peut jamais se valider à partir d'exemples où tout va bien surtout quand ces exemples ne sont pas représentatifs de l'énorme diversité des cas d'applications et des attaques que rencontreront les systèmes qu'on veut sécuriser la sécurité exige des garanties contre toutes sortes d'attaques y compris celles auxquelles on n'a pas encore eu l'intelligence de penser ou au moins des tests très poussés et très extensifs sur des algorithmes qui sont vraiment ceux qui vont être déployés en pratique on admettant on n'est pas capable de tester toutes les attaques auxquelles nos algorithmes seront confrontés affirmer une robustesse sans garantie de la sorte mathématique ou à travers des expériences très poussées c'est au mieux dangereusement trompeur et au pire c'est mettre volontairement la vie de milliards d'utilisateurs en graphe danger et autant ok Google est une multinationale et du coup il faut malheureusement attendre d'elle qu'elle agisse comme les et même en pire encore puisque elle a vraiment les meilleurs ingénieurs au monde et plus de moyens encore là où je suis extrêmement déçu c'est davantage vis-à-vis de la communauté scientifique qui non seulement gob et valide scientifiquement cette désinformation dans des revues scientifiques prestigieuses mais va même trop souvent jusqu'à célébrer ce genre de publication en invitant les auteurs de tels à prendre la parole tant des universités et faire ainsi de la publicité gratuite pour les groupes dans lesquels ils vont travailler en rendant Google et Facebook toujours plus cool et sexy j'aimerais que les chercheurs soient beaucoup plus méfiants de leurs collègues qui travaillent chez Google et beaucoup plus enclins à leur poser des questions difficiles comme au sujet de la transparence de l'éthique et de la sécurité des algorithmes que leurs employeurs développent voir sur l'origine finalement questionnable de leur propre salaire cette fois vu la gravité de ses affirmations avec Sadek fraterny et Oscar villemo notamment on a pris le temps et fait l'effort de des bonnes pièces cet article et toutes les informations autour de la sécurité de l'apprentissage fédéré dans un article qui est récemment été accepté à publication à icml l'une des tops conférences en machine learning et je dis ça comme c'était facile de publier iciml mais c'est vraiment un travail monumental à abattre qui fait rêver plus d'un académie voire qui fait rêver des entreprises en gros l'article de Google dont on a parlé affirme que les attaques par injection de données sont bénignes alors que les attaques byzantines sont irréalistes notre article débunk ses affirmations en démontrant que dans les cadres théoriques prédominants de l'apprentissage fédéré personnalisé ces deux attaques sont en fait équivalentes et donc si l'une est bénigne l'autre doit nécessairement être bénigne elle aussi et inverse si l'une est dangereuse alors l'autre doit extrêmement considéré dangereuse elle aussi notre article exploite ensuite cet équivalence pour concevoir des attaques par injection de données dévastatrices contre l'apprentissage fédéré non sécurisé y compris dans des cas simpliste comme la classification d'images mistes et surtout en démontrant l'équivalence notre article permet d'importer les théorèmes issus de la littérature sur la résilience byzantine au cas de la résilience contre l'injection de données malveillantes en particulier nous démontrons l'impossibilité de sécuriser tout apprentissage fédérés dans le cas où il y a une forte hétérogénéité dans le comportement des utilisateurs authentiques ce qui est le cas en pratique malheureusement encore une fois la désinformation de Google a beaucoup plus de citations que notre article en l'occurrence 26 fois plus que notre article qui n'a été cité qu'une fois j'aimerais pouvoir vous dire que ça va changer nettement dans le bon sens mais bon je suis pas très optimiste ceci étant dit comme nous le soulignons dans l'article de synthèse de littérature tout le travail qu'on a effectué sur la sécurité du machine learning depuis maintenant 6 ans reste à prendre avec des pincettes surtout quand il s'agit des théorèmes positifs de sécurité en effet c'est théorème positif de sécurité repose eux-mêmes sur des hypothèses peu réalistes ou très problématiques comme le fait qu'il faut généraliser le comportement de la majorité des utilisateurs alors bien sûr il y a des cas d'application ou généraliser les données et justifiables notamment lorsqu'on applique le machine learning à la physique ou à la chimie cependant cette hypothèse est devenue si commune et si répandue que très souvent les scientifiques n'attendent plus de leurs collègues qu'il la justifie dès lors le champ d'applicabilité des travaux publiés et flous et inexpliqués dans une énorme proportion de la recherche académique or dans les applications pratiques les plus influentes les plus créatives et le plus dangereuses en machine learning notamment le ciblage publicitaire et la recommandation de contenu généraliser les données revient à normaliser le statu quo à amplifier les addictions ou triompher la désinformation ce qui me semble terriblement dangereux pour les sociétés humaines et le problème n'est pas limité à des histoires de faux comptes même en supposant l'absence de tout fauconte ce qui est complètement irréaliste mais selon charitable à l'instant le comportement de beaucoup de comptes authentiques peut-être très indésirable à apprendre et à généraliser comme on l'a vu par exemple ou dans le cas de taïwes cette IA de Microsoft a appris de compte troll mais probablement authentique et elle a du coup généralisé leur manière de s'exprimer ce qu'il a transformé très rapidement en un monstre sexiste raciste et génocidaire la désinformation le harcèlement et la haine sont en fait massivement produits au moins relayés par une énorme fraction peut-être même la majorité des comptes authentiques surtout sur des plateformes comme Twitter ou Reddit tout algorithme qui apprendra de ces données même avec les meilleurs procédures de sécurité proposées jusque là dans la littérature scientifique sera avoué répété et à amplifier massivement beaucoup de comportements non seulement détestable mais qui pourraient aussi et surtout aggraver le déclin démocratique à l'échelle mondiale les risques de guerre mondiale il est urgent que toute la communauté du machine learning mesure réellement l'ampleur de la responsabilité que leur travail implique et des conséquences dangereuses de leur silence depuis que Google utilise nos algorithmes à l'échelle planétaire nous ne sommes plus des chercheurs dont la science fondamentale est déconnectée du monde réel nos choix de sujets de recherche chez nos hypothèses de travail mais aussi nos décisions des collaborations à établir les intervenants à inviter et des collègues à recruter tout ça joue désormais un rôle majeur dans la guerre de l'information sur la sécurité des algorithmes les plus influents du monde moderne et sur l'urgence du besoin d'investir massivement dans leur audits à chaque fois que nous faisons l'hypothèse qu'il faut minimiser la fonction de coût qui cherche à coller aux données surtout sans souligner les conséquences d'invoquer ces hypothèses sur l'inapliquabilité de nos travaux notamment pour les algorithmes les plus influents nous célébrons et nous normalisons l'apprentissage de la génération de toutes les données accessibles et nous contribuons à omettre et à améliorer les problèmes pourtant largement avérées de cyber-harcèlement au moment de la conception des algorithmes à chaque fois que nous nous questionnons pas cette hypothèse dans les publications et dans les présentations des autres nous normalisons et nous facilitons le déploiement d'algorithmes qui apprennent généralise et amplifie massivement et dangereusement la désinformation et les appels à la haine et à chaque fois que nous relions des images produites par des algorithmes de Facebook des textes produits par GPT 3 ou des avancées biologiques de Google deep mind nous amplifions la publicité gratuite pour la spectacularité du machine learning et surtout nous faisons toujours plus de la sécurité et de l'éthique des algorithmes une mute news c'est à dire un sujet dont presque personne ne parle en plus de normaliser l'absence de contrôle démocratique sur les algorithmes qui contrôlent le plus le flux de l'information et de la désinformation à travers le monde et si vous pensez qu'il n'y a rien de mal à célébrer les avancées spectaculaires du machine learning c'est certainement parce que vous n'avez jamais essayé de promouvoir activement la sécurité et l'éthique du machine learning et que vous ne vous êtes ainsi jamais rendu compte de l'ampleur de l'inattention voire du mépris que suscite ces sujets pourtant beaucoup plus important célébrer à spectacularité du machine learning dans le contexte actuel c'est un peu comme célébrer l'ingéniosité de la construction spectaculaire de stade climatisé en plein désert c'est prioriser l'avancée technologique et le confort du métier de chercheurs en machine learning aux dépens des souffrances avérées et dramatiques de millions d'humains [Musique] j'espère vous avoir convaincu que des géants comme Google ce sont bel et bien lancées dans la désinformation scientifique et que celle-ci est extrêmement dangereuse surtout dans le contexte de cyberguer et de guerre de l'information du monde moderne et malheureusement la communauté scientifique est en train de gober d'amplifier et de normaliser la désinformation de Google mais surtout les rares a vraiment prendre le temps d'identifier cette désinformation et de la déconstruire sont sous valorisés sous-financés et ignorés voilà qui semble garantir le triomphe désinformationnel et dystopique de Google dans une guerre de l'information dans laquelle tous les chercheurs en informatique sont impliqués qui le veuillent ou non ne serait-ce parce qu'ils demandent de l'attention des collaborateurs et du financement potentiellement au détriment de ce qui luttent vraiment contre la désinformation typiquement beaucoup de ressources que j'avais à ma disposition notamment des collaborateurs de premier rangs ont ainsi été volés par d'autres académiques qui les utilisent maintenant pour des travaux qui me semblent beaucoup moins utiles voire parfois néfaste à la société oui parce que pour des bunker les travaux de Google comme je l'ai fait plutôt au niveau de la communauté scientifique il faut vraiment beaucoup d'expertise de temps et de sacrifices je sais que je me suis grillé vis-à-vis des entreprises qui sont pleinement engagées dans la désinformation et ont littéralement perdu tous les tiques en plus de mettre fait quelques ennemis chez eux mais je crains aussi que même des universités ou des chercheurs qui peuvent avoir des partenariats très forts avec Google puissent refuser de s'associer à moi à l'avenir et alors moi en fait j'ai d'autres projets notamment de startups pour fournir des vrais solutions de sécurité et pour former des équipes de Data science à la sécurité du machine learning pour au hasard éviter d'énormes fuites de données via les algorithmes de machines learning mais le problème est souvent beaucoup plus délicat pour mes indispensables collaborateurs généralement bien plus jeunes et moins établis qui charge des stages des bourses et des opportunités de carrière il m'arrive ainsi même de conseiller à des jeunes de parfois faire ce qu'il faut pour publier comme motiver le machine learning par dipoteries alors que clairement ce sera beaucoup plus probablement utilisé pour faire du ciblage publicitaire ou comme motivé la recherche sur l'accélération des algorithmes par une réduction de consommation énergétique alors que clairement il va y avoir un effet rebond inéluctable et que surtout ça fait encore de sujets de l'éthique et de la sécurité un sujet secondaire voire une nuqueuse malheureusement le monde Academy gouverné par le prestige et des métriques comme le h1dex envient souvent à ignorer l'impact sociétal des travaux et a tué la carrière de ceux qui ne font pas ce qu'il faut pour publier publish en perruche comme on le dit souvent mais du coup les survivants ceux qui décident des recrutements et des financements seront plus souvent ceux qui ont pris pour habitude et de valoriser ceux qui font ce qu'il faut pour publier quitte à surfer sur les tendances souvent initiées par Google avec des objectifs lucratifs ou de diversion et quitte à ignorer les sujets plus difficiles non publiables et souvent beaucoup moins cités par les Pères comme la sécurité et l'éthique pire encore ces scientifiques établis ont trop souvent encore ce préjugé tenace selon lequel un bon scientifique ne doit s'exprimer que sur la science laquelle doit rester neutre et impartial alors que normaliser le fait de négliger la sécurité en faisant notamment des hypothèses ID on supposant que les données doivent être généralisées en fait absolument pas neutre ni impartial et encore moins responsable ni scientifiquement correct en tout cas aussi triste que cela puisse paraître on en est aujourd'hui là dans un état du monde académique que la désinformation de Google mais aussi de Facebook ou encore d'openaerei à sans doute chercher et à Clermont réussi à obtenir à l'instar des algorithmes tant que les financements et les promotions dans le monde académique ne tiendront pas sérieusement compte de l'impact social des travaux effectués la science aura davantage la voix de ceux qui ont le plus de moyens pour écrire des articles d'apparence scientifiques et il faut craindre que la théorie du machine learning prédominante soit celle qui arrange Google à savoir une théorie qui ne suppose pas de dangerosité dans la généralisation et l'amplification des données ou qui ne garantit la sécurité que sous des hypothèses irréalistes ou dans des expériences très restreintes et alors des banquiers c'est bien mais construire des solutions vraiment sécurisées notamment pour la solidité des démocraties et pour la paix dans le monde c'est bien sûr beaucoup mieux idéalement j'aimerais pouvoir consacrer l'entièreté de mon temps à cela plutôt qu'à passer mon temps à lire les tissus de mensonges que Google écrit pour ensuite expliquer à la communauté scientifique et au grand public le danger que ces torchons représentent mais bon comme presque personne ne fait ce travail de demonking et qu'il est quand même important que quelqu'un le fasse mais bien sûr ce qui m'anime le plus ce qui me motive le plus c'est davantage de construire des solutions pour notamment gouverner collaborativement de manière sécurisée les algorithmes en particulier j'aimerais pouvoir développer des solutions techniques pour garantir autant que possible que ces algorithmes fassent ce que nous voudrions vraiment qu'il fasse et non pas à ce que des données incontrôlées téléchargées du web poussent ces algorithmes à faire et après maintenant 6 ans de lecture et de réflexion sur ce sujet précis et 12 ans de réflexion plus globale sur la gouvernance collaborative j'en suis venu à la conclusion qu'un projet participatif comme tournesol était de très loin le projet le plus prometteur pour combattre la désinformation et la haine et pour valoriser beaucoup plus l'information de qualité au lieu de faire l'hypothèse I d tournesol suppose que chaque utilisateur à ses propres préférences non seulement sur les recommandations qui étaient à les recevoir mais aussi et surtout sur celles que les autres utilisateurs sont amenés à recevoir tournesol utilisant ensuite l'état de l'art en machine learning sécurisée qu'on a développé depuis six ans et en particulier un algorithme appelé pour agréger de manière équitable et sécurisée les jugements de nos différents contributeurs le projet a soulevé de nombreux autres défis de recherche qui vont du développement de nouveaux algorithmes et de la démonstration mathématique de leur sécurité à l'analyse empirique des données collectées en passant par une étude de l'impact social de tournesol sur les habitudes de consommation des utilisateurs une étude que l'on est encore en train de mettre en place qui si tout va bien va être effectué prochainement et vous pourrez être les sujets mais pour en faire plus il nous faut absolument faire en fait beaucoup plus tournesol aura surtout urgemment besoin de beaucoup plus de moins que les dons de quelques personnes pour payer plus de personnes que notre unique employée responsable aujourd'hui du développement informatique du site web malheureusement jusqu'à présent nos 6 demandes de subvention ont tout été rejetées par diverses entités qui me sont en fait toute grave en sous-estimé l'impact monumental des algorithmes de recommandation et l'importance de concevoir à nos solutions pour les gouverner collaborativement dans ces conditions faute de moyens impossible de se battre contre Google des principaux objectifs de tournesol c'est en fait d'attirer l'attention de la communauté scientifique sur le problème de la gouvernance collaborative et sécurisée des algorithmes en identifiant clairement des problèmes concrets dont la résolution aurait des réelles impacts bénéfiques donc si vous avez des amis dans le monde académique envoyez-le cette vidéo engagez la discussion avec eux sur la désinformation dont leur milieu souffre et demandez-leur s'ils ont déjà vraiment réfléchi à la sécurité et à l'éthique des algorithmes les plus influents du monde et si ils cherchent des sujets de recherche éthiques vous pouvez mentionner ton seul notamment cet article en particulier tournesol invite tout académique à télécharger la base de données publiques de jugement des contributeurs cette base de données contient maintenant presque 50 000 comparaisons ce qui en fait une base de données comparable à ministes ou six fardistes qu'on a mentionné plus tôt mais ça serait vraiment bien qu'on dépasse la barre symbolique des cent mille comparaisons et vraiment convaincante si on dépassé celle des 1 million de comparaisons d'autant que cette base de données nous semble beaucoup plus intéressante que ministres ou si Fardi puisqu'il s'agit de jugement humain sur ce que devrait faire les algorithmes les plus influents du web qui plus est la structure de ces données ressemble beaucoup plus à la structure des données collectées par les géants du web à savoir des données associées à des utilisateurs souvent ainsi naturellement les problématiques de privacy et de sécurité enfin ces jugements correspondent à des annotations de vidéos YouTube de qualité les objets d'étude beaucoup plus intéressants que des images nistes ou si par 10 en particulier on peut ainsi espérer comprendre et généraliser la notion de qualité d'une vidéo selon des jugements humains une généralisation qui me semble beaucoup plus désirable à faire que la généralisation des comportements des utilisateurs sur les réseaux sociaux mais pour être entendu de la communauté scientifique pour détourner leur attention de la désinformation de scientifiques de Google et des opportunités fantastiques qui avoir à devenir ami avec eux et pour orienter la recherche de ces académiques vers les problèmes de tickets de sécurité de l'information qui requiert tant leur expertise leur attention et leur travail il va falloir crédibiliser tournesol et valoriser sa base de données publiques et ça ça dépend énormément de vous vous pouvez grandement aider à améliorer la qualité de la recherche scientifique en intelligence artificielle tout simplement en créant un compte sur tournesol et en comparant la recommandabilité de différentes vidéos YouTube que vous avez visionnées vous pouvez aussi contribuer à notre base de données publiques et ainsi fournir des données intéressantes pour les chercheurs académiques en vous portant garant pour vos amis et en demandant à vos amis de se porter garant pour vous et ça au passage ça augmentera aussi vos droits de vote enfin vous pouvez relayer les appels à contribuer à tournesol et par exemple demander à vos vulgarisateurs journalistes et influenceurs préférés de parler davantage de ce projet ou de partager les liens tournesol plutôt que les liens youtube notamment pour les vidéos déjà pas mal notées sur tournesol la recherche en éthique et en sécurité est aujourd'hui hackée dénigrés et ignorer elle a besoin de vous pour s'épanouir et enfin contribuer à protéger les sociétés humaines contre les fléaux de la désinformation du harcèlement et de la haine