ça fait environ 10 épisodes qu'on parle de l'importance de la morale désir on a vu qu'il n'ya bien conçu pouvait entre autres combattre la discrimination sauver des vies réduire notre impact environnemental et lutter contre les risques de catastrophe majeure mais on a aussi vu qu'une liasse en valeur morale elle est très probablement empirer les filter bobl sur internet promouvoir le pit a cliqué l'indignation augmenter notre addiction au numérique voir combattre pour sa propre survie et acquérir toutes les ressources disponibles bref l'ia est en train de prendre de plus en plus de place dans nos sociétés elle acquiert ainsi de grands pouvoirs et de grand pouvoir implique de grandes responsabilités et du coup il est intéressant de se demander quelle est la morale d iliad aujourd'hui ou plutôt qu'est ce qui motive les ira à faire ce qu'elles font eh bien il se trouve que la quasi-totalité des ja d'aujourd'hui repose sur un même principe sesia cherchent à maximiser un score ou de façon équivalente à minimiser des sortes de pertes c'est quelque chose dont on a déjà parlé dans l'épisode 9 sur la régression linéaire une régression linéaire ça consiste à déterminer un modèle qui minimisent l'écart entre les prédictions du modèle et les observations et même plus précisément la somme des carrés des écarts entre la théorie et les données il s'agit là d'un principe très général la cp de l'épisode 20 détermine l'hyper plan qui minimise les écarts entre un nuage de points et un hyper plan li a donné fixe maximise la probabilité que vous cliquez sur ses suggestions elia de youtube maximise le temps que vous passerez un boîtier des vidéos sur youtube et de façon très générale les ja fonctionne souvent avec un principe de carottes et de bâtons les programmeurs leur donne des carottes quand elles font ce que les programmeurs veulent qu'elles fassent et des coups de bâton quand elles font ce que les programmeurs ne veulent pas qu'elle fasse les ja sont alors programmé pour maximiser les carottes et minimiser les coups de bâton mais si vous y voyez une analogie avec les humains bah oui en fait historiquement le formalisme des carottes et des bâtons que l'on appelle aussi l'apprentissage par renforcement c'est quelque chose d'inspirer de la biologie comportementale que l'on associe souvent à des chercheurs comme pavlov ou szpiner en particulier skinner s'est amusé à récompenser des pigeons avec non pas des carottes mais des graines à chaque fois que les pigeons faisaient l'action suggéré par un mot écrit les pigeons a pris ainsi à associer certains mots avec certaines actions afin de recevoir leurs carottes et là on parle de pillon mais ce genre d'apprentissage pour renforcement est présent chez l'homme aujourd'hui typiquement à l'école où nous engueuler quand on fait des fautes d'orthographe et on nous donne des bonnes notes quand on pense à appliquer le théorème de pythagore au triangle rectangle et ce genre d'apprentissage a lieu bien au delà du cadre de l'école on la retrouve aussi dans la loi ou dans la religion surtout dans la région d'ailleurs la punition est la récompense que du réserve est infiniment plus motivante qu'aucune de celle qui peut appuyer la loi si dieu veut être pu lire ce sera une éternité de souffrance et si dieu veut récompenser ce sera une éternité de bonheur donc l'enjeu est immense l'éternité c'est long surtout vers la fin dieu tient une énorme carotte et un énorme bâton c'est un truc sexuelle la loi en comparaison elle a un tout petit bâton et pas vraiment de carottes et alors on pourrait croire que ce genre d'apprentissage un petit peu bateau est limité mais l'espoir d'une carotte ou la peur d'un bâton peuvent soulever des montagnes ou en tout cas conduit à une intelligence stratégique stupéfiante par exemple en 2015 l'ia de deep net a atteint des performances de niveau humain ou jeu d'arcade d'atari en exploitant uniquement les couleurs des pixels à l'écran et en ayant pour carottes plus corps du jeu l'objectif de maximiser les carottes a suffit à motiver cette ia pour concevoir en elle un modèle du jeu et une recherche de stratégies sophistiquées et ça a bien sûr c'est en 2015 aujourd'hui les ja pas renforcement sont capables de prouesses toujours plus sophistiquées en atteignant des niveaux humain ou surhumain à des jeux comme starcraft dota 2 ou capture the flag est d'ailleurs la performance de 2018 à capture de flag est particulièrement impressionnante parce que capture de flingues est un jeu collaboratif qui exige de lire qu'elle comprenne ce que ses coéquipiers humains ou ia ou l'intention de faire l'autre exemple bien connu est celui du jeu de go avec notamment la défaite de lys idole contre alpha gore 2016 le cas d'hugo est d'ailleurs particulièrement intéressant parce que la carotte est relativement abstraite oui parce que dans ce cas la carotte et la victoire en fin de partie et le bâton la défaite mais cette carotte et ce bâton n'interviennent qu'en fin de partie or une partie de go est souvent très longue l'astuce d'alpha go flûte alors de s'armer d'une estimation de la probabilité d'avoir la carotte en fin de partie et c'est cette probabilité de carottes khalfa go maximise au cours de la partie on parle aussi de fonctions d'évaluation et je vous renvoie vers la vidéo de david sur science étonnante pour savoir plus alors il ya quelques détails techniques additionnel que je passe sous silence mais en gros cette fonction d'évaluations correspondent plus ou moins une fonction que l'on appelle la fonction cul oui ça s'appelle la fonction qu c'est pas moi qui ai choisi la fonction q10 alia pour chaque état du jeu et pour chaque action entreprise quelle est la quantité de carottes futur qu'on peut espérer une fois qu connu il suffira ensuite de choisir à tout instant l'action qui maximise la fonction qu étant donné l'état du jeu oui encore des histoires important de facteurs d ce compte des carottes du futur et de dilemme exploration versus exploitation sur lesquels il faudrait disserter mais je vais me contenter de textes à l'écran et je vais passer tout ça sous silence à l'oral résoudre le problème d'apprentissage correspondait alors à bien estimer la fonction qu on parle d'apprentissage de la fonction cube ou de cul learning et on cherche souvent à estimer cette fonction qu à l'aide de nombreux outils plus classiques comme l'apprentissage supervisé à partir d'exemples et les fameux réseaux de neurones profond ou deep learning mais oublions ces détails de l'estimation de la fonction qu pour aujourd'hui de nos jours il semble que le paradigme du cul learning soit le paradigme le plus puissant pour concevoir désir dans des systèmes d'interactions complexes et tout le principe du cul learning c'est tout bêtement d'estimer les probables futurs carottes et coups de bâton quand dans un état donné ont choisi d'entreprendre telle ou telle action bref les carottes et les bâtons se sont en fait des outils redoutablement puissant pour motiver les ja a longuement réfléchir pour mieux comprendre le monde qui les entoure et à prendre des décisions qui amèneront très probablement à des conséquences qui sont celles que les programmeurs soit si bien que cette approche semble aussi être le fondement d y aller plus puissantes du futur en tout cas il semble que ce soit essentiellement ainsi que fonctionne via de youtube cette ia possède un score qui est le temps que les utilisateurs passent à regarder des vidéos pendant disons un mois donné et pour maximiser ses carottes l'ia de youtube va chercher à optimiser les suggestions de vidéos pour que les utilisateurs cliquent reste et reviennent aussi longtemps que possible sur youtube mais alors ce comportement de l'ia de youtube il est tout à fait légale et on a même tendance à dire qu'il est parfaitement innocents cependant même si j'en ai déjà parlé dans l'épisode 29 j'aimerais insister encore une fois sur les conséquences potentiellement désastreuses de ce comportement de y'a de youtube parce qu'à bien y réfléchir même si ça serait exagéré de dire que l'ia de youtube en serait responsable on peut néanmoins constater que youtube a favorisé la prolifération et le partage d'idées très dangereuse par exemple de nombreuses vidéos se positionnant fermement contre les vaccins ont été rendues virale par youtube youtube a donc été un catalyseur du mouvement antivaccin alors ça ne veut pas dire que les vidéos anti vaccins sont majoritaires cependant à cause de la personnalisation des contenus suggérés aux utilisateurs on a des phénomènes de filter bubbles qui apparaissent et de la même manière que mes suggestions contiennent beaucoup de vidéos de maths des suggestions des anti vaccins ont probablement beaucoup de vidéos anti vaccins et ça pose problème parce que les sciences sont assez univoque ne pas vacciner votre enfant celle exposée à de nombreuses maladies mortelles et des maladies horrible qui plus est facilitée la prolifération des contenus anti vague sans certaines communautés peut donc être vue comme un acte criminel ou du moins un acte qui a conduit à des conséquences très tragique d'une certaine manière lié à de youtube a partiellement causé la mort de jeunes enfants alors je prends là l'exemple des vaccins mais les idées dangereuses qui se propage à travers youtube qui sont en fait très nombreuses par exemple suggéré aux utilisateurs des vidéos de 4,4 promeut la consommation d'engins très polluant ce qui pose tous les problèmes que l'on connaît de même recommandé à des utilisateurs des vidéos extrêmement sexistes revient à normaliser les inégalités de joueurs dans nos sociétés auprès de certains individus et notez bien que dans tous ces cas il ya de youtube n'a pas de mauvaises intentions en agissant ainsi elle rend les utilisateurs plus fidèles à youtube ce qui augmente le witch tale et fait donc gagner tout plein de carottes alia de youtube cette ia fait uniquement ce qu'il faut faire pour maximiser les récompenses qu'on lui a promise d'une certaine manière on peut dire que la seule morale de l'ia de youtube c'est de maximiser le witch time et ça c'est moralement discutable et ça a des conséquences potentiellement tragique et ça surtout malheureusement c'est la morale de la quasi-totalité des i had aujourd'hui on les motive à coups de carottes et de bâtons ces carottes et ses bâtons ne sont pas liés à des valeurs morales humaines mais du coup pour maximiser leurs carottes et minimiser leurs coups de bâton saisie à risque fort d'entreprendre des actions qui ont pour dommages collatéraux des phénomènes que nous autres humains juges ont quasi-unanimement moralement indésirables l'aderf en a parlé de catastrophe planétaire et du rôle que pouvait jouer il n'ya pour essayer d'empêcher ces risques planétaire et hissler nous demande aussi des nanomachines tueuse est un scénario plausible et j'ai envie de dire que c'est extrêmement difficile de dire que ça une probité 0 d'arrivée encore une fois quand on raisonne par rapport à tous ces scénarios du futur la question n'est pas de savoir si un scénario donné va arriver la question c'est est ce qu'il est probable qu'il arrive et sachant tous les scénarios possibles de fin du monde il faut à peu près ajouté toutes ces différentes probabilité pour se rendre compte que lorsqu'on cumule tous ces scénarios possibles il ya quand même une probabilité qui me semble non négligeable qu'il y ait une catastrophe planétaire d'ici la fin du siècle et en particulier sur une anomalie tueuse dont par le hissent leur en effet ces races et ça peut être assez dangereux notamment pas nécessairement parce que leur but est de tuer ce qui me paraît peut-être même plus probable que des nanomachines de tueuse parce que bon pour tuer on a déjà les drones tueurs ce qui est ce qu'ils seront qui seront sans doute plus facile à mettre en place surtout sur les imprimantes 3d qu'on arrive à avoir des modèles bref je passais sur la partie aux boutures mais si je parle juste d'un point de vue occidental c'est qui pourrait arriver c'est qu'on conçoivent des nanomachines qui sont des réplicateurs de bonne manne qui est réplicateurs de von neumann ce sont des structures qui sont capables de trouver des objets autour d'elle à partir de cette matière première est ce qu'ils sont capables de se répliquer et ça correspond à un modèle assez simple qu'on prévoit d'ailleurs pour l'exploration spatiale spatiale et dont j'ai déjà parlé dans l'épisode 27 lorsque j'ai joué au jeu des trombones aujourd'hui on sait déjà qu'il y à des molécules qui ont un peu près s'être propriété notamment les clients qui sont des protéines qui ont une structure spatiale qui n'est pas la structure spatiale de qu'on aimerait qu'elle faisait et qui peuvent poser des frais qui peuvent être mortels tout simplement je vous renvoie vers la vidéo de cette étonnante sur les prix notamment pour à en savoir plus et ça me paraît assez difficile l'exploit possibilité qu'il y ait plein d'autres molécules qui auront cette faculté à se répliquer assez facilement avec peu de matériel notamment à ces possibles que concevoir la première de ces molécules soit une étape difficile c'est pour ça que l'évolution n'a pas su créer la première étape de ces molécules mais c'est possible que avec des techniques d'ingénierie on arrive à créer cette première molécule et si cette molécule nous échapper le poids se répliquer et un truc qui s'auto répliques comme ça s'ils sont au de réplication est plus rapide que son taux de mortalité une certaine manière si elle se croit plus vite qu'elle ne meure on aura une croissance exponentielle qui pourraient être potentiellement très rapide du nombre de ces nano machines aux deux de ces molécules long ou enfin des nanomachines et l'art la secret place qui serait potentiellement extrêmement dangereuse puisque une croissance exponentielle ça à perdre très rapidement les limites physiques un truc comme ça qu'ils se réplique très facilement partout sur terre c'est un truc qui me ferait vraiment vrac c'est vraiment un risque la licence yali l'équipe a complètement anéantir énormément de choses et qui pourrait être également très difficile à combattre c'est pour celle que le risque globalement des nanotechnologies est ainsi intériste qui me semble devoir être pris au sérieux alors je suis pas du tout expert ben je sais pas du tout quelle est la probabilité d'avoir de tels événements mais me semble que l'état la connaissance actuelle en nanotechnologie est suffisamment limité pour qu'on ait du mal à exclure complètement ce genre de possibilités et ça ça m'amène un raisonnement fait par nick bostrom dans son livre super intelligence où il parle de ce risque de nanotechnologies et il se pose la question on va sachant qu'il ya des risques existentielle est bon il ya aussi intelligence artificielle qui a ses propres risque existentiel encore une fois les épisodes notamment 26 et 28 pour parler pouvoir déjà ce que j'ai dit sur les risque existentiel que peuvent causer une heure et demie à deux niveaux humain et le truc aujourd'hui c'est que c'est très difficile de freiner le progrès tellement d'intérêt économique rend un show que c'est assez difficile d'imaginer qu'il n'y aura pas énormément de recharge pour que la création d'une ligue à deux niveaux humain en ce moment il ya énormément de recherches qui vont plus ou moins dans cette direction et du coup c'est un risque qui est très difficile à exclure le risque de l'intelligence artificielle de niveau humain le risque d'un à noter que j'ai évidemment est très difficile à exclure mais du coup il ya une question de timing si les nanotechnologies apparaissent avant l'intelligence artificielle de niveau humain alors on aura d'abord le risque existentiel des nanotechnologies qui deviennent des régulateurs de nomades ou des trucs vraiment très dangereux et ensuite le risque de l'intelligence artificielle pour pouvoir survivre 21e siècle va falloir survivre à cela seul de ces deux risques donc la probabilité de survivre va être diminuée de la somme des probabilités des deux événements maintenant si on arrive à faire l' inverse c'est à dire si on arrive à le faire d'abord l'intelligence artificielle de niveau humain et ensuite les technologies arriverait après à ce moment là on pourra utiliser les intelligences artificielles de niveau 1 pour mieux contrôler les risques et sans doute qu'une intelligence artificielle au niveau humain pourrait même peu essentiellement anéantir les risques des nanotechnologies parce qu'elle comprendra ça beaucoup mieux que nous surtout si elle atteint un niveau surhumains et du coup la probabilité de survie au 21ème siècle ne serait que finalement la probabilité de survivre à l'intelligence artificielle de niveau humain donc ça c'est un argument de bochum pour dire que en fait c'est super important que l'intelligence artificielle de niveau humain apparaissent avant les autres risque existentiel potentiellement catastrophiques comme les nanotechnologies les biotechnologies mais disons que les drones tueurs ce petit raisonnement de bostrom il conduit à la conclusion qu'en fait il faut accélérer autant que possible la recherche sur l'intelligence artificielle même si l'intelligence artificielle de niveau humain notamment posent un risque existentiel scène plus racée contre intuitive mais que je trouve assez assez convaincant soit finalement je pense que c'est assez important de développer la recherche en devenir même si le problème de la morale désire me semble encore encore à l'âge de pierre pas à part de ça du coup vraiment une des priorités vraiment aujourd'hui c'est vraiment travailler sur la recherche sur la morale les ja je pense aujourd'hui et je pense qu'il ya vraiment pas encore assez de gens qui s'intéressent à ce problème et surtout qui parle sérieusement de ce problème ou qui prennent au sérieux ce problème est qu'ils ne le moc pas et en particulier beaucoup trop de chercheurs en intelligence artificielle qui prennent plaisir à se moquer des gens qui travaillent sur les problèmes de risques risques causés par une intelligence artificielle enfin dominique and the man pose la question par rapport à lla capable de prédire le chaos donc en fait le titre de l'article de quanta magazine est assez musclé ligne est assez trompeur puisque non liés à n'arrive pas à prédire les sytèmes chaotique puisque les systèmes chaotique ont quelque chose de vos amants assez fondamentalement imprévisibles notamment le principe du cas où c'est justement qu une imprécision dans les données observées fait que le futur est finalement imprévisible à partir d'un certain temps peu importe le niveau intelligence que l'on a si les données sont pas suffisamment précises et dans un système chaotique une toute petite erreur dans une mesure même de la 1% peut causer des changements de comportements radicaux et donc même une ia n'est pas capable de faire quelques prédictions que ce soit cependant ce qui a été réussi et c'est donc par l'article de compte et magazines ce sont des prédictions à plus long terme que ce que d'autres méthodes plus classiques qui suivent par exemple les principes de la physique arrive à faire typiquement quand on parle de système chaotique on aime avoir une unité de temps qu'on appelle le temps de lire tout neuf qui correspond au temps à partir duquel en gros en ordre de grandeur le système deviendra complètement imprévisible et bong a généralement atteindre le temps de la pin up en terme de prédiction c'est fiable c'est déjà un problème très très difficile et sesia certaines dia dont certains problèmes ont réussi à atteindre plusieurs fois à faire des prédictions à plusieurs temps de l'iap neuf mais bon ce sera jamais à plus de 100 temps de l'es9 et ce sera clairement jamais à plus d'un milliard de fois le temps de lire plus le pont neuf c'est mathématiquement impossible de faire plus de prêts si c'est quand même bon de rappeler qu'une intelligence artificielle même super intelligence c'est pas non plus un truc qui c'est tout c'est pas du tout une momie quelque chose de mission ça reste quelque chose qui résonne à partir de données pour faire des prédictions puisque ce calcul là est bien mieux fait que ce qu'on sait faire nous mais ça ne reste que ce genre de calcul que sait faire que ce sera fair une intelligence artificielle et en particulier sera limitée par les données complètes qu'elle pourra avoir accès nous on pouvait pas faire de bonnes prédictions vous pouvez pas être intelligent si vous n'avez pas des données pour devenir intelligent et puis tant qu'à parler des limites des ia du futur les ja du futur seront également nécessairement limités par les limites de puissance de calcul en particulier une théorique la théorie de la complexité algorithmique qu'il qui dit ce qui ne peut pas être résolu en temps raisonnable tels que l'âge de l'univers disons et notamment il ya les pointes de cryptographie qui sont pas résolues bhl a priori en un temps plus grand que l'âge de l'univers et sesia seront pas capables de casser les codes de cryptographie pour cette raison à moins d'utiliser un ordinateur quantique moi je vous renvoie vers la vidéo que j'ai fait avec string theory notamment j'ai parlé un petit peu de cryptographie post quantique et j'espère qu'ils avaient aimé cette vidéo sur l'apprentissage pas renforcement et du coup quand les ja d'aujourd'hui acquiert une fonction d'objectifs dans les prochaines vidéos on parlera beaucoup de cette fonction objectif est de comment la concevoir notamment si vous avez aimez cette vidéo pensez à la locale a commenté à la partie pensez à vous abonner pour les pilotes version hip hop l'ordre et j'espère que vous serez là la prochaine fois si dieu n'existe pas non tout n'est pas permis tout n'est pas moralement permis il n'est pas moralement permis de tuer quelqu'un comme ça juste pour rire que dieu existe ou non pour nous punir de l'avoir fait non c'est pas faux mais du coup quand même vaut mieux croire que dieu est là pour nous punir au moins sa force morale et bien en fait gérés plutôt que ça nous empêche d'être immoral et c'est pas tout à fait la même chose et en fait on pourrait même montré que ça nous empêche aussi d'être moral states action ajusté 1 0 par action conséquence en action est en baisse l'action state street et stuck on développe agent s to maximize du golfe persique en actions est un maximum à 16h est extrêmement grave