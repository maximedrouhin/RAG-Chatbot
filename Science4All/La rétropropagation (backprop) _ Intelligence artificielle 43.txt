dans l'épisode précédent on a vu le fonctionnement et l'importance de la descente de gradient pour le développement d'une intelligence artificielle de façon grossière la descente de gradient permet l'exploration guide et d'algorithmes de grande complexité ce qui si l'on en croit turine est indispensable à la conception dire de niveau humain et dans l'épisode d'encore avant j'ai suggéré que l'efficacité stupéfiante des réseaux de neurones s'expliquait en grande partie par leur aptitude à se prêter merveilleusement bien à la descente de gradient et sas avait en particulier d'une formule mathématique appelé la dérive et des fonctions composés qui dans le cadre des réseaux de neurones a été depuis renommée algorithmes de rétro propagation ou bac propagation ou juste bac propre en anglais c'est cet algorithme qui permet aux réseaux de neurones de calculer rapidement et efficacement les gradients qui permettront ensuite d'ajuster les paramètres de ces réseaux de neurones pour mieux expliquer les données avant de commencer leur aplomb comment un réseau de neurones effectué des prédictions pour ce faire il prend des informations en entrée via sa première couche de nos rhône puis une suite d'opérations des neurones et de transmission de messages via les synapses des neurones conduit à une certaine activation des neurones de la dernière couche on a aussi vu que la suite et d'activation neuronale du cerveau dépend des deux trois types de paramètres premièrement il ya les fonctions d'activation des neurones ensuite il ya les billets des neurones enfin chaque synapses est associé à un poids qui fait de la synapse une synapse plus ou moins inhibitrice ou plus ou moins activatrice alors les fonctions d'activation généralement on n'y touche pas du coup les paramètres du réseau sont au final les billets des neurones et les pois synaptique qui correspondent à une grande collection de boutons à tourner il faut donc imaginer qu'un réseau de neurones est une immense machine avec un bouton par synapse et par neurones et comme on l'a vu la dernière fois un gradient doit nous dire dans quelle direction et à quel point il faut tourner les bouton de la machine imaginons pour simplifier le cas où le réseau de neurones doit prédire la présence ou l'absence de chats dans une image ou donne alors une image que chat aux réseaux de neurones qui effectue ses calculs complexes pour conclure que l'image une probabilité 0,6 de contenir penchard 0,6 c'est pas mal mais idéalement on aimerait que le réseau de neurones conclu que l'image est une probabilité un de contenir un chat comme on l'a vu on peut mesurer l'erreur du réseau par le carré de l'écart contre la prédiction et laval attendu dans notre cas et pour cette image le réseau de neurones a donc une pénalité un mois 0,6 au carré qui est égale à 0,4 au carré est égal à 0,16 ça fait cool ça nous donne la performance du réseau de neurones mais ça ne vous dit pas que faire pour améliorer le réseau de neurones or pour faire de l'apprentissage c'est justement les modifications désirable des paramètres du réseau de neurones qu'il nous faut déterminer de façon cruciale le réseau de neurones peut également déterminer à quel point il faudrait que son dernier run s'activent davantage intuitivement plus paix est loin de 1 pour une image de chats plus il est urgent d'augmenter la valeur de paix les matheux parmi vous savent que cette urgence a augmenté p correspond à la dérive et de la pénalité 1 - p au carré par rapport à pec qui va être ici égale à 2 x 1 - b autrement dit de façon plus intuitive ça veut dire que piper est loin de 1 plus il ya urgence à augmenter sa valeur dans notre cap est égal à 0,6 ça nous fait une urgence a augmenté p qui est égale à deux fois 0,4 qui est égal à 0,8 ok on sait l'urgence à modifier l'activation du dernier neurones mais tout ceci ne nous dit rien sur comment modifier les boutons on y vient en effet étant donné la fonction d'activation du neurone le neurone peut alors déterminer comment ce qu'il a reçu devra être modifié pour que le neurone se mettent davantage à s'activer alors techniquement ça correspond quelqu'un que je mets à l'écran mais de façon intuitive si une grande variation positive du signal entrant dans le nord- on cause une grande variation positive du signal sortant sachant qu'il ya une urgence plus 0,8 à modifier le signal sortant alors il y aura une urgence plus 0,8 à modifier le signal entrant si maintenant une petite variation positive du signal entrant cause une grande variation positive du signal sortant alors l'urgence a modifié le signal entrant sera encore plus grande peut-être de l'ordre de + 2 car même une infime variation de ce signal entrant pourrait régler notre problème enfin à l inverse une énorme variation positive du signal entrant ne change pas grand chose au signal sortant alors il n'y aura aucune urgence à modifier au signal rentrant puisque celui ci n'a de toute façon que peu d'effet ce que j'ai décrit la intuitivement est vraiment le coeur du théorème des fonctions composer et je vous renvoie vers cette excellente vidéo de tribus pour affiner votre intuition du problème on a tout ce qu'il faut pour ajuster le premier de nos paramètres on va voir comment ajuster le biais du dernier neurones en effet le neurone sait exactement de quoi est composé son signal entrant on l'a vu il s'agit d'une combinaison linéaire de contribution des synapses pente raide et du biais du noron sache en effet immédiat du biais du neurone sur le signal entrant de neurones peut aisément déduire l'urgence a modifié son billet cette urgence sera en fait égal à l'urgence a modifié le signal entrant venons en maintenant aux synapses pour rappel leur contribution se décompose comme le produit de l'intensité du message qui traverse la synapse par le coefficient inhibiteur ou activateur de la synapse imaginons qu'un message de très grande intensité et traverser la synapse on voit alors que le coefficient activateur de la synapse va jouer un rôle primordial il va donc être urgent d'ajuster en effet sachant que le message d'une très grande intensité toutes légères modifications positives du signal activateur de la synapse va causer une grande variation positive du signal entrons du nord ainsi de façon générale pour toutes synapses activatrice qui a conduit un message de grande intensité il y aura alors urgence à augmenter le coefficient activateur de la synapse n'est une remarque similaire pour tout synapses inhibitrice qui a conduit à un message de grande intensité en diminuant légèrement le coefficient inhibiteurs de cette synapses on enlève une grande contribution ou bi triste ce qui augmente grandement le signal entrant de la synapse comme il ya urgence à augmenter ce signal entrant il y à encore plus urgence à diminuer le coefficient inhibiteur de la synapse voir à rendre la synapse activatrice au lieu d'être finie maîtrise en considérant qu une synapse inhibitrice correspond à un point négatif qui sera d'autant plus négative que la synapse et inhibitrice tout notre discussion revient simplement à dire que si l'intensité d'un message est très grand et si il y a urgence à augmenter le signal entrant alors il ya urgence à augmenter le poids de la synapse à travers laquelle le message est passé al'inverse bien sûr si le message qui a traversé une synapse et d'intensité faible ou quasi nulle alors il n'y aura aucune urgence à modifier le poids de la synapse bref on vient de voir comment modifier les boutons qui correspondent aux biais du arni neurones et aux synapses qui vont vers ce dernier neurones mais le génie de la rétro propagation c'est de faire remonter l'information sur les urgences à modifier les paramètres des neurones et des synapses vers les couches précédente et pour cela il nous faut maintenant étudier l'urgence à modifier non pas les poids des synapses précédent le neurone finale mais l'urgence à modifier l'intensité des messages qui traverse ses synapses considérons une synapse très activatrice on voit alors que l'intensité du message va avoir une énorme influence positive sur le signal entrant du neurone en effet une légère augmentation de l'intensité du message l'a grandement augmenté de signal entrant du neurone mais du coup ça veut dire qu'il y aura une urgence a augmenté de l'activation des neurones à l'origine du message qui a traversé la synapse et bien sûr l inverse si une synapse et très inhibitrice et s'il ya urgence à augmenter l'activation du signal poste synaptique alors il y aura urgence à diminuer l'activation du neurone très synaptique c'est là toute l'astuce de la rétro propagation parce qu'on sait l influence de l'activation d'un neurone sur l'activation des neurones qui le suivent étant donné l'urgence à modifier l'activation des neurones en bout de ligne on peut remonter l'information est déterminé l'urgence à modifier l'activation des neurones des premières couches alors là pour l'avant dernière couche chaque neurone reçoit l'information que d'un neurone de la couche suivante mais bien sûr dans lequel général chaque neurone va recevoir une combinaison de messages des neurones de la couche suivante qui l'informent de l'urgence ou non de diminuer ou augmenter son activation le neurone valeur déterminer comment changer son activation en prenant tout simplement la somme des messages qui lui sont remontés ce faisant chaque neurone se retrouve dans la situation du dernier neurones dont on a déjà parlé étant donné l'urgence qu'il ya à davantage s'activer ou à - s'activer chaque neurone va déterminer l'urgence a modifié son signal entrant et donc à modifier son billet les poids des synapses entrantes et à modifier l'activation des neurones qui le précède alors dernier point un peu technique on vient en fait de calculer le gradient du réseau de neurones vis à vis de sa prédiction pour une image donnée pour calculer le gradient du réseau de neurones vis-à-vis de ses prédictions pour tout un jeu de données il suffit en fait de calculer les gradients vis-à-vis de chacune des données et de prendre ensuite la moyenne de tous et gradient en fait ce qui montre là c'est surtout que les réseaux de neurones se prêtent particulièrement bien non pas la descente de gradient en tant que tel mais plutôt à la descente de gradient it stochastiques bref si j'essaie de résumer un réseau de neurones fonctionne finalement un peu comme une organisation hiérarchique pour fabriquer quelque chose on va avoir des messages qui vont des décideurs aux hommes de terrain mais pour améliorer la fabrication globale il est crucial que les hommes de terrain fasse remonter l'information de ce qui doit être amélioré sur le terrain pour que les décideurs combine ces informations et puissent améliorer le processus de fabrication et la magie des réseaux de neurones c'est de permettre cela entre des entités aussi basiques que des neurones chaque neurone peut même fonctionner indépendamment sans se préoccuper du fonctionnement global du réseau de neurones et néanmoins malgré cette façon très décentralisé de traiter l'information de façon merveilleuse le réseau de neurones dans son entièreté demeure capable de calculer des gradients ce qui revient à déterminer comment ajuster au mieux ces paramètres ce qui permet l'exploration guidée des réglages de sa machinerie et il semble bien que ce soit cette très jolie prouesse qui soit le coeur du succès des réseaux de neurones moderne là enfin on avait parlé d'aller centre de gradient stochastique et on avait notamment mesuré les performances de la machine avec le pkr est où un mot opéré en fonction de mios kpatcha et j'ai un login bergen demande est ce que ce choix et son c est ce qu on pourrait pas tout simplement passés par péan - paie mieux de mettre tous et au carré ça changerait quoi en fait ça faudrait pas à grand-chose fonctionnement en particulier ce changement ne nagerai absolument pas les calculs en tout cas de descente de grézan puisque c'est vraiment la partie en partant de l'apprentissage cca descente de gradient puisque la fonction objectif en faudra avoir la décence de gardiens comme cette façon de guider l'apprentissage est finalement guidé avec pk ray ou p ça change pas énormément de choses j'aurais même tendance à dire que m au caire et en fait c'est légèrement mieux puisque ça permet d'insister un peu plus sur le fait que par exemple peut dire prot a été égalé 0,9 quand il ya une image sans chat c'est vraiment beaucoup beaucoup moins bien que dire 0.5 et le fait de mettre ça au carré permet de faire ressentir le beaucoup beaucoup moins bien d'une certaine manière un autre point qui pourrait être intéressant à signaler c'est que ici on a fait on a des images quand on sait s'il y un chat ou s'il n'y a pas de chat et en pratique on peut imaginer que au lieu de dire s'il ya un chat aussi à pâtir en fait il ya une incertitude nous autres humains qu'on voit une image il est possible que l'image soit elle que cela soit difficile de savoir si un chat ou s'il n'y a pas de choix est plutôt que de dire aux réseaux de nankin bonnes réponses et pégase 0 ou pigalle un lien possible qu' il soit mieux que de dire que la bonne réponse dans certains cas c'est une valeur qui est strictement entre 0 et 1 et ça ce principe est en fait très liée aux générative adversaire networks au haut de gamme qui sont vraiment en ce moment la superstar du machine learning et dont on reparlera dans le rhin futurs épisodes et dans le cas du gan il se trouve qu'en fait utiliser le carré est pas une très bonne idée et on a tendance plutôt utiliser ce qu'on appelle la divergence qu'at elle donc ça j'en parle dans mon livre et on en parlera très brièvement dans un futur épisode de sant forum fument ils nous demandent ne signera pas de meilleures méthodes que la descente de gradient stochastique y parle notamment de la méthode de newton alors la méthode de newton je vais pas expliquer dans les détails ce que c'est ça avoir avec exploite aussi la courbure de la fonction plutôt que juste la pente de la fonction donc la pointe soit arrivée première la courbure s'appelle dérivée seconde également et en utilisant ces deux informations en fait on est capable de démontrer que la descente qu'on obtient la méthode de newton va converger plus vite que la descente de gradient en tout cas dans le cas des fonctions de convection je suis désolé de dire des mots un peu technique tout ce qu'il faut comprendre c'est qu'il existe des méthodes qui après marche au mieux que la descente de gradient mais pour que ces méthodes de marche mieux en fait il est assez indispensable de faire des calculs qui sont supplémentaires et en particulier de faire ses calculs de dérivés secondes qui sont extrêmement difficiles à faire dans un réseau de neurones et en particulier le temps de calcul sera a priori quadratique ans le nombre de paramètres de votre machine car à moins que vous aurez réussi à trouver une machine demain qui soit une alternative aux réseaux neurones dont on peut calculer les dérivés seconde en autant qu'ils aient pas quadratique en nombre de paramètres j'ai bien peur que ces méthodes d'ordre supérieur comme les appelle sont des méthodes qui ne seront pas applicables en pratique parce qu'il faut imaginer que les réseaux de neurones vont avoir ont déjà des milliards de paramètres les plus gros d'entre eux donc si on a un temps des calculs quadratique ça veut dire qu'il faut faire un milliard de milliards d'opérations et un milliard de milliards d'opérations c'est quelque chose qui est absolument pas faisable avec les technologies d'aujourd'hui fin ça prendra un temps vraiment vraiment fou il faudrait voir venir paralléliser calculs et on pourrait pas à faire beaucoup d'itérations de la méthode de luton bref tout ça pour dire qu'en fait la descente de gradient parce qu'elle a un temps de calcul assez rapide comme on l'a vu en gros il suffit que chaque neurone façon groupe quelques une poignée d'opérations qu'en gros le temps de calcul sera linéaire en le nombre de paramètres donc c'est l'ordre du milliard d'opérations et avec des ordinateurs d'aujourd'hui milliards d'opérations à ça ça prend une seconde donc ça c'est tout à fait raisonnable toutes ces remarques font que la descente de gradient sera certainement un studio et parler ce sera très certainement en avis un composant fondamental d intelligence artificielle du futur ce sera sans doute pas le seul mais ce sera sans doute un composant central des intelligences artificielles du futur et ça ça m'amène au petit sondage qui l'avait fait sur twitter vous avez demandé de parier sur la probabilité que le cerveau humain utilise quelque chose comme la descente de gradient toro stochastique est ce qui est assez étonnant de voir c'est que il ya beaucoup de désaccord c'est à dire que on est très loin d'un truc où les gens ne savent pas ils mettent environ 50 % en fait il ya vous n'êtes pas mal être dans l'incertitude avec m entre 10 % et 90 % mais la plupart d'entre vous met des valeurs quand même assez extrêmes et moi j'aurais trouvé ça va tu m'as l'air assez extrêmes également correctement ça met plus de 90% parce qu'il me semble vraiment que le raisonnement de turing à la base est extrêmement convaincant et que la descente de gradient est vraiment le chaînon manquant qui manquait au raisonnement de turing qui permet cette exploration des espaces d'algorithmes à forte complexité de solomonoff ou baru demande quel est le lien entre la forme et la descente de gradient et bien il y en a un et il ya même un article qui est sorti en 2016 qui dont le titre est tout simplement la descente de gradient stochastique en tant que inférences bayésienne approcher une conférence balise argos et à formuler à les appliquer de façon approcher est ce que cet article montre en fait c'est que notamment le fait de ne pas converger le fait de ne pas arriver un minimum global avec la descente de gradient stochastique est une très bonne chose et c'est ça qui fait qu'en fait on exporte plusieurs modèles et qu'on peut prendre ensuite la moyenne de ces modèles pour avoir une élection qui serait beaucoup plus béziers donc la décote décembre de grade notamment stochastique en fait un fondement à ces balises hien donc ça c'est un des aspects qui fait qu'elles sont de gradient paraît très belle et zen un autre aspect si on légalise les geraghty va du rock se trouvant par là dans une prochaine vidéo enfin le proviseur a commenté par rapport à l'hypothèse de riemann qui est un peu le gros truc de la semaine dernière donc là au moment je filme on est mardi donc la preuve de l'hypothèse de riemann a été révélée par michael attiyah hier donc je sais pas trop ce qui s'est passé dans le reste de la semaine mais alors qu'il est j'aurais tendance à parier comme beaucoup de gens d'ailleurs que l'hypothèse de riemann en fait n'a pas été démontrée par à kia pas ce qui me semble assez probable donc dans un des commentaires youtube et je pense vraiment en ce moment là je suis en train de parler à 99% que l'hypothèse de riemann n'est en fait n'a pas été prouvée malheureusement paradis avec qui il ya une erreur quelque part dans son raisonnement donc ce préjugé que j'espère à ses balises est un que j'ai n'ai absolument pas fondée sur la preuve elle même parce que je comprends absolument pas la preuve donc je ne sais pas capable du dew de juger à la preuve en tout cas mardi aujourd'hui il ya assez peu de gens qui ont émis un avis sur la preuve il y en a quelques-uns notamment sur twitter et qui questionne notamment un truc s'appelle les fonctions de todd qui aurait l'air d'être un composant central qui sont un composant central de la preuve d'atiyah et malheureusement ses fonctions le toit de peuvent pourrait être problématique d'après certains mathématicien sur twitter et ce qui est assez bizarre c'est que la manière dont attiyah a introduit ses fonctions il a fait ça sur deux papiers différents de manière assez bancal il a en plus dans le premier de ses papiers sur la structure fine il fait des prédictions sur la valeur de cette constante de structures fines qui sont pas du tout les bonnes valeurs de la physique c'est d'après certaines simulations qui tourne notamment sur edith et sur twitter bref il ya quelque chose qui a alerte pas très clair dans la preuve des tirs donc voilà on est mardi c'est un petit peu embêtant parce que vous voyez ça une semaine plus tard et peut-être que voilà vous regardez cette vidéo on sait que l'hypothèse de riemann n'a pas été prouvée paratilla ou peut-être qu'on sait que rien n'a été prouvé ou qu'elle est juste mais en tout cas en ce moment martèle j'ai tendance à dire que très probablement elle n'est pas juste il reste quand même un petit soupçon pas ce que vont peut-être que les commentateurs sur twitter ce ne sont pas du tout fiable à 100% a priori c'est possible d'argent qu'eux aussi ils font face des erreurs dans leur jugement de la preuve est également qu' il n'est pas bien compris toutes les idées des tirs at il à l'air très sûr de lui donc voilà ça je mettrai pas même un petit pour cent que la preuve de tillard est juste mais ça sent pas très bon elle espère que j'avais aimé cette vidéo la prochaine fois on va étudier un peu les propriétés de la descente de gradient avec une approche un peu plus géométriques et notamment on va se demander si cette descente de gradient permet vraiment d'atteindre global ou peut-être que ça atteint 14 min un blocage voir ce qu'on appelle des points c'est l'on parlera de tout ça d'après une fois ça fera des liens avec la topographie et la montagne si vous avez aimé cet épisode penser à la ligue et à l'augmenter à le partager pensez à vous abonner pour ne pas manquer futurs épisodes merci un petit peur paulo redon et j'espère que vous serez là la prochaine fois et si vous devez apprendre la dérive est la pire chose que vous pourriez faire un sens c'est ne vous reposer que sur une seule interprétation de la dérive et en particulier dans cette vidéo précédente je vous ai donné cette interprétation différente du concept de dérivés et ce qu'il vous reste à faire si non seulement comprendre chacune des interprétations mais surtout de faire le lien entre les différentes interprétations [Musique]