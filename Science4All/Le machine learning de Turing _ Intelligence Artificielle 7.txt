historiquement l'intelligence artificielle était un domaine de recherche qui s'intéressait à la résolution de problèmes par des machines on pensait que pour bien résoudre un problème il fallait écrire le bon code c'est à dire la bonne suite d'instructions à suivre pour les machines une certaine manière on avait une approche assez déontologique pour les machines on leur disait précisément ce qu'elle devait faire et ce qu'elle devait ne pas faire du genre si tu connais la bonne réponse s'affiche à l'écran sinon mais un écran bleu mais au cours de la dernière décennie la roue a tourné aujourd'hui on préfère l'apprentissage par l'exemple on donne aux machines des tas de données et on fait confiance aux machines pour apprendre tout seul ce qu'elle doit faire et ce qu'elles ne doivent pas faire on parle parfois d' approche data driven c'est-à-dire guidée par les données ou encore de machines leur nez et les succès du machine learning sont nombreux et spectaculaires ainsi la quasi-totalité des intelligences artificielles de l'épisode 1 ne sont autres que d'une machine learning à tel point qu'aujourd'hui il semble obsolète que de faire de l'intelligence artificielle sans machine learning mais ce succès retentissant soulève une question d'envergure on est tous d'accord pour dire que le machine learning fonctionne super bien en pratique mais est ce que ça marche en théorie est-ce que sur le papier le machine learning fonctionne en croit certains il s'agit de l'un des plus grands mystères des sciences modernes ces dernières années quant à magazine écrivait que le machine learning fonctionne spectaculairement bien les mathématiciens ne savent pas trop pourquoi le woerthgate précisé que le diplôme ning est en train de créer des systèmes informatiques que ne comprenons pas entièrement singularity aube a ajouté que le monde dépend d'une technologie que personne ne comprend alors que big think confirmé que même les créateurs des ja ne comprennent pas comment les ja complexe marche plus étonnant encore en décembre 2017 and it la plus grande conférence en intelligence artificielle à lire à amis chercheurs à google n'a pas hésité à sortir les gros mots à ce big rob a reculé le machine learning est devenu de l'alchimie at il affirmé voilà qui aura particulièrement énervé yann qu'un l'un des leaders mondiaux en machine learning et s'en est suivi l'un des clashs les plus académique du web bref aujourd'hui il n'est pas scandaleux pour un chercheur en iran que d'affirmer que le succès du machine learning mystérieux ils ont pas lu turing où les clashs les chercheurs ni à son tour blague à part je suis vraiment surpris que si peu de chercheurs en venir mettre en avant les travaux pourtant révolutionnaire tales and turing en 1950 surtout la section 7 de cet article parce que tout y est turing il ya développé un argument théorique très convaincant qui conclut que le machine learning sera nécessairement la clé de l'intelligence artificielle autrement dit dans cet article fabuleux il explique pourquoi sur le papier en théorie le machine learning est voué à fonctionner et ça ça répond tellement bien at-elle mon interrogation autour de la mystérieuse efficacité du machine learning alors je surtout pas vous donner l'impression qu'il a aucun mystère machine learning en a plein et on en parlera mais ce que je prétends c'est que le mystère du succès du machine learning n'en est pas un après tout alan turing l'avait prédit pour des raisons purement théorique dès 1950 et pourtant j'ai beau avoir cherché la seule référence à la section 7 de cet article que j'ai trouvé une note en bas de page de l'article wikipédia anglophone bon au moins dans cette note de bas de page turing est en bonne compagnie très très bonne compagnie mais bon je divague aujourd'hui donc on va voir comment cet article merveilleux qui était très très très en avance sur son temps avait quasiment t montrer plus value et la nécessité d'une machine learning dans la quête d'une intelligence artificielle aujourd'hui vous allez découvrir quelque chose que de nombreux experts on n'y a semble ignorer à savoir la raison théorique pour laquelle le machine learning a fini par surpasser toutes les approches déontologique de l'intelligence artificielle on va voir pourquoi le futur de l'intelligence artificielle c'est le machine leur nez alors je laisse la parole à turing le problème est surtout un problème de programmation les estimations de la capacité de stockage du cerveau varient de 10 puissance 10 à 10 puissance 15 buts d'information je serais surpris si plus de 10 puissance 9 était nécessaire pour jouer de manière satisfaisante au jeu de limitation c'est à dire aux thèses de turin not la capacité de l'encyclopedia britannica 11e édition est 2 fois 10 puissance 9 à mon rythme actuel de travail je produis environ 1000 buts de programmes par jour de sorte que 60 programmeurs travaillant de manière régulière pendant 50 ans pourrait accomplir la tâche en supposant que rien ne finissent à la poubelle les méthodes plus expéditive sont requises tout est dans cette citation absolument visionnaire la clé de l'argument de turing tels que l'intelligence humaine est un code informatique incroyablement longs turing estime que la partie incontournable de ce code fait plusieurs giga octets mais surtout il sous-entend que ce code est incompressible autrement dit turing sous-entend qu'aucun programme de moins d'un gigaoctet ne parviendra à résoudre le test de turing langage moderne de l'informatique théorique on dirait que le test de turing à une complexité de kolmogorov de l'ordre du giga octets en particulier turing va à l'encontre d'un rêve de physicien qui consisterait à réduire toute la complexité de l'univers en une équation d'une seule ligne pour turing il existe des complexités essentiellement incompressibles et l'intelligence humaine en est un exemple voilà qui explique d'ailleurs pourquoi une intelligence artificielle qui serait comparable à l'intelligence humaine ne peut pas être décrite de manière précise et succincte en effet si le code de l'intelligence artificielle est justement incompressibles c'est justement qu'il n'en existe pas de descriptions précises plus courte puisque sinon cette description précise plus courtes pourra être utilisée pour générer l'intelligence artificielle est passé le test de turing on aurait alors créer une intelligence artificielle avec moins d'informations l'incompressible complexité de l'intelligence que turing postule être de l'ordre du gigaoctet signifie que toute description précise de l'intelligence doit nécessairement faire plusieurs milliers de gros bouquin d'ailleurs l'existence de complexité incompressible est sans doute l'un des plus grands mystères de l'univers mais bon là encore je divague mais surtout la conséquence scotia du fait que l'intelligence humaine soit incompressibles c'est que programmer une intelligence comparable à l'intelligence humaine requiert nécessairement beaucoup beaucoup beaucoup de lignes de codes des milliards de lignes de code si l'on en croit tuerie or de façon pragmatique écrire des milliards de lignes de code c'est super long et difficile je ne sais pas si vous vous rendez compte de la longueur d'un tel programme il serait aussi long que tout wikipédia pour écrire un tel code il faudra nécessairement collaborer en équipe pendant des décennies sauf que des décennies de travail en équipe surtout quand l'équipe est grande un conduit nécessairement à des redondances des failles des bugs des incompréhensions des réécritures voir des engueulades des insultes et des abandons bref écrire un milliard de lignes de code en équipe c'est le bordel à titre de comparaison en mathématiques pures la classification des groupes simple fini et justement une sorte de maxime collaboration entre des centaines de mathématiciens de très haut vol qui commença en 1955 et fut achevée en 2004 soit tout un demi siècle le tout pour produire seulement des dizaines de milliers de pages de preuves dès qu'elles sont même probablement ou pré cible a contrario écrire le milliard de lignes de code incompressibles pour résoudre le test de turing sa part est complètement illusoire même avec des milliers d'ingénieurs de premier rang qui collaborerait pendant des décennies on en arrive là à la limite de la preuve déontologique de l'intelligence artificielle il est humainement impossible d'expliciter clairement et sans ambiguïté tout ce qu'il faut faire pour paraître à peu près intelligent comme le dit turing des méthodes plus expéditive sont acquises dans le processus d'essayer d'imiter un esprit adulte humain nous faut bien réfléchir au processus qui l'a amené dans l'état où il est on peut remarquer trois composants à l'état initial de l'esprit disons à la naissance paix l'éducation à laquelle il a été sujet c'est les autres expériences autre que l'éducation auquel il a été sujet lieu d'essayer de produire un programme pour imiter le cerveau adulte pourquoi ne pas essayer d'en produire un qui simule celui de l'enfant s'il était ensuite sujet à une éducation appropriée on obtiendrait alors un cerveau adulte on peut supposer que le cerveau d'un enfant ressemble incarné comme ce que l'on achète aux papetiers plutôt peu de mécanismes et beaucoup de feuilles blanches notre espoir est alors qu'il ya si peu de mécanismes dans le cerveau d'un enfant que celui ci est facile à programmer on a donc divisé notre problème de par le programme enfants si le processus d'éducation ainsi naquit l'idée de machine learning plutôt de leur nid machine dans la terminologie de turing récapitulons le merveilleux arguments de turing turing imagine que le cerveau humain se complexifie avec le temps si c'est le cas ça veut dire que le cerveau d'un enfant est probablement moins complexe que le cerveau d'un adulte turing postule que le cerveau de l'enfant est une sorte de cahiers vierges alors je sens venir les psychologues parmi vous oui en effet aujourd'hui on pense que le cerveau d'un bébé est davantage une machinerie bien huilée qu'une feuille blanche mais l'argument de turing qu'un néanmoins cet argument c'est que le cerveau de l'enfant est incompressible ou du moins turing postule qui n'est peut-être pas si difficile d'écrire un code source avec les mêmes facultés d'apprentissage que le cerveau de l'enfant et par passé difficile j'entends quelque chose de précis à savoir le fait qu'il existe un programme suffisamment court pour être écrit par des ingénieurs en quelques mois disons et qui génère quelque chose de semblable au cerveau de l'enfant avec peut-être quelques connaissances initiales mais aussi et surtout avec la capacité de s'auto complexifier suite à l'apprentissage turing comprend dès lors que le noeud de la recherche en intelligence artificielle ce qui permettra un jour d'obtenir des il ya deux niveaux humain voire surhumain c'est la conception de ce programme initial ou dit autrement de cette learning machine il suffira ensuite de lui fournir un enseignement appropriées pour que cette learning machine s'auto complexifie et atteignent des performances surhumaine et urine prétend que cette approche est voué à marcher elle dispose d'un pouf of concept à savoir le cerveau humain a cassé turing en vient alors à la conclusion que le machine learning est une condition nécessaire et suffisante à la conception par l'humain d'une intelligence artificielle ok c'est peut-être aller un petit peu vite du coup je vais reformuler un peu les arguments de turing notamment parce que c'est un des plus beaux raisonnement que je connaisse la première chose c'est que l'intelligence notamment celles requises pour le test de turing au caire une grande complexité de kolmogorov sans doute de l'ordre du gigaoctet du coup numéro deux il faut donc programmé un très long code informatique incompressibles pour concevoir une intelligence artificielle or 3 il est humainement impossible d'écrire des codes informatiques incompressibles s'ils ont du coup quatre toute méthode capable de générer de tels codes obtiendra des facultés qu'aucun code informatique programmée par des humains ne pourra acquérir or 5 il semble que l'apprentissage permet bel et bien d'obtenir de telle complexité de kolmogorov l'exemple du cerveau humain le montre du coup 6 les learning machines sont indispensables à l'intelligence artificielle en particulier turing à la une réponse toute faite à la question comment expliquer le succès d'une machine learning sa réponse si je la refais formule c'est que seul le machine learning permet l'exploration de l'espace des algorithmes à grande complexité de kolmogorov le machine learning permet donc de faire des choses très utile qu'aucune autre méthode ne saura faire et c'est pour ça qu'en 1950 turing a affirmé qu en théorie le machine learning ça marche la dernière fois on a parlé de l'importance de la logique pour l'intelligence et nixon corriger une erreur que j'ai faite sur le nombre de particules dans un verre d'eau mea culpa mais ça ne prenne ce qu'il ya aussi envoyé vers un lien de wells loeb qui expliquent l'histoire de tartaglia et de ces équations du troisième et du quatrième degré clément patients dont je recommande vivement la vidéo de welch allyn donc la vidéo est en anglais il ya ludovic il devrait être aussi qu'il propose un lien vers un article est écrit par michael launay qui parle aussi de tarte à venir nicolas schmit se demande si notre impossibilité à simuler notre univers en temps réel à l'inter de notre univers fait que les situations à la black mirror sont nécessairement faux pas vraiment la situation n'en est pas du tout invalidé en fait qu'on ne puisse pas simulé l'univers plus rapidement que l'une à l'intérieur de l'univers on peut tout simplement voir des simulations dont une seconde à l'intérieur de l'univers correspond en fait peut-être des milliards d'années dans notre annuaire physique plus généralement il semble que des simulations dans des simulations dans des simulations ça fait perdre un peu de complexité algorithmique et de complexité de kolmogorov typiquement à chaque fois qu'il ya un univers l'inter dans notre univers est bon tout ça un petit peu spéculatif autre chose pour donner des sensations de réalisme on n'est pas obligé de simuler tout l'univers ou de simuler tous les temps de planck on peut peut-être faire des approximations pour que l'univers simuler ressemble à l'univers que l'on connaît bien ayant en particulier l'épisode de noël de black mirror il peut être pas si irréaliste que ça en tout cas j'ai pas l'impression qu'ils soient vraiment incompatibles avec les lois de la physique et la thèse toutes sortes tuerie vous avez est un peu perplexe par rapport à l'irréductibilité algorithmique en effet il ya beaucoup de calculs par exemple le calcul qui propose en descriptions dont on peut en fait trouver des short cut des courts-circuits éviter de faire tous les calculs donc par exemple on avait vu dans une vidéo qu'un +2 plus propre samedi est le premier membre entier naturel est égale à une fois elle pu seulement divisé par deux et ce qu'elle julen fois une puce 1 / 2 est beaucoup plus rapide que la somme de tous les entier on avait parlé aussi dans un défilé alors oui en effet il ya beaucoup de calcul dont on peut trouver des raccourcis de calcul et éviter de faire tout le calcul pour trouver le résultat est ce que l'irréductibilité algorithmique dit c'est que l'ensemble de ces problèmes est en fait un ensemble de cas très particulier et que en général si on point un système qui est tu as une complet et on prend dans des conditions initiales qui sont aléatoires arbitraire en général il sera impossible de trouver des raccourcis de calcul mathieu pose la question de l'importance de l'erreur pour les intelligences artificielles comme pour l'intelligence humaine d'ailleurs je pense que les erreurs sont très très très importantes et elles sont d'ailleurs programmées dans pas mal de d'accueil de machine learning que ça dépend comment on me disait erreur après clair de savoir ce que ce qu'elle est vraiment mais thomas on avait notamment parlé dans la phase d'exploration notamment les algorithmes de colonies de fourmis mais plus généralement il ya ce problème classique de machines en u qui est le dilemme exploration et exploitation que ça correspond en particulier un sous domaine du machine learning qu'on appelle le online machine learning il y aura énormément plus à dire dans bref oui dans une phase d'exploration il faut encourager les erreurs potentielles tengoku nous demandent s'il faut finalement pas voir la logique comme un outil plutôt qu'une fin en soi et je pense en effet en étant assez constructif je suis assez constructiviste comme mathématicien je pense en effet que la logique ne doit être vue comme un outil absolument pas comme une vérité fondamentale mais encore une fois et je vous renvoie notamment vers un épisode de la série sur un fini beaucoup de mathématiciens dit platonicien ne seraient pas d'accord avec moi je dirais qu'il ya quand même quelque chose vérité sous la tente qui est absolument merveilleuses à découvert et tout ça je suis tout à fait d'accord avec le fait que l'univers qui est créé par une certaine logique peut être absolument merveilleux en tant que tel reste que je pense que ces vérités ces mondes idéaux sont complètement déconnectés de la réalité physique et que pour pouvoir faire des trucs intéressants dans la réalité physique et il ya déjà énormément des choses très intéressantes il est plus utile et je pense avoir une approche plus instrumentaliste c'est-à-dire considéré plus la logique comme un outil ben arfa faisait aussi posé le défi impossible et qui consistait à définir le mot structure one that's artistes nous proposent une définition qui mêle un petit peu des idées d'un ensemble qui arrive à être cohérents et que l'on peut décrire d'oppression microscopiques mais bien sûr tous ces maux sont assez imprécis et d'ailleurs ils se rencontrent dans sa définition et sont en fait c'est un poème très difficile c'est un peu le but recherché si vous étiez vraiment de définir tous les mots que vous utilisez dans la définition on en vient vite à la constatation qu'en fait c'est un point très difficile alors du coup il ya des reproches beaucoup plus simpliste qui consiste à dire que une structure c'est juste un ensemble d'éléments qui peut être intéressant entre eux mais si on y réfléchit on sent moins quand même qu'il ya des ensembles de trucs dans le jeu de la vie qu'on n'a pas forcément envie d'appeler des structures parce que c'est tellement le bordel que l'impression qu'il ya pas de structure une idée de koenig et de serge poulin c'est le fait que la structure doit forcément être usé d'une certaine manière derrière leur définition une certaine idée de connexité mais si on regarde pas dans des canons à planeur ce compte que c'est pas des structures qui sont en tout cas connexes au sens mathématique d'ailleurs que d'une certaine manière il y a un trou entre deux parties de la structure et mais quand même envie de dire que même si la structure est composée de plusieurs composantes connect l envie de dire que ça reste quand même une structure en tant que tel globalement on en vient à l'elysée de bach aux pierres swing et fier 0,34 parmi d'autres du pas mal de commentaires du coup j'ai pas pu faire des listes exhaustives mais lydia chaque fois dans ses commentaires ses deux d'identifier une sous région de l'espace qui a l'air d'une certaine manière assez indépendant du reste de l'espace qui d'une certaine manière c'est la séparation entre cet objet interne et le reste de l'univers qui fait qu'on peut appeler c'est au gré interne une structure alors c'est vraiment pas mal comme des d'ailleurs il ya des chercheurs en biologie notamment carl rinsch stone qui essaie d'identifier le vivent comme étant quelque chose qu'on est capable de séparer à travers notamment par exemple une membrane cellulaire du monde extérieur il ya même une formalisation mathématiques un peu plus rigoureuse et tout ça il consiste à raisonner en termes de couverture de markov mais là encore cette définition on peut un peu la critique et en disant bas il ya quand même des relations avec le monde extérieur même si elles sont peut être moindre que pour une région de l'espace prises au hasard et là encore il ya des trucs qui sont un peu n'importe quoi action assez séparables du monde extérieur mais qu'on n'a peut-être pas forcément envie d'appeler structure en tout cas ce que je trouve amusant dans ma propre façon de réfléchir c'est qu'à chaque fois la manière dont j'essaie de parler de vos définitions c'est à travers d'exemples et si on y réfléchit on se rend compte que la manière dont on a appris ce qu'était une structure même instinctivement en fait c'est uniquement par exemple puisqu'on n'a jamais lu de définition formelle de la structure et son cgem il y obtient ça c'est la définition formelle du mot structure on à un ferrer à chaque fois ce qui était que la notion de structures en voyant beaucoup d'exemples et du coup ça me permet de faire le lien avec la vidéo que vous venez de voir puisque d'après turing la meilleure façon d'apprendre à une assurance artificiers à ce qu'est qu'une structure ou un chat ou un chien ou ce que vous voulez c'est en raisonnant uniquement à partir d'exemples et en inférant à partir de ces exemples quelque chose qui ensuite correspond aux nombreux exemples de chiens de chats ou structures que l'on a vu visionnaire et 7,7 nous fait d'ailleurs la remarque que pour la définition d'un truc aussi simple qu'un tas alors en fait on a beaucoup de problèmes ils avaient un accent de vidéos de dirty baloji sur le problème de ces finitions puisque formant imposer une frontière une limite mais quand on définir le nombre n de grain de sable pour que lorsqu'on a moins de haine grain de sable n'a pas de ta alors lorsqu'on a bu de n1 tard briand ne sera pas d'accord sur la valeur de m du coup la définition même de ce qu'est un tas est assez floue ça n'empêche pas le concept est d'être utile en pratique alors je parle bien sûr de tapas que c'est un truc un exemple assez simple à ces visuels mais surtout qu'ils ne veuillent pas lui kahnisme en nous mais dès qu'il faut définir des trucs plus sophistiqués comme qu'est ce que la démocratie forcément ça devient vraiment vraiment beaucoup plus compliqué d'ailleurs comme le propose au chili tenda il ya un exercice de style de très intéressant qui consistait à partir d'une phrase assez banal comme le chat - chez la souris ou dans l'autre cas va partir de la phrase c'est une structure qui avance et remplacer les mots de cette phrase par leur définition dans le dictionnaire donc j'ai fait ça avec la phrase c'est une structure qui avance et juste avec le mot structure et une rotation de nos structures qu'après une itération ça me donne c'est une disposition des parties d'un tout constituant une sorte d' architecture de construction qui avancent donc cette définition l'impression qu'elle clarifie absolument rien voir qu elle complexifie encore plus le truc et c'est encore plus le cas si on applique deuxième itération en passant chacun des mots de la définition du mot structure par leur définition dans le dictionnaire donc j'ai utilisé le wiktionnaire et à chaque fois j'ai choisi la définition dans le dictionnaire qui me semblait la plus pertinente mais au bout de deux itérations ça me bloque déjà c'est une manière dont une chose est mise dans l'ordre la plus convenable des portions de ceux qui corresponde d'intégrité dont il refait la totalité d'une chose considéré par rapport au nombre et l'étendue ou à l'intensité de l'énergie constituant une sorte de façon dont une chose est ordonnée d'assemblage de disposition des matériaux des diverses parties d'un édifice qui avance tout ça pour dire que les définitions du dictionnaire en fait pas du tout rigoureuse et elle clarifie pas du tout les choses parce qu'elle soit ce réflexe de se dire bon bah on connaît pas la définition amour est ce qu'on va la trouver on va chercher dans le dictionnaire en fait si on regarde la définition dictionnaire elle renvoie tellement d'autres mots qui renvoie a tellement d'autres mots ayant ici on déplie un peu le schmilblick on obtient un truc complètement incompréhensible plus généralement tout ceci vient en fait du point des définitions circulaire ils vous envoient vers l'excellence vidéo de monsieur fille sur les actions de khalid qui explique très bien tout ça en fait on l'a vu notamment dans la série sur l'infini lorsqu'on essaie de bien poser des définitions comme ça en fait ce qui est intéressant c'est une relation entre les mots sauf que si on essaie de construire ses relations entre ces mots j'invite des trucs complètement incompréhensible du coup la construction du langage naturel à partir de règles formelles c'est juste un truc qui n'est pas faisable humainement et en fait on pourrait imaginer que la seule façon d'avoir des définitions rigoureuse qui parle vraiment de quelques trucs fondamentaux comme des 0 et des 1 et qui permet ensuite de construire ils ont une fonction qui à chaque fois qu'on lui montre une photo de chadi à sacha notre photo je vais a dit ça c'est pas un chat la seule façon de construire cette fonction là pour que cette fonction soit défini de façon non ambiguë est grosse même s'ils avaient pas forcément parfaite parfois ça secoue cette fonction elle va dire qu'un truc et recharge à rien de construire cette fonction est absolument difficile et sans doute même que le code source le plus court de cette fonction-là fait forcément peut-être pas des milliards mais peut-être des milliers voire des millions de lignes de code un truc qui ne serait pas du tout lisible par l'humain et qui sera encore plus dur à écrire pour un humain au final de façon pragmatique peut-être la meilleure définition de ce qu'est une structure c'est celle que oppose c est moi et corrigée en gros une structure au final ce que c'est vraiment et bien une structure c'est un truc intéressant on n'a quasiment rien dit quand on dit ça puisqu'il faut encore définir ce qu'est un intéressant truc c'est juste ma chasse et du sein bejune cesu se faire référence à quelque chose et si ce quelque chose d intéressant on peut peut-être appeler ça une structure la grosse difficulté bien sûr c'est de définir le mot intéressante j'ai envie de dire que de façon pragmatique ce qui est intéressant c'est ce qui permet de décrire ou de faire des prédictions autrement dit si j'ai d'abord un petit peu plus cette définition je ne dirais qu'une structure c'est un composant important d'un modèle voilà je pense que de façon pragmatique c'est difficile finalement de faire bien mieux que ça mais moi je suis sûr qu'il ya beaucoup de gens qui sont pas d'accord le cas des fils de cuivre de données et j'espère que vous avez aimé cet épisode qui conclut un peu le début de cette série qui parlait pas mal l'algorithmique et de concept derrière le calcul qui je pense sont vraiment indispensables pour comprendre le succès du match énorme et on a tendance à dire que machine learning c'est juste trouver des corrélations dans les gommer et si on pense que la machine learning c'est ça les voiliers à être surpris par le succès demain ce qui est amusant c'est que tu as une approche complètement différente de celle horning machines' encore de passer le premier à introduire cette idée de machinerie pour turing ça semble être davantage une exploration de l'espace des algorithmes à forte complexité qu'on peut pas exploré autrement autrement dit à la tuerie on peut voir le machine learning comme étant une automatisation de l'écriture de code et si on voit le machine learning en tant que tel je pense que le succès du mât chinois est beaucoup moins surprenant plus généralement cette élection 7 notamment de l'article de turing 2050 est absolument fabuleux je pense qu'il introduit pas mal d'idées qui serait utile plus tard notamment par exemple les algorithmes génétiques ont le truc c'est que c'est un peu un article très vulgariser qui s'adresse pas forcément des informaticiens du coup y'a pas plein de formalisation mathématiques et tout ça mais les quantités sont déjà dans cet article pour tendre très très prévenants allez le lire j'en profite aussi pour mentionner rapidement rei solomonoff qui est pour moi le grand grand grand génie de l'intelligence artificielle d une machine roues et qui je pense que devrait être beaucoup beaucoup beaucoup beaucoup plus connus s'il ne l'est malheureusement en gros je pense que soldat fait simplement résolu le premier machine mim avec notre pays viennent malheureusement dans cette série je ne parlais pas d' approche parisienne du coup je ne parlerai pas de ça j'ai vraiment sur le ménage dans la suite des idées c'est vraiment la suite naturelle des idées de turing donc on peut voir un peu seul au sommet comme la confession des idées et des tueries enfin pour ceux qui veulent en savoir plus son corps se retiennent yad vashem des signes d'une banane a sorti une vidéo où il lit un texte une tuerie a lu sur la radio bbc dans les années 50 et c'est un document absolument merveilleux qui est tellement le comprendre plus les débats qui avaient à l'époque et pourquoi est ce culturel est incroyablement en avance sur son temps vraiment turing crg ni la semaine prochaine et pour les prochaines dizaines de vidéos quelque chose comme ça on va parler de machine learning théorique enseignée aujourd'hui tels que c'est enseigner aujourd'hui qu'ils sont très très proches du coût des notions tactiques pas mal parlé aussi des deux statistiques leur footing et tout ça et en particulier la semaine prochaine on va parler de l'importance de la mémoire et on verra notamment que le mémoire ne suffit pas grave on pourrait croire naïvement être intelligent bien comprendre le monde ce n'est pas mémoriser des faits et des connaissances si vous avez aimé cet épisode penser à le laïcat le comique avait partagé par ça vous y revenez pour présenter les futurs épisodes merci aux supporteurs dont il espère qu'il serait là la prochaine fois faites il ya un autre chercheur qui selon mme nov qui est l'autre grand gagnant de l'histoire côté de la classe cc solide nov l'idée de seulement 9 c'est d'imaginer toutes les probabilités calculable possible woodside