excuse not only taste to do so my school scripts very solidly you know I'll just like talk loudly and depend on people okay welcome to this talk thanks for coming so this is organized by 50 vote wisdom Lausanne we talk more about this later on and it's for the launch of the book which I forgot to bring which is very good sales person but the book we've written media and I about the Fabri endeavor how to make AI sorbitol official goes to presenter MIDI quickly he's just means his PhD on learning the postdoc at MIT he also wrote a first book called the equation of knowledge on biology and it's my pleasure to contribute to this project with him this time so we we work on AI safety this sentence when we say this friends family people in the people in the public transportations this is this is a typical reaction we get oh you are oh you're worried about far future concern this is important with this it's still it's still long-term concerns and people think we care like so people when they hear that that's our concern is own AI safety typically they imagine that we care for something like them later or Skynet actually what we care about is something that is more like this problem than that problem so what we will try to convey in this talk at the first part of this talk is about the urgency when we say that we should work on AI safety on ethic and ethics we're not talking about a far-fetched or speculative long term futures we're talking about dishes decision-making decision-making algorithms so tools that process information and take decision and recommend cancer content to billions of users on a daily basis and that can have consequences such as fuel in the anti-vaccine propaganda or climate change denial ISM or hate speech so this is what we care about and when we say eh I already said like don't think about the super intelligent conscious being think about an information processing to so an algorithm that processes information takes inputs and produces some outputs that is also information so anything that processes information and also the the concerns we will raise in this talk appear mostly with modern a eyes which are based on data which learned from data and are not fully programmed by hand and can be audited by hand by by programmers so when we say AI according to what I just mentioned this is not what we mean by AI as of today as of today if you see a robot in a in a commercial exposition there's probably not much of AI in it but we mean something like the YouTube recommender system so for those who are not aware of the scale of this kind of of of basic but still very powerful AI today there are reportedly more views on YouTube then searches on Google in one minute there is about 4.6 million or more views on YouTube and only 4.3 or 3.4 sorry 3.3 point something million Google searches and so so so this is something that processes information makes decision influences people choices on a daily basis at scale according to this chief product officer of YouTube the CEO of YouTube in a recent interview 70 percent of these views are recommended by the algorithm so our are made because the algorithm decided the person will watch this and that and not and not active search so it might search for latest EPFL new use or latest science for all like say science for all game theory video and then you will watch this but for each video you watch actively you would probably watch three passively so to say because the algorithm took the decision so this is what we care about decision making algorithms that process data and have a scale effect so just a few questions to see if you got the point so as of today is this AI if you see something like that in a supermarket yeah this is probably there is not much AI in it okay easy question yes what about this one so yeah you have some Andrew it with the face just like Sofia again yeah not AI excellence so this is this is according to what we we said not because there is this stupid robots but because there is a Netflix logo and Netflix has a very powerful algorithm this processes information look us your behavior and tries to recommend content or keep you engaged with the platform so so this is this is what we will focus a lot on in this talk and by this we try to convey the urgency already already deployed guys they can be very dumb they can be stupid they are not conscious but they are powerful ok so yeah this is the kind of decisions they make they will try to predict whether you clicked on this video of lei or whether you don't click and click on this one that is very hardcore mathematics or not whether or how much would you watch would you stay up to the end of this video or would you leave after watching just a few seconds and then and and and then leave the platform and their goal is to try to engage you and keep you keep you engaged with the contents and of course predict predict what contents will engage you and to do so they are not programmed by hand because there are lots of things to look at how do you scroll your mouse who are your friends etc so if you did a few a bit of computer science you know about programming programming is writing code writing instructions if this else process this if you see that a is different than B suggest this etc so you can think of a program as a very very large set of tuners that you want to you want to find you and in the 1950 in 93 - you one right 15:51 Alan Turing realized that if we want to have as much in 1950 not 51 in 1950 Alan Turing realized that if we want to fine tune a large program to do intelligent behavior we would end up with billions of buttons and if you want to write that by hand it's one scale and very early on he realized the way to go is to learn what are the good parameters that match input output for example video prediction of what whether you would stay or not so that I would predict what kind of video would make you stay or what kind of product would make you to order etc so and to do so very very very quickly we look at pairs of inputs and outputs we have we compute the error made by our setting and we try to minimize the error so that we predict the output we predict the match in in a in a better way in the future and this is done with an algorithm called gradient descent so it's just optimization optimizing optimizing the parameters so that we can predict the output given the input using so minimizing some error and by minimizing this error were almost joining so another framework in machine learning is what we call reinforcement learning which is let's let's just let the algorithm do random actions the parameters that do a correct behavior are reinforced or kept the parameters the wrong behavior are not yet are not reinforced so we try to maximize some summary so we try to like have a reward that the Train forces good behavior this framework is called reinforcement learning and at the and of this framework there is there is an important notion of reward which is not very far so this is very high level this is not a technical introduction but this it's not very far this this reward this carrot is not very far from the error you want to minimize in the preview setting you have inputs outputs you want to minimize the error here you have behaviors and you want to maximize the rewards so you want to do the behaviors that maximize the reward okay for those who didn't do computer science think of a child learning language the child is not programmed with instructions of language they would probably go to to a lady and say hi uncle and then the lady was say I'm not your uncle so this is a negative reward and then maybe if they say hi aunt is happy give the child a candy the child will reinforce match in the face of female to say in aunt and match in the face of male should not say so first to say in a female to not saying uncle so there is something to program to program algorithms we use this we use this analogy and it is important to understand that there is not much difference when we take when we talk about safety the only thing you have to keep in mind that there is some objective okay minimize error maximize reward maximize what time engagements maximize the number of products that are ordered on Amazon etc okay so there is some objective and so that that should be reached so maximize something and then we might we might have a conversation so this is a good way to have intelligent machines or intelligent algorithms we don't need to program them by hand but what do we give them as a metric as an objective a profit seems to be a reasonable thing to maximize if you want to survive as a company it's very fair it's completely reasonable to try to maximize your profit or at least maximize profit within the law okay within what is acceptable and legal in the country or operate so there is nothing a priori that is not but in maximizing profit or as Lee likes to say if you ask people 10 years ago using basic platforms do you want do you want to be recommended content that that that is interested for for you they will probably say yeah I don't want to have content that it not fits my interests so if we try to have a proxy like clicks or watch as an objective so it seems it seems a priori reasonable to make to make the algorithm relevant so at the end of the day something like YouTube like 10 years ago I was asked do you want to work on our algorithm that just want to entertain people I would say probably yes that sounds not only group but even like beneficial like me proposing people giving to people what they want after all and you may ask then what can go wrong so maybe you want to do the clicky so what can be like angle what is so bad about doing this sort of maximization and it turns out that it does have some bad implications and i think thats bad is not the objective of YouTube like entertaining people doesn't seem that bad but the big problem is the side effects now whenever you have an influential entity it can be a company can be an art agency can be a human whenever you have something that is deployed at massive scale whatever it is optimizing for this is going to have side effects because whenever you are interacting with billions of people at the same time there's going to be some weird synergies going on and things that you cannot predict easily now this way this you can take the example of the great hawk who has seen this the great hug it's a documentary on Netflix I recommend it it's it's very interesting so it's about the kimbridge analytic Facebook scandal so like the brief story the very birth story is that essentially there was this company called completion Attica it retrieve data from users on Facebook and use that use this data to do targeted political campaigns especially during the the presidential election in the US and what's interesting is that all along this movie like you feel that the journalists they argue that there's something bad going on but it's not very clear what is so bad about what was going on like we clear this impression but arguably not different reasons why we we find something wrong about what happened one of the the reason is the privacy is privacy like the privacy of the data like many people argued that while people consented said yes to sharing the data on Facebook they didn't realize that they were saying yes because we get this notification all the time and they ask for us for agreement on sharing some kind of things and most of us actually kissed me but most of us don't read the full details I don't think anybody like essentially nobody does and the details were in this agreement so in a sense there was nothing really illegal so there are like like a contagion like there are like debatable points here but essentially like the way the data was shared with arguably not that illegal the problem is more that people were not concerned enough about these things but from a legal point of view it's not clear what was wrong the other Harlem which is quite different actually from the first one is the fact that thanks to these tools you could now have massive targeting of political campaigns and this is really new like if you think about it for a long time when we give things at scale we had do industrialization we had to do some uniformization of avoided that we were planning to sell or to to propagate as to spread as a message but thanks to artificial intelligences you can now do targeted you can really target and and adapt customize your message to different audiences agencies and this was really criticized by the documentaries but again if you think about it it's not that illegal like after all like doing poor ritual campaigns is regarded by many as the basis of democracy so it's not very clear what was wrong again there are borderline cases but although it was not that illegal however if you watched the documentary you do feel that something is wrong even though it's not necessarily wrong legally there's something that seems wrong ethically but perhaps the most important takeaway out of this of this documentary perhaps even more important than the trend that I've mentioned is the fact that all this was not anticipated at all like all of this seems like a surprise to most people even to Facebook Facebook employees and that really the difficult part of the difficult thing about making but thinking about these artificial intelligence these eyes is that it's something quite different from what used to be is very hard to anticipate the consequences of the deployment of these hey eyes on very massive scales and that's what are you the moment take away we should have this kind of documentary en and events is that these things are already happening and they have been happening for quite a few years now but we were not prepared and we are still not prepared our societies are not prepared for this kind of things legally we don't have anything that adapted to this kind of of society and also like the way we think about the different issues is not adapted to the way the world actually is it's very hard it's very very very hard to do to foresee all of these because we have limited brains and we used your limited surrounding and also like we have this very like we care but what's today but the thing about AI is is that they are constantly progressing there's a lot of research going on in AI and there's a lot of new breakthroughs coming up every year in AI so it's not sufficient to think about a eyes of today it's actually important to try to project ourselves into the future and you wonder what can what these areas can be in the future what can be impact and how should we adapt to a world that we don't know that much about so this is one of the things that's very difficult now this way straight one other side effect I think the word side effect is really critical here like again it's not the intention of the eyes that's problematic is the side effects that maximizing this objective function has when you deploy these areas on massive scale perhaps one of the most scary things the perhaps the scariest graft graph out there is perhaps this one is a graph of the increase while the projection in the u.s. you have these two big parties like the Democrats and the Republicans and you see that overt and the becoming more and more polarized the demo the Democrats are becoming more and more Democrats on all points and the Republicans are become more and more Republicans on on all points there are things that are even perhaps more worrying than these like the fact that the Democrats and the Republicans more nor hate each other like each side like a half of each side see sees the other side as a threat to the future of the US this is very puzzling and concerning and also that's a big so-called perception gap meaning that the Democrats have a very very exaggerated and biased view of the Republicans and vice-versa and this is not really good for the primes of coordination and I'm just like thinking correctly about these different issues for the future of mankind another worrying side effect that he's really rigid to this is the prime of addiction so if you think about this this is like it's even bit of a stretch to call it a side-effect because after all if a is our hair to get to get our attention that's often what they're maximizing for well you should expect addiction to be the the natural outcome of things trying to maximize addiction attention now I want to talk a little bit about attention because I think it's very very big deal if you think about it more and more like we often say that the power arm is I often hear that the problem with capitalism is companies maximizing for for money or something like this turns out that more and more the currency especially for for big companies on the internet also like startups everything else on the internet the big currency is both becoming more and more attention rather than money if you think about this many startups what they're going to try to do first is to get a large share of the market a huge market share first that's grabbing your attention and then they think maybe about having a business model but sometimes they don't even get to think about the business model if you think of Instagram for instance these guys were really optimizing for attention above all so the real currency there the thing that a is already optimizing for and many companies optimizing for is actually attention they're not the only one like so many people out there are trying to get attention I'm talking about politicians for instance well any political movements in general physicians organizations try to grab attention charities wants your attention for you to give money to charities and media's want your attention and of course the guys like me youtubers I want your attention we all want your attention and there's a big competition the big competition today is about grabbing attention and guys are doing this very well and if you think about what to do when you're trying to maximize for attention it turns out that creating diction is really really good strategy and so if you think of tinder for instance well Tinder's business is supposedly to match people together but the word I actually won't get more profits and more market share is by maximizing attention they want people to be using tinder over and over again so we attention is very big year and it has big boring consequences so there has been a study funston's I showed that it was back in 2004 the the median attention span of people working in a workplace was three minutes so every three minutes people switch tasks they could not focus more morphine or not now few minutes is not a lot of time like you don't have a lot you cannot do a lot within three minutes but that was three minutes turns out that these days this attention span has gone down to 40 seconds so now the the median attention span according to a study is more like 40 seconds and much of the interruption of the the attention actually half of it is not actually due to notifications on the phone though this is definitely a big part of it but actually half of the time is people self interrupting themselves why because we just got used to this and this is a big issue because if you think of the big problems that face humanity a constant climate change of making s were basically beneficial these are very very complicated problems they cannot be solved within 40 seconds they require a lot a lot of attention and unfortunately these AIS are not maximizing for our ability to focus in fact their maximization has as a side effect or inability to focus and this is very worrying for the big challenges that face us now another problem that maybe raised raised is the problem of information so here's for instance a question that was asked by a guy named Hans Rosling so the question is how did the number of deaths so to total number nor the fraction the total number of deaths per year from natural disasters change in the last century did it more than doubled did it remain more or less the same or did it decrease less than half so who thinks is answer a hey B C do you think people answered so yeah so hence first thing did this Swedish people was turns out that Swedes are not very good at this like half of them thought answer eh so the right answer is until you see the only 12% of them now he also asked this question to a audience so Ted if you don't know about this like well I guess you know about the TED conference but if you don't know about the people who returned in these conferences so it's an easy steady not TEDx the the main difference is that Ted is much more expensive if you want to get in there you have to pay a lot of money so you have to imagine that the audience of Ted is like which which well-informed people it's very like most informed and most and also it's a bunch of people who really care about the future of money here whatever and so if you are and so he has this question should the public of of Ted and as you can see like the public of Ted was really not that good so joke as hands walking like super adult is that they did worse than chimpanzees that the would answer randomly and and I think this really shows that the prime of misinformation is not a problem for like very stupid people like we tend to think that somehow the quality of information that someone has is correlated to how intelligent or how successful he is and many cases this is the case but for some question it's not a surrender case and you can see here that actually here the public of Ted is misinformed it's not that it's ignorant is that it is wrong it is more wrong than chimpanzees which I'm told don't with the news and this is not specific to public of Ted asked this question to the media for instance and within the media's also there's a huge amount of errors on quite critical questions like this seems like to be that the kind of questions you would need to have a good answer for should you know mean when you say I know about the world this is the kind of questions you should probably know something about if you want to know about the world now I've talked about many problems with current areas like current areas are not maximizing for the quality of information like like for instance there's a study that came out that showed that the climate the pro climate change videos had roughly the same number of views we should take the top 200 videos when you research climate change on YouTube you have essentially the same number of views for pro climate change as for climate skeptics so 16 million views on each side in balance so clearly the quality of information is not promoted by these by these AIS and if you care about climate change or maybe you care about social justice or me like whatever it is that you care about it turns out that information is key and in particular the quality of information is really critical but that's not what a eyes are promoting now I've been talking about bad things about bad side of side effects of a eyes but there are also opportunities and the opportunities that were probably in unimaginable before the advent of a eyes the corner of them is the possibility to treat mental disease or mental troubles so today like one of the worst thing in terms of happiness in the world is probably all sorts of depression and loneliness even like suicide and it's very hard to treat them because it's very hard to get the data about who's feeling lonely who's being depressed this is a very hard disease to cure but turns out that using these these AIS using the activities of people on YouTube Facebook Twitter or whatever or Instagram you can actually infer a lot from them so they had already been studies once in some phase on Facebook that showed correlation between the way people scroll on their phones when using Facebook and reported depression so you can actually do massive scale diagnosis using this a is if these areas were met beneficial and this is a massive opportunity not only can you do diagnosis but you can actually also do treatment because a lot of these disease all the information people are exposed to and instead of promoting another enraging video another angry video and other depressing video these these areas could actually suggest therapeutic videos or even videos that recommend the user to go and see a doctor and I think this is a huge opportunity that should be seized like it's about the well-being of hundreds of yawns of people now perhaps the greatest of my worries going back to read the words is the fact that these areas are often promoting anger and there's available reason for this is that anger is extremely addictive it's also extremely engaging it's been shown that the thing that people will share the most easily is angry videos people like to be angry and people when especially when people are already angry like when we angry like we tend to want to be in angular we're trying to explain why on Greeners and choose show to prove why the anger that we have is justified and should be promoted and should be share and this has a huge spread of anger on social medias which is really really not good for the abilities of people to think correctly about difficult and complicated issues and their ability to coordinate on important challenges now how to I show you that with there's a lot of risks there's like a lot of very bad side effects already but there are also huge opportunity using these as you could reduce this kind of huge but you are still able to organize and it took us a long time before inventing writing writing is the first medium that is external in information externalize our non information processing information carrier I can write a message I can give it you can come back and read it you can give it to someone you can copy it and as soon as this information is outsourced our journey took off really when we started outsourcing information from from our body so it's important to go back that far in history to understand what's happening what we are facing so again what million years ago no language so human is a few thousand years ago no right we can share information with other [Music] information we discovered information processing the processing information we can program the machine to do that okay and then a few days ago we realized we can automate of programming the universal information processing so if you think about it even organizations like effective altruism that wants to do good Indian people who work for thank you in 250 volt RISM what they're doing is a century data processing information processing my job is information processing I'm just reading these articles archived and I'm making videos out of them his job is information processing like he's reading all the things and writing papers so a lot of things is information and information is critical if you want to do good you need better information both both data and better information processing so you've all know our high when he came to into EPFL point he said sometimes those who control the the flow of data in the world to control the future not only of humanity but also perhaps of life itself and so I think it's a very good point and it begs the question like who today is controlling the flow of data who is in control of where the data is collected and where it goes well I would argue that today's entity that does this the most that controls the flow of data is already neither human nor cooperation it's already an AI like the youtubes AI is controlling the flow of data in all societies arguably more than any other entity and so carry DCI is very important and just by doing this by doing better information processing you have huge amounts of opportunities and in many different fields so like we talked already about health when talk about mental issues but essentially all of Health is about information you want to do diagnosis you want to do treatment and this all boils down to to finding more about the problems the reasons of the disease the diagnosis and so on well education is of course trivially a primer of information and sometimes not that trivially I mean a big part of good education you knowing when to recommend what book to what person and this is very very hard we need to know about the thought process of the student so again any information processing is critical and there are other applications but I won't go too much into details but a lot of the energy problem is about information especially if you think about here's the case of fusion energy for instance where the control of the plasma is really information processing power and recent papers in deep learning about this and also if you want to do good generally in societies and help the people in need well you better know what all their needs and there's a big information foreman regarding this day's documentary about Bill Gates discussing these kinds of issues which are highly recommend so I hope that by this point we've more or less convinced you that it's become urgent to make a is like the YouTube recommender system reversely beneficial so perhaps we did not insist enough on the world robust lately but you don't want a solution that only works half of the time and we're going to talk more about robustly in in the sequel as we're going to defend the second series which that unfortunately it's also extremely hard like making sure that these areas like the YouTube recommender system behave in Rober significant manner it's extremely hard it's a really challenging research Harlan and it's not sufficient to just ask these companies to do good you also need to construct brand new tools to make sure that these [Music] for example the video on the previous one so you can imagine like this machine learning algorithm is trying to detect pedophilia in maybe 99% right and that's not sufficient because there's still like the people who wants to upload a video can still find gaps in the algorithm and trick it into thinking that there's no better fear in this video so this video big part now you might think that you to prevent this problem what you should do is to do continuous learning like if you just learn one one algorithm and you deploy it well if he has one gap then everybody can exploit it and that's bad so maybe by continuous learning you can avoid this well department when you're doing continuous learning typically you're going to learn from the data that you that is provided to you and so I don't know if you've heard of this page width but in Sarnia that basically learned from data from Twitter and I don't know if you know but Twitter but on Twitter has so this is the yeah so there are many many other changes you want to make robustly beneficial it's going to mention one more but the book many many more perhaps the most important one is what what does it mean to be beneficial it's a very very difficult now what people usually do is more philosophers like to check these very abstract anima which have been made more a bit less abstract by the advent of the future grant I guess of autonomous cars so this is the trolley problem I guess at Roy point for autonomous cars so you have this car and it's going to cannot stop and it has to choose between killing four people or killing or pets but one of the people is a is a robber so I don't know how the car knows it but and so this is the classical dilemma and when this case maybe mostly but in many cases we strongly disagree and this is a problem but the trick here was to instead of having this approach were moral philosophers tell us what to do the MIT quickly the website called a Moore machine where they ask these dilemmas to different users well in a user and thereby they collected a lot of data like millions of data points of people saying what I prefer this situation to this situation and it generated a lot of different gary mass now this all sounds good but if you want to actually implement it in a car there are further challenges so like for instance you can ask which answer am I going to follow there are millions of answers which am I going to follow now the idea of the bunch of people from MIT and Carnegie Mellon University is that instead of listening to any individual why not having a vote about this so they a church you have a vote on the different users a century the preferences of the different users but there are other problems that concerns the number of imagine more dilemmas the number of dilemmas that the car may face is extremely huge you can think of all the possible combinations it's an exponential number so it's it's huge clearly we don't have the data points like the preferences of every individual about every possible dilemma so we need to do extra operation we need take what people said and extrapolate and imagine what they would probably say with four different for other dilemmas and another point is that the people's preferences are not necessarily consistent like there's a lot of research that shows that if you ask someone a question then you ask depending on how you ask the question his answer may be different so that's a lot of challenges but essentially they're also interesting harms like the car needs to do all of these functions in milliseconds because you don't want what the car has to make a choice in milliseconds so plenty of technical changes regarding around ease now this was for the trolley problem which is a nice poem but it's very abstract and poly not that impactful poem because they're not going to be that many try dilemmas for autonomous cars however there is an area that's constantly faced with ethical dilemmas you think about it if you think about this any recommendation of the YouTube algorithm is actually an ethical dilemma so for instance imagine a user that types the word on YouTube and he searches for for Trump the first video that's going to be suggested by the the recommender system is going to have a huge influence and clearly disagree we clearly disagree about what video should be promote in this case of course I've taken the case of Trump but I could take any slightly controversial topic and there's going to be a dilemma similarly like YouTube and Facebook and all had this huge problem of hate speech motivation is this video ahead does this video falling to hate speech it's a big ethical dilemma and should I recommend it this is a big dilemma if I recommend it the user is going to like it he's going to have a nice time but this is going to also have side effects how do you answer all of these ethical dilemmas on videos that have not been uploaded yet like at all videos many videos uploaded on the on YouTube at every like every minute there are 500 was of videos uploaded on YouTube so this is very very hard and what makes it even harder is that we cook we're going to disagree like it's completely hopeless you have a deliberation on every possible recommendation of the YouTube recommender system also because like the weather recommendation is ethical or not depends on the whole context maybe one video is not recommended on this particular day because there were events on this same day or maybe not to this particular user because he has some kind of bug on it's a very very complicated question and what makes it even harder is that we are very bad with Morpheus of in general we have more intuitions that are that full of fallacies like of cognitive biases if we ask someone the same question in different manners we get different answers if we ask the same question to someone whether he's eaten or not he's going to have different answers people are much more severe when they are hungry so like for instance middie when he passed his private defense of his ph.d he gave cookies to the examiners because he knows about this respect so there are plenty of things that are wrong with our moral intuitions and I could go much much further like we already like we like to think to have these ideas we we have these often beliefs that we have this belief that we try to defend even though we don't really have a justification for this like it's very very hard to have a good more and so the proposal that has been made by one guy called years old Kazuki was that instead of taking our preferences as they hope we should actually not do this and instead consider something called coherent extrapolated volition particle the important word is polish so intuitively the volition is what we would want you want as opposed to what we want so just to make this distinction clear the example I always take is the case of video recommendation of a football video and to me and it turns out that I like football so when I get proposed these kind of videos I like I want to click on it but I also know that it's going to waste a bit of my time I'm possibly a lot of my time kind of addicted to this and so when I see this video especially if it's late in the evening or whatever I know I like I want to click on it but I would want not to want to click on it so and that's basically the idea like in many cases we have these preferences that are just addictions and we want these things but we would want not to want these things I would argue it's become critical to make this distinction because programming what we want is already what they are all doing these days if you think about what YouTube is doing is computing what you're going to like literally on Facebook and it turns out that if you only do what people want you have huge side effects so we really shouldn't think be thinking about doing more than this like not just what we want but perhaps something that's more what we would want one and probably ideally if we were smarter if were nicer and we were better informed people actually changed my mind if they get better informed on on important topics and this should be taken into account perhaps ahead of time now all of these combined should lead to the conclusion theory so if I can it's you that so if we've convinced you that it's urgent to make as beneficial and inject and it's extremely hard to do so then then the natural corollary is that it's urgent to put all sorts of talents in the best conditions to make a is wobbly beneficial that's we really the key message of a book like we don't we're not solving the form of making either obviously beneficial to help in a book any conversation on our if you'd also like in terms of putting in the best conditions also means giving people the part that is of interest to them and what I want you do to the Jamaican as beneficial and that's really what we're trying to do in the book like we trying to present the stereo program but also like specific poems that can be tackled sort of independently of the rest so hopefully if you read the book you can find something that's really interesting to you and that's also useful now what to do from this an important one is actually to learn more like it's very like so far a lot of people have been trying to talk all the time from different perspectives but it's never been clear teach teach me how all of these ideas really fit together in a coherent manner that's really what we're trying to do in the book but more generally something that I think what more people should be trying to do it's very easy to come up with forth good ideas especially if you don't have a big picture of you and so that's why we really recommend you to learn more and turns out that there's a new book that is published today to do this in find so far back in English really soon now you also need to engage and discuss I think it's very very important because these ideas you cannot solve the loan loan you need coordination and you need to engage people also to get people to work on this program and it's very difficult to do because on these topics there are lots of people who have not the best ways to debate I'd say like when comes to eyes there's a lot of hype and under and criticism like it's very not very good debating and also we need to coordinate because these are all challenges that are very interdisciplinary and that's actually one of the things that the organization effective altruism is trying to do and just to conclude I think that so hopefully I've convinced you that making it as robust belief here is important it's to be parents but I also want to insist on the fact that these are fabulous I never liked writing this book I really really discovered a lot of things that are fascinating it really questioned what I thought I believed and also it was really thrilling and exciting like it's really really some of the most interesting questions that I know of we came out of thinking about these kinds of forums it's really related to Bayesian ISM as well so there are really a lot of fascinating and it's not boring like that's what I'm trying to say it's a big pond and arguably is the greatest part of mankind like we've I think it's much more important and beautiful and fascinating than going to the moon for instance because because going to the moon did not achieve that much didn't change design much of the world and our way was not that hard like we could do with control few years some people would say this is a very much much harder prime much more changing and it's going to change everything about the world so it's really very exciting and I hope you think about figuring out how you can contribute to this project yes yes well I do yeah so it's very hard to compute we cannot compute all the side effects and actually ahead of time it's a very challenging time it's Indian like can you apply Bayes rule if your vision and the prime means that the prime Bayes rule is not feasible in a reasonable amount of time so there's always going to be uncertainty also you just like the data so clear you cannot print all the side effects but you can try to estimate how sure you are about the possibility of very bad side effects like first of all I think you should think more probabilistically well give you no ambition it's not surprising I say this but I think is very important and it's been recognized more know as a very important part of of artificial ingredients and if you combine like a more probabilistic thinking about possible side effects and an estimate of how bad the side effects are using like some sort of objective function then you can try to avoid like very bad side effects there is the question here in the YouTube stream I'm going to a lot of people make this clear distinction between let's say we care in strong AI it's never clear to me what they really mean by this view but I think it's also problematic to do this because there's this sense that as long as it's not strongly i it's not a problem and as soon as it's from AI everything change changes another thing is that bigger word if there's that big of a shift like the GTA is just becoming more and more influential and it's also becoming more and more sophisticated doing more and more things that are harder to do the concerns if you think about it is doing automatically which is 500 hours of videos per minute which is and within this time it's also like does a lot of image recognition face detection on on the videos it does captioning so natural language processing automatic translation of the subtitles and also it learns the from the profiles of billions of users like given what I've seen my history on YouTube you can infer a lot of things it has so called a vector representation of me and a vector representation of each video it's already extremely sophisticated it's going to get more and more sophisticated it's not that narrow either like he's doing all of these things so of course they're different components or more or less the particular they do connect to one another in the same way that in your brains they are these different components at all doing these different things that the visual cortex and all of this is highly intertwined and so like the generative best comes from doing more and more things and getting all of these components being more connected that's already