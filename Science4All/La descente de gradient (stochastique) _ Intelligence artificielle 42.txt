la fin on a parlé des réseaux de neurones et je vous ai dit que ça ressemblait avant tout à une grosse bidouille dégueulasse ce qui avait amené le lait de 2011 à conclure que ça ne pourrait jamais être bien utile et très clairement si le lait de 2011 a conclu cela c'est qu'il n'a pas lu l'article de turing de 1950 dont on a longuement parlé dans les épisodes 2 à 7 sur vous un épisode 7 oui parce qu'en 1950 turing avait déjà anticipé le fait que l'intelligence artificielle de niveau humain émergerait sans doute d'une bidouille de code extrêmement sophistiqués guidés par des données autrement dit turing avait anticipé le machine learning pour rappel dans l'épisode 7 on a vu que turing avait compris que tout algorithme très intelligent correspondait nécessairement un code source extrêmement longs sans doute d'au moins 1 gigaoctet or écrire un tel code est une tâche herculéenne qu'aucun humain ni groupe d'humains ne serait capable de réaliser en moins disons d'un siècle pour turing la seule façon d'écrire un tel code c'est de partir d'un algorithme initial est de chercher à le complexifier au fur et à mesure en le guidant à l'aide est un très grand nombre de données typiquement une façon d'implémenter les idées de turing c'est de considérer que notre algorithme contient un très grand nombre de paramètres c'est-à-dire des millions voire des milliards de boutons à tourner et d'explorer les valeurs possibles de ces paramètres en étant guidé dans cette exploration par les données selon turing ce nec ainsi qu'après cet apprentissage on disposera d'algorithmes de grande complexité qu'ils soient nés en moins pertinent pour des applications en lien avec le monde réel et bien ça c'est exactement le principe d'une machine learning moderne ou considère des modèles avec des millions de boutons à ajuster et pendant la phase dite d'apprentissage on exploite des données pour déterminer dans quelle direction tourner ses différents bouton ok mais reste encore déterminer comment ajuster les boutons en fonction de chaque nouvelle donne est observé et là on pourrait imaginer plusieurs façons de faire en baliser un nom algorithmes d'apprentissage préféré bien sûr la formule de bêtises mais en bayésiens pragmatique je sais aussi que la formule de bay street hier des calculs beaucoup trop long en pratique en fait ce qui serait plus simple c'est qu'étant donné les positions actuelles les boutons et étant donné des données à analyser on puisse déterminer exactement les changements optimaux d'opposition des boutons pour que notre machine apprennent et bien ça c'est exactement ce que propose la descente de gradient pour comprendre cela prenons un cas simples imaginons que la machine cherche à reconnaître des chats on lui donne des images en entrée et elle ressort un nombre entre 0 et 1 qui est la probabilité selon la machine que l'image en question soit une image de chat pour tester les performances de la machine on peut lui donner plein d' image différente quand on lui montre une image où il n'ya pas de chat la machine est censée répondre 0 mais de façon intéressante on peut dire que si elle répond 0,1 elle n'aura pas trop faux revanche elle répond 0,9 au 1 là elle aura très fort une façon classique de mesurer l'erreur de la machine sur une image sans chars et de calculer le carré de sa sortie ainsi s'il n'y a pas de chat et si la machine 10 0,1 alors elle aura une petite pénalité de 0,1 au carré qui est égal à 0,01 mais si elles disent 0,9 alors sa pénalité sera alors 0,81 de façon générale si la machine répond pas à une image son chat alors elle recevra une pénalité égale ap carey al'inverse ans montre une image de chat la machine celle ci est censée répondre quelque chose de très proche de 1,7 fois la pénalité du chat pourra être mesurée par la quantité un monte au carré ainsi si paix est égal à 0,9 alors la pénalité sera un moins 0,9 au carré qui est égal à 0,1 au carré kiéthéga les 0,01 ce qui est très faible pour mesurer la performance de notre machine on peut lui donner plein de données par exemple des milliards de photos et calculées les pénalités moyenne de la machine ainsi une machine avec une erreur quadratique moyenne de 0,25 sera plutôt mauvaise tandis qu'une machine avec une erreur qu pratique moyenne de 0,01 sera très bonne l'erreur quadratique moyenne de la machine est aussi appelée fonction de perte ou l'osce fonctionnent en anglais et au final le machine learning consiste à déterminer les réglages de la machine qui minimise cette fonction de pertes reste maintenant à déterminer ses réglages de la machine pour cela considérons d'abord le cas simple où la machine ne possède qu'un seul bouton qui peut prendre des valeurs disons entre 0 et 100 si on est patient on peut tester toutes les positions de la machine pour chaque position peut ainsi déterminer l'erreur moyenne de la machine avec le réglage en question on peut même tracé une courbe qui donne les erreurs de la machine en fonction de la position du bouton et sur ce graphe on voit alors très bien ce qu'il faut faire pour minimiser les erreurs de prédiction il faut mettre le bouton à la position 42 hockey mais le problème c'est que ça nous a pris un peu de temps et en pratique il serait vraiment bon de savoir si lorsqu'on est à la position 63 10 ans il vaut mieux tourner le bouton vers la gauche ou vers la droite mieux idéalement il serait bon de savoir s'il faut beaucoup tourner à gauche légèrement à gauche légèrement à droite beaucoup à droite ou rester tel quel eh bien il se trouve qu'en mathématiques il existe un outil qui a été inventé pour répondre à cette question cet outil est appelée la dérivée de la fonction de pertes en fonction du réglage du bouton si cette dérive et est très positive c'est qu'il nous faut beaucoup tourner à gauche et si elle est légèrement négative c'est qu'il nous faut légèrement tourner à droite alors il y aurait beaucoup plus à dire sur la dérive et malheureusement je ne vais pas prendre le temps de bien en parler mais j'ai fait pas mal de vidéos sur ce sujet sur la chaîne loin de 10 da pour ceux qui veulent en savoir plus ok la dérive et ça marche bien dans le cas où on n'a qu'un seul bouton à tourner mais qu'en est il du cas où il ya plusieurs boutons et bien tout bêtement on peut regarder les effets de chaque tournage de boutons sur les performances de notre machine dans le jargon on parle de dérivées partielles de la fonction de pertes par les réglages que chaque bouton on obtient ainsi pour chaque bouton un nombre qui nous dit à quel point il faut tourner le bouton pour améliorer les performances de la machine la collection de ces nombres forme alors un vecteur que l'on appelle le gradient de la performance de l'âme la chine par les réglages et bien l'algorithme par descente de gradient ça consiste tout bêtement à tourner les boutons proportionnellement aux dérivées partielles des boutons ou dit encore autrement ce revient à suivre les indications suggéré par le gradient et comme ce gradient ai déduit des performances de la machine qui sont déduits des données d'entraînement de la phase d'apprentissage on voit bien en quoi l'algorithme par descente de gradient correspond bien à une exploration des réglages de la machine guidée par les données alors c'est bien beau tout ça mais le problème en général c'est que calculé le gradient peut être un problème très difficile en tant que tel en fait dans un cadre très général ce calcul de gradient peut être si l'on qui n'est pas raisonnable de l'effectuer en pratique en particulier pour une machine quelconque je vous déconseille d'entreprendre ce calcul cependant comme je lai brièvement mentionné dans l'épisode précédent il se trouve que dans le cas des réseaux de neurones les gradients sont formidablement simple à calculer comme on le verra la semaine prochaine et c'est vraiment ça à mon sens ce qui fait le succès des réseaux de neurones en fait cette faculté à être guidé par un gradient semble si crucial que certains chercheurs comme yann le cun préfère désormais parler de programmation différentiel c'est-à-dire de programmation qui sapent il presque exclusivement sur le fait d'être guidés par les gradients déduit de performances de la machine sur des données d'entraînement selon ian qu'un le futur de l'informatique c'est la programmation différentiel donc ce jour-là capitulaire descente de gradient ça consiste à partir d'abord d'un réglage quelconque ils ont ensuite procédé à des itérations pour améliorer au fur et à mesure ce réglage la première étape c'est de calculer le gradient qui correspond aux réglages au cours puis on va suivre les indications données par ce gradient pour en déduire un nouveau réglage et il s'agit ensuite de répéter cette procédure encore et encore pour améliorer encore et encore notre machine alors tout ce que j'expliquais jusque là peut paraître assez clair j'espère mais en fait ça s'applique surtout aucun restreint de l'apprentissage d'y superviser c'est à dire le cas où l'on cherche à prédire la belle à partir des données comme par exemple prédire la présence de chats dans une image malheureusement dans un cas plus générale d'apprentissage notamment dans le cas de la précarité non supervisées il n'est pas forcément évident qu'il existe une fonction de pertes amis miser or ces fonctions de perdre sont vraiment indispensables pour pouvoir ensuite définir la notion de gradient et pour pouvoir ensuite effectué de la descente de gradient on est au moins sauver la descente de gradient ont ainsi tendance à diluer ce que l'on veut vraiment faire pour ceux ramenés un problème de minimisation de fonction de pertes c'est typiquement cette façon de ramener la formule de bèze un problème de minimisation de fonction de pertes qui est au coeur du succès spectaculaire dégénérative adversaire le network ce dont on reparlera dans une future vidéo d'ailleurs même dans le cas de l'apprentissage superviser il reste le problème de la surinterprétation dont on apparaît dans les épisodes 11 à 18 fort heureusement les chercheurs ont trouvé une manière de combattre la surinterprétation tout en restant dans le cadre d'une programmation différentiel en introduisant par exemple la régularisation dont on a parlé dans l'épisode 18 de façon générale il semble que le machine learning prometteur c'est celui qui parvient à intégrer des notions fondamentales de turing de la formule deux pays tout en obéissant au paradigme d'exploration des réglages par descente de gradient qu'impose le paradigme de la programmation différentiel enfin une dernière chose vous aurez remarqué que le titre de cette vidéo ne parle pas de descente de gradient il parle en fait de descendre de gradient stochastiques oui parce qu'ils se trouvent qu'en pratique calculer les performances exactes d'une configuration de machine prend beaucoup de temps puisqu'il faut tester la machine avec un très grand nombre de données plutôt que de se faire la descente de gradient stochastique propose de ne tirer qu'un petit échantillon de données de calcul le gradient vis-à-vis de ces données uniquement et de tourner les réglages des boutons en fonction de ce gradient et il se trouve que c'est ainsi que fonctionnent les algorithmes de machine learning les plus performants d'aujourd'hui on peut interpréter cette descente de gradient stochastique comme suit à chaque étape on tire quelques données au hasard on ajuste les paramètres de notre machine pour aller dans le sens de l'explication de ces données sont toutefois cherché à pleinement expliqué ces données et puis on réitère ses petits quant à d'apprentissage un très grand nombre de fois cet apprentissage forme ensuite une sorte de danse des paramètres composé de certains pas parfois étrange mais de façon cruciale est assez stupéfiante la combinaison d'un très grand nombre de ses pas de danse nous guident alors vers des paramètres plus pertinents qui peuvent alors donner lieu à une la chine aux performances surhumaine la lymphe on a parlé des réseaux de neurones et gamer 12/13 nous demandent quel langage utilisé pour pouvoir coder des réseaux de neurones alors je suis absolument pas développeur codé de réseaux de neurones du couvain je vais pas avoir une réponse forcément très intelligente mais en gros le truc qui a l'air d'être utilisé par vraiment pas mal de gens en tout cas dans mon labo de recherche c'est vraiment elle sert flot le truc de google qui marche vraiment vraiment bien alors c'est parce que dire à mes collègues mais moi ça marche vraiment pas mal c'est passé d'utilisation c'est souvent utilisé en combinaison avec python mais je suis sûr vous pouvez l'utiliser à la plaie avec d'autres tapis et pour apprendre à mettre les mains dans le cambouis et a programmé des réseaux de neurones je vous recommande les tutoriels de plusieurs chaînes youtube notamment celle en français de thibaut neveu et sinon c'est le côté anglophone qui marche très très bien de cirage ravale où il ya vraiment pas mal pas mal de choses sur river demande si je vais parler des algorithmes génétiques dans cette série sur un silence artificielle et la réponse est non en tout cas pas dans un épisode dédié du coup j'en parle le matin très rapidement ce qui est amusant c'est que les algorithmes génétiques c'est une idée qui a été proposé par alan turing encore une fois dans son papier 2950 vraiment essayé de poser la question de comment est-ce qu'on en arrive et un cerveau humain et voilà c'est rapide ont bien que m l'évolution darwinienne eu un rôle très important à jouer dans ce développement de la complexité du cerveau humain et du coup turing a proposé très naturellement l'idée que des intelligences artificielles soient conçues et optimisées et apprises à partir de principes similaires aléas sélection darwinienne et c'est ainsi que sont nés un peu les algorithmes génétiques alors très rapidement le principe est bien sûr très inspirée de la biologie mais en particulier de trois aspects de la biologie le premier c'est de maintenir en vie une population assez diversifié de différence d'intelligence artificielle donc on va pas chercher uniquement à voir le la meilleure possible l'intelligence artificielle mais on va essayer de s'assurer ce qu'est une certaine diversité dans une famille d'intelligence artificielle ensuite la deuxième étape c'est à croire a consisté à faire des croisements et des mutations pour créer des nouveaux des nouvelles variantes en fait noté lyon-st artificielle qui sont peut-être combiné ensuite avec d'autres d'algorithmes d'apprentissage pour appeler les optimiser petit peu parce qu'il suffit pas de faire un croisement c'est a priori le croisement avait donné lieu à quelque chose qui est pas terrible mais si on prend quelque chose de pas terrible - ce sera peut-être nouveaux et s'ils ont ensuite on les ducs on aura peut-être quelque chose qui sera du même niveau que notre famille notre population l'attirance artificielle et de temps en temps on parfois d'un truc qui sera bien meilleur et enfin le troisième aspect très important c'est la sélection qui va consister en essentiellement à supprimer de la population d'intelligence artificielle les intelligences artificielles qui sont les moins performantes et ainsi au fur et à mesure des itérations et des générations on peut s'attendre à ce que la population de l'intelligence artificielle s'améliore au fur et à mesure et atteignent des niveaux qui serait difficile à calculer autrement ce qui est assez contre actif c'est que ce principe très simple qui combine du coup avoir une population diversifiée créer des mutations et sélectionner uniquement les meilleurs en tout cas supprimer les moins bons ces trois principes très simples suffisent à ce moment avoir des performances qui sont sur humaine on se rend souvent ça peut résoudre des problèmes temps mais beaucoup d'application des problèmes qui sont souvent une paix durable par exemple le voyageur de commerce où on arrivait assez facilement avoir principalement on arrive à voir aujourd'hui des solutions qui sont meilleurs que ceux qu'un humain serait capable de trouver et qui sont même parfois meilleur que tout autre algorithme qui a été proposé donc il arrive que ces principes très très basique qui ne repose pas sur 7 des séances de gradient et sur un indicateur de dire dans quel sens tournai bouton parfois se fait de tournée vous ton semble parfois limités pour certains problèmes et du coup pour à ces problèmes là il peut être utile d'avoir d'autres méthodes comme par exemple les algorithmes génétiques donc si je devais parier je dirais que la descente de gradient reste un truc extrêmement important qui sera certainement un composant central dans l'intelligence artificielle du futur mais peut-être que très probablement même ça ne suffira pas et qu'il faudra combiner ça pas mal d'autres techniques comme par exemple ces idées des algorithmes génétiques je pense aussi des idées comme des trucs un peu plus bas et vient comme les méthodes d'échantillonnage notamment monte-carlo mark mark of kings seront des méthodes qui vont jouer aussi des rôles importants à venir je vous renvoie à terme envers mon livre où je parle de pas mal de ses différents algorithmes enfin un cafard demande si un réseau de neurones est capable d'apprécier mais n'importe quelle fonction est ce que ça veut dire qu'elle est capable d'apprécier la formule de belize en gros la réponse à cette question et non pour des raisons parfois un peu compliqué mais même au delà de ça en fait il y à des fonctions que de façon pragmatique les réseaux de nos ne seront pas capables d'à proxy mais pour une raison assez simple c'est que la taille du réseau de neurones nécessaire pour approximer ses fonctions pourraient être exponentiellement grand donc vraiment irréalisable dans notre univers dont par exemple il est possible que pour certaines fonctions pour avoir une approximation assez bonne de cette fonction le réseau de neurones doit avoir un google de neurones ce qui est pas ce qui rentre pas dans l'univers observable puisqu'il ya moins d'un google de particules dans l'univers observable et en fait ça peut souvent même à être bien pire que ça puisqu'on peut avoir des configurations il faut un googleplex de neurones par exemple pour pouvoir approcher une certaine fonction du coup il ya son problème de la complexité en fait du réseau de neurones qui est absolument primordiale à prendre en compte est en fait c'est une raison additionnelle pour laquelle la le terme de l'universalité des réseaux de neurones est loin d' être conclusif quant au fait que les réseaux de neurones sont la solution ultime au machine learning bref j'aurais tendance à dire que l'auteur de l'approximation universel c'est un peu une condition un peu nécessaire pour le succès des réseaux de neurones mais ca tue ne sont pas suffisants pour l'expliquer et il me semble que vraiment le truc qui fait que les réseaux de neurones marche aussi bien c'est le fait qu'ils se prête très bien à la descente de gradient et donc à la programmation différentiel elle espère que vous avez aimé cette vidéo la prochaine fois on va parler de comment est ce qu'avec un réseau de neurones on peut calculer assez facilement le gradient du réseau de neurones dans certaines données et tout ça va reposer sur un algorithme qu'on appelle la rétro propagation ou encore bach propagation propre en anglais si vous avez aimez cette vidéo pensez à la ligue et à la commenter a partagé conserver d'abonnés pour notamment cris futurs épisodes maire ce type en power dont et j'espère que vous serez là la prochaine fois la barre des 50 [Musique] un de gaines indiqué mme jackson