si le machine learning n'est autre que de l'apprentissage ne s'agit-il pas d'un simple problème de mémorisation par exemple pour programmer syrie ou au quai google ne pourrait-on pas faire retenir des dialogues à unir pour qu'elle connaisse les réponses classiques aux questions usuelles unis à ayant mémorisé beaucoup de savoir ne serait-elle pas intelligente vous voyez le problème on en a déjà parlé dans l'épisode 3 le nombre de discussions imaginables est beaucoup plus grand que le nombre de particules dans l'univers est donc clairement un ordinateur ne peut pas mémoriser toutes ces discussions ne serait ce parce que la quasi-totalité des discussions imaginables ont non seulement jamais eu lieu elles n'ont même jamais été écrites et c'est un petit peu dur de mémoriser des données qui n'ont jamais été produite mais beaucoup de discussions se ressemblent je vous dis rentrer ça va et comment ça va c'est un peu du pareil au même d'où l'idée de faire de l'interpolation père de l'interpolation c'est dire que si je n'ai jamais vu la phrase comment ça va mais si j'ai vu passer les questions ça va comment tu vas comment ça roule alors les réponses à ces questions surtout si ce sont les mêmes sont sans doute des bonne réponse à la question initiale bon là je vous ai donné la version test de turing de l'interpolation mais les applications de l'interpolation sont très nombreuses dans tous les domaines imaginables en sciences en entreprise ou pour choisir le prochain film que vous allez voir au cinéma imaginons que vous veniez de voir la fiche du film pentagone papers vous vous dites que ça doit être vachement cool parce que ça a l'air de pareilles de cette étrange figure géométrique et le pentagone mais vous hésitez parce que il n'ya pas de frein taiwan sur la fiche du coup vous en parler un ami qui vient de le voir et de façon cruciale qui partagent les mêmes goûts cinématographiques que vous il a aimé le film vous devinez que vous allez aimer le film vous aussi et si la détestez vous devinez que vous allez le détestez vous aussi félicitations vous venez d'appliquer un algorithme de machine learning qu'on appelle l'algorithme du plus proche voisin uni rust never en anglais pour faire une prédiction y pour un individu x cet algorithme du plus proche voisin va chercher l'individu exprime le plus similaire à x dont il connaît la valeur de y prime ici bien sûr exprime c'est votre ami cinématographique mon proche y prime est la vie de votre ami et la prédiction y que vous allez faire est calculé via l'équation y est égal à y prime ça va tout le monde suit alors dans le langage usuel du machine learning les x sont appelés des caractéristiques ou fitch 2 et les y sont appelés des étiquettes ou les boys je m'excuse auprès des défenseurs de la langue française mais je vais utiliser la terminologie anglaise parce que c'est celle qui est utilisée au niveau de la recherche et puis il a fait aussi deux fois moins deux syllabes et enfin et surtout je fais mon politiciens démagogues qui ne fait que suivre l'opinion publique sur twitter le problème du machine learning donc c'est souvent présenté comme la prédiction de les buzz à partir des faiseuses d'une donnée en fait plus précisément c'est ce que l'on appelle l'apprentissage superviser ou sup vice teneur ning en anglais les applications de l'apprentissage superviser sont très nombreuses en sciences ont santé ou sur internet le problème de l'apprentissage superviser c'est de construire une fonction de prédiction c'est à dire une fonction qui prend des fishers et qui calcule des levels qui correspondent à ses futures et pour construire cette fonction l'apprentissage superviser va s'appuyer sur un ensemble de perdre fit choses et les boys retenez bien tout ça on va beaucoup travailler avec ce formalisme dans les vidéos à venir dans cela l'algorithme du plus proche voisin et bien une solution au problème de l'apprentissage superviser il repose d'abord sur la mémorisation de tous les exemples puis pour effectuer une prédiction pour des futurs x l'algorithme va calculer f 2 x est égal à y prime ou y primé le les bols de plus proche voisin exprime 2x en mémoire et ce qui est vraiment bien avec cet algorithme c'est quelqu'un de super simple mais j'imagine que vous sentez venir le problème même votre ami qui vous est le plus semblable n'est pas toujours d'accord avec vous parfois il raconte même des trucs à bitumer un truc marrant hier j'ai renversé deux cyclistes voilà une millions d'instinct en sang pour être moins susceptibles aux erreurs de votre ami une variante de l'algorithme du plus proche voisin consiste à son d'évoca amis les plus proches cinématographiquement et ensuite combiné leurs avis pour estimer vos chance d'aimer le film c'est ce que l'on appelle l'algorithme des cas plus proches voisins ou kanye west amber ou encore k n est qu à nn ou des variantes de cayenne c'est un algorithme et de machine learning souvent utilisé en pratique notamment par les géants du web qui peuvent chercher vos semblables au delà de vos cercles d'amis sauf que la recherche que jean semblables sur le web pose maintenant un autre problème à savoir le calcul de la similarité entre des individus pour calculer la similarité entre deux profils facebook facebook peut utiliser toutes les données entrées par les utilisateurs mais faisons simple pour commencer supposons que facebook ne considère que l'âge de la personne et le nombre de ses amis facebook autrement dit on va supposer que les phishers d'un individu sont de nombre x âge et x amis on peut alors représenter graphiquement la personne en question comme étant un point d'un espace dans les axes sont de l'âge et le nombre d'amis l'espace des phisheurs est alors un plan de dimension 2 intuitivement on va supposer que les individus avec des âges et des nombres d'amis facebook semblables ont des goûts cinématographiques plus semblable autrement dit on va supposer que les individus proches en l'espace d fishers sont aussi proches cinématographiquement on a tout ce qu'il faut pour appliquer qu à nn pour commencer il nous faut une phase d'apprentissage qui dans le cas de knn n'est qu'une simple mémorisation plus spécifiquement vous allez chercher des lives c'est à dire une liste de like dislike et du film pour chaque live good is like vous regardez le profil de l'individu et en fonction de son âge et de son nombre d'amis vous allez situer son light et dans le graphe fait cela pour tous pleins d'individus et vous obtenez alors une carte des les boys dans l'espace défectueuse on a terminé l'apprentissage on peut passer aux prédictions pour prédire si un individu va aimer un film on va chercher ses features les placer dans le craf puis on va chercher les futurs les plus proches en mémoire est typiquement si le futur est entouré de like on va prédire que l'individu va aimer le film sinon s'il est entouré de quelques laïcs et beaucoup de dislike on va prédire qu'il est plutôt probable qu'ils n'aimera pas le film bref quesnel est un algorithme de machine learning incroyablement simple et dont l'apprentissage est uniquement une simple mémorisation des données et la a vu le titre de cette vidéo est tout ce que vous avez entendu à propos des réseaux de neurones ou du dip learning vous vous dites sans doute qu'il y a un truc qui va clocher avec klm et pourtant il existe un théorème qui dit que knn permet toujours de très bien à prendre ce théorème et le théorème de cover art en 1967 il affirme que klm finira par faire quasiment autant d'erreurs qu'une prédiction optimale oui parce qu'une prédiction parfaite des préférences cinématographique d'un individu à partir uniquement de son âge et de son nombre d'amis facebook vous l'imaginez c'est en général impossible même une prédiction optimale se trompera une fraction air du temps et bien qu'elle ne se trompera jamais plus d'une fraction deux fois air du temps quand verra-t-on même prouvé une meilleure bande qui dépend aussi d'une ombrelle de catégorie que l'on considère oui parce qu'au lieu d'avoir juste j'aime j'aime pas on aura aussi pu avoir bien moyen bof on voit que si la prédiction optimale se trompe très peu alors knn se trompera très peu lui aussi et ça ça semble pas mal résoudre le problème de l'apprentissage superviser qu'en pensez vous bien sûr la réponse est non parce que sinon on n'aurait pas besoin de diplôme ning mais où est le problème d'après vous en fait il y en a trois trois problèmes de complexité les deux premiers sont des problèmes de complexité algorithmique quand on a plusieurs téraoctets de données la mémorisation des données par cannes n va faire un fichier de plusieurs téraoctets ça c'est pas très pratique déjà ça tire pas sur un téléphone on dit que quesnel et non paramétriques parce qu'il grossit avec la taille des données et pour certaines applications comme par exemple au lhc ou des pétaoctets de données sont collectées à chaque instant c'est juste pas jouable et l'autre problème du coup c'est que ça va aussi faire de longs calculs typiquement sont un des téraoctets effectuer une prédiction peut prendre des minutes de calcul mais surtout le troisième et le plus important problème c'est la complexité d'échantillonnage c'est à dire le nombre de données nécessaires pour effectuer un bon apprentissage c'est vraiment là que le bât blesse et pour comprendre pourquoi il va nous falloir parler de la dimension de l'espace des freeters et oui les bizarreries des espaces de très grandes dimensions sont en fait indispensable à comprendre pour bien comprendre de machine learning dans notre exemple on a vu un espace des features deux dimensions de mets en pratique si vous considérez toutes les infos d'une page facebook notamment toutes les pages qui ont été leakée tous les postes qui ont été leakée ou pire encore toutes les photos on a rapidement un espace d fishers deux dimensions 1000 voire un million et oui on a une nouvelle dimension pour chaque type d'informations qu'on trouve sur une page profil facebook sauf que les espaces de dimensions 1 millions sont en un sens beaucoup plus passion que espace des dimensions 2 pour comprendre tout ça on va commencer simple on va partir de la dimension de on va prendre l'objet le plus classique de la dimension de à savoir le carré un carré ça à quatre sommets j'espère que jusque là tout de suite en dimension 3 1 carré devient récurent et un cube a maintenant lui sommet en fait à chaque fois qu'on monte d'une dimension on double le nombre de sommets puisque montée d'une dimension revient à un copier coller le cube et à translate est l'une des copies à travers la nouvelle dimension il vous renvoie vers cette vidéo de michael sonné pour en savoir plus mais alors l'hyper cubes de dimension des va avoir deux puissances des sommets imaginons pour simplifier que les phishers soit toujours des sommets de ce cube alors après avoir collecté deux puissances dément une donnée vous n'aurez couvert que la moitié des sommets de l'hyper cubes ce qui veut dire que pour la majorité des futurs imaginables vous n'aurez aucun très proches voisins sur lequel vous appuyer pour effectuer une prédiction autrement dit en gros en dimensions d il faut environ deux puissances des données pour que ken fonctionne sauf qu'en pratique on l'a vu dès souvent de l'ordre de 1000 voire un million voire en fait beaucoup beaucoup plus si l'on considère par exemple le test de turing horde est le cas d égale 1000 pour que quesnel fonctionne il faut deux puissances mille donné et ça c'est bien plus grand que le nombre de particules dans l'univers et c'est pour ça que ken échoue en apprentissage superviser dès que le nombre de dimensions de l'espace défi just n'est pas tout petit alors tout n'est pas à rejeter pour autant en pratique cayenne et souvent combinée à un apprentissage des pictures pertinents typiquement peut-être une variation de 10 amis facebook n'est pas aussi pertinent pour la prédiction de préférence cinématographique une variation d'âge de 10 ans prendre en compte ce genre de considérations revient à changer ce qu'on entend par proximité ou similarités on parle d'apprentissage de la métrique de canet et sa basse et un sacré problème est la façon qui semble la plus commune pour apprendre cette métrique tout cas aujourd'hui ça semble consister à transformer l'espace des features via un réseau de neurones typiquement en transformant l'espace des symboles syntaxique en un espace de sémantique on y viendra mais il va nous falloir pas mal d'épisodes pour en arriver là l'anr fois on a parlé de l'argument de turing est la raison pour laquelle le machine learning est nécessaire et sans doute suffisant pour permettre une intelligence artificielle de niveau humain l'argument de turing repose énormément sur la notion de complexité de kolmogorov et jean michel sarrat vrai que je parle davantage de cette notion alors énormément de choses à dire bien sûr ça prendra peut-être un petit peu trop de temps je vais pas le faire aujourd'hui mais je tiens à signer quand même que je suis fait taper sur les doigts par un de mes collègues j'ai converti au sur le monophysisme et qui m'a fait remarquer que j'ai appelé sa complexité de kolmogorov comme tout le monde alors qu'en fait c'est sur le mode off qui a inventé cette complexité et en plus bien sûr je le sais je n'ai pas trop osé aller en l'encontre de la terminologie rétabli mais c'est vrai que en fait bassée salomé 9 qui a inventé cette complexité trois ans avant que le golfe et en plus selon mena fa vraiment exploiter cette notion pour pouvoir comprendre ce qu'était l'apprentissage donc je pense que vraiment sur le mode off mérite beaucoup plus que kolmogorov d'être associé à cette notion du coup je vais appeler ça maintenant la complexité de solomonoff et je regrette un petit peu dans la vidéo précédente de pas l'avoir appelée ainsi non votera rapidement la complexité de solomonoff et donc la longueur de la description de la plus courte solution algorithmique un problème et on peut utiliser cette notion de complicité de seulement 9 pour notamment formaliser la notion de rasoir d'occam qui nous notions très importante en épistémologie et en machines en inc dont on reparlera dans un prochain épisode même si la façon classique de parler du rasoir d'occam en apprentissage statistiques notamment des pavia la complexité de somme 9 mai à travers notre notion qui est à dimension wc on reparlera de tout ça maxence du ms demande si j'ai pas un peu traitée de manière un peu expéditive l'incompressibilité du cerveau humain alors oui car mort y aura beaucoup plus à dire et c'est pas un sujet très facile particulier emmanuel jacob se demande pourquoi est-ce-que turing sorti ce chiffre de 10 puissance 9 qui semble un peu sortir de nulle part je pense qu'on est assez d'accord sur ça disait bon ballon que le cerveau est pas entièrement incompressibles et qu'en plus une bonne partie du cerveau qui ne sont pas indispensables à la résolution du test de turing et il a eu une estimation de la taille du cerveau entre dispense 17 10 puissance 15 aujourd'hui on sait que c'est beaucoup plus proche de 10 puissance 15 que de discussions se disent comment le nombre de synapses et à l'intérieur du cerveau et du coup on pourrait se dire que peut-être qu'à partir dispensable du cerveau pour pouvoir passer le test de turing est peut-être plus proche de 10 puissance 15 que de différences neuf en tout cas s'il est plus grand que 10 puissance 9 les arguments de turing s'applique encore plus unique 20,48 une demande comment c'est une façon de prouver qu'un programme est incompressible ou de calculer parlant de sa complexité de solomonoff ait baha pense malheureusement et non en fait là on nage en plein de l'indécidabilité c'est à dire qu'il n'existe pas de façon algorithmique de déterminer la complexité de solomonoff d'un problème et du coup le postulat de turing qui avait mis en avant dans la vidéo précédente c'est un problème qui même s'il était parfaitement définis mathématiquement on sait aussi qu'il n'existe aucune façon mathématique de prouver qu il est vrai plus précisément il est possible de trouver des bornes supérieur à la complexité de solomonoff d'une tâche pour ça il suffit en fait d'exhiber un algorithme qui résout 7h et de calculer la longueur de la description de cet algorithme ce moment la longueur de la description de cet algorithme est une borne supérieure mais on pourra jamais exclure fait qu'il existe des algorithmes beaucoup plus courts qui sont capables de résoudre la tâche aux aussie est la raison pour laquelle il était impossible de montrer qu'il n'existe pas de tête zago rythme c'est parce qu'il ya plein d'algorithmes assez étrange qui vont faire des calculs assez bizarre et dont il est impossible d'anticiper le fait que ses algorithmes pouvons terminer encore moi qui ont donné une bonne réponse et tout ça est lié aux pointes de l'indécidabilité du poème de l'arrêt dans les machines à turing c'est aussi très lié à la complétude de gödel elle va encore une fois faire la vidéo de thomas sur patience où il parle d'incomplétude bref tous ces arguments ils ne sont pas entièrement convaincant parce qu'il repose autant sur des postulats qui sont indémontrable mais ça n'empêche pas le fait qu'on puisse avoir une grande confiance en ses postulats parce que j'ai pas mal confiant son postulat de turing semble qu'en directement aujourd'hui on a l'impression que résoudre au test de turing sera pas facile avec un algorithme assez court et qu'il faut forcément des algorithmes très compliqué or la seule façon d'explorer les algorithmes avec des codes sources aussi lancé via le machine learning et vous forcément le machine learning est la clef de l'intelligence artificielle alors il ya peut-être un petit trou dans cet argument pointé par jonas daverio qui est le fait que je suppose qu'il faille forcément faire du machine learning pour explorer l'espace des gros algorithmes crt les codes écrit à la main ça ça marchera jamais mais je sais pas en quoi peut-être imaginer comme l'imaginent laborantins du temps que des viassois capable un jour des clercs déjà plus compliqué donc l'exploration des espaces plus grands pourra peut-être se faire directement par d'autres et c'est là que la complexité de ce land of est un concept très pertinent c'est que si on a une ia qui est capable d'écrire une ia avec un code source plus long que qu'elle même en fait le code source plus loin il est compréhensible du coup on fait les la tâche qui est résolu par la plus grosse elle est résolue bhl parler à plus petit suffit en fait de dire lié à plus petit de résoudre le problème pour ça va générer le le plus gros est elle va résoudre le problème du coût la complexité de solomonoff de la tâche en question sera pas celle de lier un plus gros ce sera celle de l'ia du petit d'une certaine manière un algorithme seul peut pas augmenter sa complexité de solomonoff du coup en cela ils incapables d'explorer l'espace des algorithmes allons code source dont le code source est essentiellement incompressibles alors techniquement il y reste possible qu'un petit algorithmes génère un algorithme plus gros dont la complexité des sols aux notes soient plus grande que celle du plq au rythme plus petits si cet algorithme le plus petit utilise le hasard pour générer l'algorithme plus grands en effet ça c'est possible il ya quand même très peu de chances que cet algorithme un plus grand s'il est apte ici la complexité sur le mono fanny siècle et l'obtient c'est via en fait un calcul aléatoire résolvent la tâche que l'on voulait résoudre puisque le nombre d'algorithmes et qui aurait ainsi pu être générés par un algorithme plus court est en fait très très grand et la plupart sans doute servent à rien en fait cette idée de gérer l'aléatoire qu on pourrait rajouter un code source c'est un problème aussi faire pas mal de vidéos là dessus c'est notamment cette notion que capture le concept de sophistication de se lever 9 si vous chercher ça sur internet vous avez rien trouvé parce que celle que j'ai inventé dans un truc qui a pas encore été publié mais on finira pas en parler bref l'argument clé c'est qu'on ne peut pas gagner une complexité pertinente en partir d'un petit algorithmes à moins d'utiliser quelque chose d'autre qui possède déjà une grande sophistication de solomonoff et ce truc qui possède déjà cette grande sophistication de zones off qui peut aider à guider l'exploration des espaces d'algorithmes plus grand et forcément quelque chose qui sera extérieur à l'algorithme lui-même du coup sera forcément des données qui seront du certains mineurs volet de notre environnement empirique dont la p6t6 ws est en fait très très grande donc tout ça c'est des arguments qui n'était pas vraiment dans turing parce que ses émotions je pense que tu ris n'y a pas forcément tout anticiper on est nous sommes qui ont été plus anticipé d'ailleurs par chronographe que parce nov il y aurait encore énormément à dire mais l'idée en gros c'est que en gros la complexité de la ghost me doivent venir de quelque part et ce quelque part c'est sans doute le monde extérieur et notamment les données empiriques et du coup pour pouvoir créer un moment une intelligence artificielle avec une grande complicité de solomonoff pertinente il faut absolument être utilisé quelque chose qui vient du monde extérieur doux l'idée d'une machine learning et d'où l'idée selon laquelle le machine learning est vraiment un truc incontournable bref comme le signale très bien je nassau daverio j'extrapole pas mal d idées de turing pour essayer de mieux les clarifier notamment pour le clarifier c'est utile d'invoquer des arguments postérieure à turing notamment ses histoires de sophistication etienne bertin fait la remarque très pertinente que le cerveau de l'enfant est très certainement compressible puisqu'une certaine manière elle a été essentiellement générée par le code génétique de l'enfant alors bien sûr ces histoires de code génétique turing ne les connaissais pas en tout cas pas très bien puisque la dn1 structure d'aden a découvert en 1953 et son art intel de 1950 notamment le code génétique humain aujourd'hui on sait qu'il est composé de 3 milliards de lettres donc ça se compte en gigaoctets et commencer que l'adn humain est capable de créer un cerveau humain on sait que la complexité de ce lot menace du cerveau humain bébé est au plus de quelques gigaoctets donc ça déjà c'est une très bonne nouvelle si on dit que c'est moins que ça en fait a priori la partie de l'adn qui est vraiment indispensable au codage de la structure du cerveau humain est sans doute une petite fraction de tout l'adn humain et du coup on peut espérer que le cerveau bébé à une complexité de solomonoff je sais pas être de l'ordre du million de l'ibo cote après déjà plus faisable que camille gare peut-être même moins pourquoi j'en sais vraiment rien mais ça me permet comme de rebondir à nouveau avec l'article de turing qui proposait de faire une sélection naturelle des leurs dix machines c'est à dire des cerveaux enfants la personne flirter avec l'eugénisme là je parle bien de la sélection des learning la chine un des algorithmes de burning et turing proposé de simuler la sélection naturelle pour pouvoir sélectionner les algues au rythme de machine learning les plus prometteurs enfin frais nato me fait remarquer que c'est pas wiktionnaire mais wiktionnaire dictionnaire wiktionnaire j'espère que vous avez aimé cette vidéo donc en train de parler des trucs assez scolaire jean les désirs de machine learning apprentissage supervisé qui est vraiment le truc qui marche très très bien en fait je pensais que tout le monde en parle en ce moment donc j'y suis vraiment pas le machine en ligne plus général qui soit la prochaine fois on va continuer dans cette veine en parlant des algorithmes et les plus classiques en fait pour l'apprentissage a supervisé qui sont des algorithmes très lié à la structure linéaire des données notamment la régression minière et la classification lumière si vous avez aimez cette vidéo pensez à la ligue et elle a commenté à la partager pensez à vous abonner pendant les futurs épisodes martiaux qui peut encore dont il espère tirer la quatrième fois la machine learning en gros c'est ça on a des données ici sous la forme d'une entrée x et d'une sortie grecque on les présentait un algorithme qui a des boutons et qu'il est tourner jusqu'à ce qu'ils aient compris le lien entre x et y tu as c'est tout ce qu'on appelle la phase d'apprentissage et puis une fois que c'est fait on peut faire des prédictions que le nombre de sommets de l'umih percu va être égal au nombre de sommets du cube de départ plus le nombre de sommets du cube que l'on va déplacer vers la na ou le kata et donc lippé recul va avoir deux fois plus de 100 m l'écurie va donc avoir seize sommet