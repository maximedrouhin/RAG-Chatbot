plus rytas nonet spawn end à signer nécessite à thé ou d'un an langue de molière les multiples ne doivent pas être multipliés sans nécessité tels est une citation de guillaume d'ockham philosophe logicien et théologien anglais du 14e siècle ocam présente ainsi un principe qu'on appelle désormais le rasoir d'occam où le principe de parcimonie intuitivement dans un langage peu proche de celui qu'on a utilisées dans cette série ce principe affirme que les théories plus simples sont a priori plus crédible mais formaliser ce rasoir d'occam est en fait loin d'être aussi simple notamment car la notion de simplicité n'est pas si simple hashtag meta de meta par exemple la théorie du multivers cette théorie qui postule l'existence de plusieurs univers est ce une théorie simple en un sens on a envie de dire oui après tout je peux expliquer cette théorie en très peu de caractère à savoir la théorie du multivers suppose l'existence de plusieurs univers mais on a un autre sens toutefois on a envie de dire non après tout invoquer un multi vert revient a invoqué l'existence de beaucoup de choses dans l'univers certains physiciens diront ainsi que le multivers est une solution simple pour résoudre des problèmes difficiles comme l'oeuf un réglage des constantes de l'univers mais d'autres rétorqueront qu'invoquait le multivers revient a utilisé un marteau piqueur pour écraser une mouche ou du moins ses postures et une énorme complexité anthologique c'est à dire l'existence de vraiment beaucoup de choses alors qu'en est il le multivers est ce une théorie simple qu'elle crée danse a priori faute il a signé au multi vert pour mieux comprendre la nuit on vient de parler il semble en fait utile de nous tourner vers l'algorithmique oui car l'algorithmique nous propose une distinction rigoureuse entre les deux notions de complexité qu'on a esquissé la première correspond ainsi à la longueur de la description algorithmique de la théorie la seconde correspond à l'espace mémoire que nécessite l'exécution algorithmique de la théorie prenons un autre exemple pour comprendre à savoir le problème de résoudre le jeu des échecs est-ce que résoudre les échecs est simple et bien il se trouve que les échecs sont simples en le premier sens mais difficile on le second en effet l'algorithme minimax résout les échecs et peut-être décrire en quelques dizaines de lignes de code dion quelques centaines s'il faut aussi d'écrire des règles d échec si on le compare au code de windows ou à ceux utilisés en météorologie c'est certainement un code très court il existe ainsi un algorithme très court a déclaré pour résoudre les échecs en ce sens des échecs sont simples au passage j'entends parfois dire que les échecs sont simples notamment par rapport disons au sens social car c'est un problème dont on connaît les règles du jeu eh bien on vient d'être plus précis que ceux là encore ce qui rend les échecs simple c'est surtout l'existence d'un algorithme simple pour le résoudre par opposition et disons au poker à huit joueurs pour lequel les règles sont connues il n'y a pas d'algues au rythme simple pour déterminer la stratégie optimale notamment parce que ça dépend du comportement des autres joueurs bref en un sens les échecs sont simples car il existe un algorithme simple qui les résout cependant l'exécution de cet algorithme va nécessiter de garder en mémoire de grandes quantités d'informations comme toutes les positions intermédiaires des échecs et si elles sont gagnantes et perdantes ou nulle pour les blancs et il semble qu'il en sera inéluctablement le cas de toute solution algorithmique aux échecs on dit que la complexité en espace mémoire des échecs est grande en ce sens les échecs ne sont pas simples alors il ya des raisons techniques pour lesquelles le rasoir d'occam ou second sens est en fait difficile à complémenter qui ont à voir avec le fait que la complexité en espace mémoire d'un algorithme aux dépens des données sur lesquelles il est exécuté et peut être très très difficile d'anticiper et même à définir a priori a contrario la longueur de la description d'une théorie est facile à mesurer il suffit de compter le nombre de lignes de code ou pour être plus précis le nombre de caractères utilisés dans le code source c'est sans doute pour cette raison que le génie informaticiens créé solomonoff à opter pour ce critère de simplicité pour formaliser le rasoir d'occam et ainsi fournir une solution complète aux fameux problème de l'induction de riom il demeure toutefois 2 difficultés pour définir rigoureusement l'induction de solomonoff en premier lieu on peut se demander si la phrase la théorie du multivers supposée existence de plusieurs univers définit vraiment la théorie du multivers ne faut-il pas au moins d'écrire ses autres univers pour répondre à cette question il est utile de se souvenir de l'épisode précédent la description du multivers qu'on a donné là est en fait insuffisantes pour décrire une théorie à la solomonoff car cette phrase ne semblent pas permettre d'effectuer des prédictions par exemple au sujet de la météo qui fera demain la description qu'on en a donné n'est pas aux standards de seulement 9 mai la théorie du multivers est loin d'être la seule dans le cas de la même manière la seconde loi de newton l'équation est fait et gala am a beau être simple méprise seule elle ne permet pas de parier sur des données observable elle est donc pas aux standards non plus de l'induction de solomonoff idem pour des théories comme dieu existe ou c'est la faute aux aliens ces théories seul ne parie pas or une théorie à la solomonoff toi parier comme on a parlé la dernière fois pour devenir prédictive les lois de newton doivent s'accompagner d'une description de l'état physique de l'univers or sa description de l'état physique surtout si elle est détaillée au niveau des atomes risque fort d'être très longue rien comme corps contient dans les milliards de milliards de milliards de particules il faudrait ainsi autour de 10 puissance 27 lignes de code pour décrire mon corps dès lors la théorie newton plus état physique exact de l'univers est en fait peu crédible a priori puisque sa description est très longue en fait ceci explique pourquoi des théories macroscopique comme les lois de la thermodynamique peuvent être en un sens plus crédible que des lois microscopique comme la mécanique quantique en effet pour devenir prédictive ces théories macroscopique peuvent souvent être accompagnée d'une description très partiel de l'état physique de l'univers alors que la mécanique quantique nécessite généralement une description beaucoup plus précises notamment d état physique des particules dans ce cas la description loi plus l'état via la thermodynamique peut être bien plus succincte que vient la mécanique quantique bien souvent toutefois même en mécanique quantique on peut se contenter d'une description statistiques des particules plutôt que d'une description fine typiquement on va décrire uniquement la vitesse moyenne des particules et pas les vitesses de chacune des particules cette description statistiques étant beaucoup plus succincte elle donnera lieu à une théorie quantique statistiques beaucoup plus crédible a priori de la même manière les théories économiques ou les modèles épidémiologistes très sophistiquée avec des milliers de paramètres pour décrire le comportement de chaque individu sont en fait des théories peu crédible si on ne dispose pas de données suffisantes pour ajuster ces paramètres après tout pour devenir prédictive ces théories devront être complétés par des choix de valeurs de ses paramètres a contrario une simple régression linéaire sur quelques centaines de données empiriques ou une extrapolation exponentielle grossière peuvent être considérés plus crédible a priori puisque la description du modèle prédictif est alors extrêmement succincte et c'est pour une raison similaire que certaines théories du multivers sont simples au sens de solomonoff par exemple le multivers est souvent invoquée pour expliquer le fait un réglage des constantes de notre univers ces constantes semble particulièrement bien ajustés pour permettre l'émergence de gallas d'étoilés de planètes et donc pour rendre toute vie possible dans l'univers la théorie du multivers des fins réglages suppose qu'il existera bien d'autres univers descriptible par une distribution statistique des valeurs des constantes fondamentales dans les différents univers voilà une description statistiques partielles certes mais surtout simple est suffisante pour prédire au moins l'existence de la vie dans au moins un univers et pour expliquer que toute vie observera ses valeurs de ses constantes dans son univers au sens de solomonoff nahla une théorie très simple qui fait des prédictions que l'on obtient en pratique donc au sens de ce la complexité d'une théorie doit être la longueur de la description de tout ce qui est nécessaire pour effectuer des prédictions avec cette théorie ok mais il reste encore quelques problèmes en particulier les langage usuel reste souvent minés d'ambiguïté qui ouvre la porte à l'interprétation et surtout à la malléabilité ainsi il existe souvent plusieurs interprétations différentes d'une théorie décrite dans le langage usuel ce qui peut conduire à différentes prédictions différentes et il n'était alors pas clair qu'elle prédictions sa théorie fait cette ambiguïté du langage a mis des millénaires à être reconnu mais comme on l'a vu dans la salle et sur l'infini notamment à partir du 19ème siècle cette ambiguïté pause et a de plus en plus de problèmes aux mathématiques et en particulier en analyse l'ambiguïté des mots aura ainsi conduit le pauvre augustin lui cauchy renseigner un bon nombre de théorèmes faux dans un cours qui étaient pourtant alors reconnu comme un succès mondial de rigueur mathématique ce n'est qu'à la fin du 19e siècle que le langage formel fut développée par des logiciens comme du cpp anneaux et gottlob frigo mais non sans difficultés après tout au tournant du siècle bertrand russell démontra que toute l'oeuvre de frigo était certes enfin vidée d'ambiguïté mais elle était aussi et surtout incohérente bon disons qu'elle avait le don d'avoir enlevé suffisamment d'ambiguïté pour qu'il soit enfin possible de prouver son incohérence ce qui est déjà une propriété en fait assez rare russell travaillera pendant plus d'une décennie sur l'élaboration d'un langage formel sans ambiguïté ni incohérences et surtout avec des règles de manipulation logiquement valide mais il connut un succès mitigé en fait les limites du langage de russell furent démontrer inévitable par un certain kurt gödel en 1931 ce n'est qu'en 1936 que les logiciens alonso chuck et alan turing développèrent enfin des langues à risque non seulement étaient formels et sans ambiguïté mais qui semblait aussi et surtout universel ces langages sur le langage du lambda calcul d'un côté et le langage machine des machine de turing de l'autre et leur universalité réside dans la thèse ap les thèses de church turing selon laquelle chacun de ces langages pouvait exprimer toutes les théories exécutable dans notre univers malheureusement le lambda calcul de chuck et le langage machine de turing demeurent tous deux très difficile à comprendre et à utiliser pour le commun des mortels et même pour le commun des génie scientifique mais au fil des décennies des progrès spectaculaires ont été faits dans l'élaboration de langage formel turing complet et compréhensible si bien que désormais certains de ces langages sont utilisées par des millions d'humains sur terre et sont même enseigné à l'école c'est langage formel et universel on les appelle aussi des langages de programmation turing complet et des langages comme basique c++ java script trust et python en sont des exemples en fait le clip est à nos frigos church et turing serait sans doute émerveillé par la prépondérance de tel langage désormais qu'ils sont ainsi parlé plus tôt écrit par des millions d'humains sur terre c'est plus que par exemple le breton désolé les bretons et ses langages sont bien sans ambiguïté puisque même une machine saura comment les interpréter c'est sans doute ce qui a convaincu solomonoff de définir la complexité d'une théorie par la longueur de sa description dans un langage de programmation turing complet on a maintenant tout ce qu'il faut lisez le rasoir d'occam si on considère un langage elle il suffit maintenant de considérer que la probabilité à priori d'une théorie à la somme 9 et exponentiellement faible en la longueur de sa description alors pour des raisons techniques dont on a parlé dans l'épisode 29 il se trouve que pour des machine de turing universelle en langage binaire les particulièrement naturel de considérer que la crédence a priori d'une théorie t descriptible en nba est inversement proportionnelle à de puissants scène où la constante est rajustée de sorte que la somme des probabilités de toutes les théories à la salle olof a priori est égal à 1 on obtient ainsi la priory de seulement 9 alors désolé pour le charabia technique ce qu'il faut retenir c'est que toute cette construction est en fait extrêmement naturelle et conduit à définir un a priori parfaitement conforme au rasoir d'occam a priori les théories plus simple à décrire son exponentiellement plus crédible mieux encore l'a priori de solomonoff convient de définir a aussi le bon goût d'être universel dans le sens où il assigne une crédence strictement positive à toutes les théories à la soul man of aucune théorie néo dogmatiquement rejetée par l'induction de solomonoff aucune ne manque à l'appel il reste une fois 1 problème tout la construction de somme 9 dépend du langage de programmation utilisé en effet certaines idées simples a exprimé dans certains langages semble nécessairement laborieuse a exprimé dans d'autres mais alors quel langage de programmation utiliser et bien à ce sujet je vous invite à lire un extrait écrit par le m pendant longtemps j'ai eu l'impression que la dépendance de l'induction de solomonoff ans le langage de programmation de référence était une faille sérieuse du concept et j'ai cherché un langage universel objectif libérer de tout arbitraire dû à un choix particulier de langage quand j'ai pensé avoir enfin trouvé un langage de la sorte je me suis rendu compte que je n'en voulais pas vraiment je n'en voyais aucune utilité notez que la citation originale parle de machines et pas de langage car solomonoff utilise de formalisme de turing est donc impérativement le langage considérer est le langage machine de la machine de turing universel de référence mais donc pourquoi n'y at il pas de quoi être préoccupé par la dépendance du rasoir d'occam en le langage de référence le choix du langage de référence ne fait-il pas une grande différence est alors oui ça fait une différence mais non ça ne fait pas une grosse différence comme solomonoff l'a démontré en 1960 et sa démonstration s'appuie sur l'existence de traducteurs entre langage turing complet démontré par tuerie en 1936 plus précisément pour tous les langages turine complet elle et m il existe un code s'est appelée compilateur pour traduire le langage elle dans le langage m ainsi pour toute théorie t on peut prendre une description t elle de la théorie t écrite dans le langage l on peut ensuite fournir cela au compilateur c le compilateur c va alors traduit rtl en un texte tm dans le langage m il va ensuite exécuté le texte tn mais alors le texte ctl est un texte dans le langage n qui est équivalent à t elle est créée dans le langage elle donc pour tout texte t elle dans le langage elle il existe un texte de longueur longueur de ctl qui est égal à longueur de 7 plus à longueur d t elle dans le langage m qui décrit la même chose que le texte t elle appelons maintenant qu'à elle de thé la plus courte description de la théorie tu es dans le langage elle et considérant que tu es l s est plus courte description en appliquant le principe de compilation que l'on vient de voir n'obtient ainsi un texte qui décrit la théorie tu es dans le langage n et dont la longueur est la longueur de ctl qui est égale à la longueur de ses puces à longueur de t elle est égale à kayes doté à savoir la plus courte description de la théorie tu es dans le langage elle met donc la plus courte description de la théorie tu es dans le langage m et forcément plus courte que ce qu'on a obtenu en appliquant le compilateur a t elle ce qui nous donne l'inégalité km2 t est inférieur ou égal à kl2 tu es plus la longueur du compilateur et de façon remarquable cette inégalité est vrai quelle que soit la théorie ou dit autrement la dépendance du rasoir d'occam au langage utilisé est au plus la longueur du compilateur l 2 c or il existe généralement des compilateurs assez court pour des langages classiques disent on peut être de l'ordre du ko pour ses puces plus où j'avais et même des compilateurs bien plus concis entre les machine de turing universelle les plus élémentaires du coup elles ne sont de plus courte longueur de descriptions de théorie est quasiment indépendante du langage utilisé cette mesure de complexité cas elle dotée pour le langage elle est souvent appelée la complexité de kolmogorov de la théorie tu es dans le langage elle mais à titre personnel je préfère l'appeler la complexité de solomonoff puisque kolmogorov qui a introduit cette notion en 1963 à lui-même reconnu que ce l'aumône of l'avait précédée dans la définition et études de ce terme et d'ailleurs ironie de l'histoire en 2003 selon l'ofs a reçu le prix kolmogorov pour son invention de la complexité de kolmogorov entre autres vu tout le prestige qui a déjà comme le goff qui a notamment posé les axiomes des probabilités décidément entre turing shouting martin 9 et d'autre lui aussi était enfant sur les probabilités et l'algorithmique prévu le prestige qui a déjà comme le core of je pense que ça ne serait pas trop heurter son héritage que de renommer sa définition de la complexité sous le nom de complexité de solomonoff bref la complexité de solomon of dépend du langage mais elle ne dépend pas trop du langage si bien considérer qu'il s'agit d'une mesure assez fondamental en tout cas si l'on considère des théories dont la complicité de saumoneaux fait de l'ordre du million le choix du langage de référence importera finalement assez peu surtout sachant le théorème d'incomplétude de solomonoff et ça on en parle dans la prochaine vidéo alors pour vous vidéo je vais contester un peu cette façon usuel de présenter l'induction de sylob 9 comme étant une formalisation du rasoir d'occam en fait le rasoir d'occam n'est pas vraiment un postulat nécessaires aux résultats de solomonoff il s'agit même davantage en quelque sorte d'une conséquence du formalisme de saumoneaux et même en fait d'une conséquence du bas et ziani sme je m'explique considérons des théories exprimable dans un langage données qui peut être même plus générale qu'un langage de programmation turing complet certaines théories seront exprimable en mille caractères d'autres en 1 millions de caractères et ainsi de suite pour tous nombre n de caractère implanté m l'ensemble des théories dont la plus courte description tien an men caractère a priori n'est pas nécessaire de donner plus de créer dans sa paix n quand elle prend des valeurs faibles a priori une envie de dire qu'on n'est pas obligé d'invoquer le rasoir d'occam cependant pour pouvoir appliquer le belize à nice mme il faut absolument que la somme des crédences a priori de toutes les théories soit égal à 1 et ça c'est un pic que la crédence des tories a un caractère plus la crédence des théories a deux caractères plus sacré dans des théories à trois caractères et ainsi de suite jusqu'à l'infini c'est à dire p2 t1 plus paix doté de puces p2 t3 plus p2 t4 est ainsi de suite ça ça doit être égal à 1 on alors une somme d'une infinité de nombre positif qui prend une valeur fini d'après la théorie des séries infinie ceci ne peut arriver que si la probabilité de l'ensemble tnt théorie am caractère tend vers zéro autrement dit de baïse à nice mme implique que les théories avec trop de caractère finissent nécessairement par devenir négligeable et de façon remarquable cette version du rasoir d'occam n'est ainsi pas un postulat du bail à nice mme il s'agit d'un théorème du bail à nice mme ce théorème de cannes dit que la crédence a priori de théorie trop longue et négligeable en fait pour les longues et ardues ring complet au moins on peut même affirmer quelque chose de plus fort que cela comme le nombre de théories am caractère est exponentielle en scène sacré danse a priori sera en moyenne exponentiellement faible entraîne les théories plus longue ne sont pas justes moins crédible elles sont même a priori exponentiellement moins crédible et là encore il ne s'agit pas d'un principe philosophique qu'il nous faut dogmatiquement accepté pour tous bayésiens il s'agit d'un théorème auquel les lois des probabilités le contraint à obéir tel fut l'une des plus puissantes et découvertes que j'ai eu le bonheur de faire avant même de suivre les pas de somme 9 lors de mon fantastique périple bayésiens mais la plus grande de toutes les jouissances au cours de ce périple inoubliable qui se poursuit encore ça restera assurément et de loin ma découverte des théorèmes de complétude et d'incomplétude de solomonoff et je trépigne d'impatience car on va justement enfin en parler la prochaine fois