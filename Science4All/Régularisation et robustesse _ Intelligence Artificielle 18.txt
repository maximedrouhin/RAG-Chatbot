Dans l'épisode 9 on a parlé de régression linéaire la régression linéaire c'est chercher à expliquer une variable Y à l'aide de d variables : X1, X2... jusqu'à Xd autrement dit, ça revient à déterminer comment ces variables se combinent pour causer Y et idéalement on aimerait ainsi pouvoir écrire quelque chose comme Y = bêta1 X1 + bêta2 X2+... et ainsi de suite jusqu'à bêta.d Xd intuitivement, si bêta1 est strictement positif et très grand, ça veut dire que X1 a une grande influence positive sur y et à l'inverse si bêta2 est nul ça veut dire qu'il n'a aucune influence sur Y Ainsi, déterminer l'expression  Y = bêta1 X1 + bêta2 X2...  et ainsi de suite jusqu'à bêta.d Xd c'est déterminer les causes de Y Alors je parle ici de causes pour faire simple, mais en fait la notion de causalité est très compliquée et subtile notamment à cause du paradoxe de Simpson, mais faisons simple pour aujourd'hui Ce qu'on aurait pu faire pour déterminer les coefficients bêta1, bêta2... jusqu'à bêta.d c'est partir d'une théorie plus fondamentale, et utiliser quelque chose comme le développement limité, dont on a parlé dans un épisode sur la relativité Mais pour déterminer des coefficients bêta1, bêta2... jusqu'à bêta.d  adéquats la régression linéaire, elle, nous invite à nous appuyer sur un jeu de données d'entraînement chaque donnée d'entraînement nous donne un ensemble de valeurs des variables X1... jusqu'à Xd et ainsi que la valeur de Y Et typiquement, la k-ième donnée va nous donner un vecteur de la forme Xk1, Xk2... et ainsi de suite jusqu'à Xkd, Yk qui correspond donc à un point dans un espace de dimension d+1 oui je vais un petit peu vite sur tout ça parce qu'on en a déjà parlé dans un épisode précédent Maintenant, on a parlé dans l'épisode 15 du théorème fondamental de l'apprentissage statistique qui disait en gros que pour éviter la surinterprétation, il fallait 100 fois plus de données que d'explications potentielles dans notre cas, ça veut dire qu'en gros, pour avoir une régression linéaire qui se généralisera bien aux données non encore observées, il faut avoir en gros d < n/100 Mais le problème, c'est qu'il arrive en fait souvent que d est très grand typiquement, si chaque donnée est un génome humain, et si l'on cherche à prédire l'addiction au chocolat à partir du génome alors le nombre de causes potentielles est chaque lettre du génome, ce qui nous fait des milliards de causes potentielles et là, on est embêtés parce qu'il n'y a que 7 milliards d'humains sur terre donc même en séquençant le génome de tous les humains, on ne pourrait pas avoir d < n/100 et on sera donc nécessairement en grave danger de surinterprétation De même, si on cherche à faire de l'analyse d'images automatisé comme par exemple reconnaître le chat de Tristan sur une photo on risquera alors de devoir traiter des images avec des millions de pixels sauf que je doute qu'il y ait des millions d'images du chat de Tristan Bref, en pratique on a même des cas où d > n, c'est à dire des cas où il y a plus de causes potentielles que de données on parle de cas "large p, small n" oui parce qu'en statistique, ils utilisent traditionnellement plutôt la lettre p que la lettre d Alors historiquement pour traiter de tels cas, on faisait du bidouillage de données brutes à la main typiquement l'analyse d'images était l'analyse d'un petit nombre de caractéristiques des images et on faisait ensuite la régression linéaire sur ces caractéristiques, pas sur les données brutes Ceci étant dit, peut-on néanmoins trouver une façon entièrement automatisée d'apprendre à partir de n données seulement, où chacune des d causes potentielles est vraiment prise en compte tout en évitant le danger de la surinterprétation, même lorsque n est plus grand, voire bien plus grand que d et bien l'astuce, c'est de ne pas s'autoriser toutes les explications possibles par des causes ou plus précisément, on va mettre dans le chapeau à explications uniquement des vecteurs explicatifs (bêta1,..., bêta d) pour lesquels il n'y a qu'une poignée de coefficients bêta.i non nuls autrement dit on va autoriser l'invocation des d causes mais on ne va s'autoriser que les combinaisons de c causes parmi ces d causes c'est à dire des vecteurs dont les coefficients bêta.i seront nuls à l'exception de c d'entre eux et la raison pour laquelle ces explications qui ne combinent uniquement que c causes évitent bel et bien la surinterprétation c'est qu'il y en a beaucoup moins que d'explications à d causes on dit parfois que le vecteur bêta doit être creux, ou "sparse" en anglais ou encore qu'il doit avoir une "norme zéro" inférieure à c bon, ça c'est un petit peu la philosophie derrière l'idée de régularisation mais le problème qui se pose en pratique c'est que cette norme zéro n'est pas pratique pour les calculs d'optimisation certes elle est facile à calculer, mais d'une certaine manière le chapeau des explications à norme zéro inférieure à c est un chapeau difforme qu'une main numérisés aura du mal à explorer adéquatement Formellement tout ceci a à voir avec la notion de convexité des espaces, et on en reparlera plus tard dans cette série Bref, pour des raisons d'efficacité algorithmique il est en fait utile de remplacer la norme zéro par une approximation facile à calculer typiquement la norme 1, ou la norme 2 surtout la norme 1 j'aime bien la norme 1 alors dans un cours de machine learning il faudrait que je m'arrête sur ces notions mais tout ce que vous avez besoin de retenir pour aujourd'hui c'est qu'il s'agit d'approximations de la norme zéro et que cette norme zéro compte le nombre de causes impliquées par le vecteur d'explication bêta Reste maintenant la question du choix de la valeur de c intuitivement,  c  est une mesure de complexité du chapeau d'explication en fait on est vraiment dans le cadre de l'épisode 13, où une trop grande valeur de c nous pousse à la surinterprétation d'où la question : comment choisir c ? Et bien la solution classique est la régularisation L'idée de la régularisation, c'est qu'au lieu de s'interdire la complexité, on va se contenter de pénaliser la complexité autrement dit, quelle que soit leur complexité, toutes les explications demeureront plausibles mais comme le dirait Carl Sagan, toute explication extraordinaire (c'est à dire une explication avec une grande valeur de c) requiert des preuves extraordinaires intuitivement, ça consiste à prendre notre chapeau et à mettre les explications trop complexes au fond du chapeau et à privilégier les explications en haut du chapeau Autrement dit, l'astuce majeure de la régularisation c'est de prendre des chapeaux dont toutes les explications ne sont pas initialement au même niveau certaines explications seront alors plus crédibles a priori, et à niveau explicatif égal seront donc privilégiées Cette astuce a vraiment révolutionné le machine learning puisque d'une certaine manière il permet d'ajuster le niveau d'interprétation en fonction non seulement de la quantité de données dont on dispose mais aussi et surtout en fonction de la complexité même des données intuitivement, si aucune explication simple n'explique bien les données et s'il y a une explication un peu plus compliquée mais qui colle parfaitement aux données il sera alors justifiable d'invoquer cette explication un peu plus compliquée et ça, soit dit en passant, c'est doublement bayésien comme principe et oui on en parlera mais pas aujourd'hui Donc intuitivement, d'une certaine manière la régularisation permet d'automatiser efficacement la validation croisée mais il y a une autre explication de l'efficacité de la régularisation dans l'épisode 12, on a vu que la variance de l'apprentissage d'un jeu de données à l'autre était vraiment la cause de la surinterprétation Et bien justement, intuitivement en favorisant les explications qui ne s'écartent pas trop de l'explication de base qui consiste à dire que rien ne cause Y on évite trop de variance dans l'apprentissage, et donc dans les prédictions de cet apprentissage l'apprentissage est intuitivement robuste à des variations des données d'entraînement et ce qui est amusant c'est qu'on a découvert que la régularisation est mathématiquement équivalente à d'autres approches qui s' attaquaient directement à cette robustesse Par exemple une solution pour garantir cette robustesse consiste à apprendre non seulement des données collectées mais aussi de données fictives obtenues en perturbant légèrement les données collectées intuitivement, un bon apprentissage ne devrait pas être bouleversé par ces petites perturbations cette approche est appelée l'augmentation des données de manière plus formelle, c'est aussi équivalent à supposer que les données collectées sont des données imprécises qui ne sont donc que des approximations des "vraies données" c'est-à-dire des données qui auraient été obtenues si les expérimentateurs n'avaient pas commis d'erreur de mesure Du coup, plutôt que d'apprendre des données collectées, l'optimisation dite "robuste" consiste à considérer que les vraies données sont quelque part à côté des données collectées et qu'il faut que nos explications soient valides quelle que soit la position exacte des vraies données Formellement ça revient à introduire un ensemble, dit d'incertitude, autour de la données collectée et à chercher des explications qui collent le mieux au pire cas autrement dit, on va chercher le modèle qui maximise l'explication de la pire valeur de la vraie donnée possible sachant que cette donnée est dans l'ensemble d'incertitude et bien la théorie de la dualité en optimisation, à coup de lagrangiens et de points-selles permet de montrer que l'optimisation robuste est en fait équivalente à la régularisation Autrement dit, non seulement la régularisation permet d'automatiser la validation croisée et d'explorer ainsi des explications complexes dans les situations où les explications simples sont insuffisantes mais elle permet aussi de lutter contre la variance de l'apprentissage et d'anticiper le fait que les données d'entraînement sont des données collectées et qu'elles ont donc de bonnes chances de ne pas être exactement des mesures du phénomène étudié tout ça fait qu'aujourd'hui, avec la sagesse des forêts, la régularisation est devenue la solution privilégiée des praticiens du machine learning pour lutter contre la surinterprétation La dernière fois dans les commentaires j'étais revenu sur le paradoxe Simpson et je vous avais proposé un autre problème où il fallait trouver un facteur de confusion à savoir le problème où on avait des fumeurs qui avaient un taux de survie plus grand que des non fumeurs sur une période de 20 ans et je vous avais demandé de trouver des facteurs de confusion de ce phénomène et vous avez été très très imaginatifs et très créatifs dans les commentaires à trouver plein de bonnes idées pour pouvoir expliquer ce phénomène par exemple, Syl L propose le fait qu'il y a peut-être des différences de classe économique entre les fumeurs et les non fumeurs et peut-être que du coup les fumeurs sont d'une classe socio économique plus élevée et du coup ils ont plus accès à toutes sortes de biens de santé Sabzy Gobi imagine que les gens qui fument font du coup plus attention à leur santé puisqu'ils savent qu'ils sont en plus grand risque, ils ont plus grand facteur de risque et du coup en faisant attention à leur santé en fait ils améliorent davantage leur santé que les gens qui ne fument pas et qui du coup ne feraient pas attention à leur santé D'ailleurs ça c'est une explication de Risque Alpha dans sa vidéo sur l'obésité pour expliquer pourquoi est-ce que les gens qui sont en surpoids ont en fait un meilleur taux de survie que des gens qui sont dans le poids idéal ceci rejoint d'ailleurs la remarque d'Hannibal Ateam qui suggère le fait que les fumeurs sont davantage suivis médicalement Enfin Cauchy Schwarzy, Nico James Buchanan, Arthur Rauw, Grégory Grandjean et Hl037 ont mentionné le facteur de confusion qui est l'âge et c'est celui qui apparaît en tout cas dans la vidéo de Ted-Ed, que je vous invite à aller voir en fait je vous invite à ne pas aller la voir pour l'instant je vais poser un autre problème de facteurs de confusion puisque vous avez l'air d'aimer ça Donc il y a des études aux Etats-unis qui montrent par exemple que les noirs et les blancs ont le même taux de sentence à mort lors des procès du coup ça laisse suggérer qu'en fait il n'y a pas de discrimination devant la loi entre les blancs et les noirs mais est-ce vraiment le cas, est-ce qu'il n'y aura pas un facteur de confusion qui fait qu'on a ce résultat malgré l'existence d'une discrimination entre les blancs et les noirs et vous voyez que tout de suite quand ça parle de trucs un peu plus "touchy" là tout de suite c'est un peu plus difficile de prendre un grand coup de respiration et d'essayer de chercher ces facteurs de confusion ce qui est vraiment la difficulté du paradoxe de Simpson puisque je vois beaucoup de gens dire que le cas du paradoxe de Simpson était évident dans le cas du match de football entre Kanté et Rabio mais en fait quand on parle de politique ce genre de trucs arrive tout le temps il faut vraiment avoir le réflexe de se dire : il y a des facteurs de confusion qui sont toujours là, ils sont toujours cachés et je dirais même que même lorsqu'il n'y a pas de facteurs de confusion évidents on pourra jamais être sûrs du fait qu'il n'y en a pas, et du coup ne pourra jamais faire pleinement confiance à des statistiques globales et ça, ça me permet d'insister sur l'intérêt notamment des expériences scientifiques contrôlées puisque l'intérêt de ces expériences est justement de contrôler ces facteurs de confusion et de savoir vraiment quel est l'effet de ce qui est testé et je vous renvoie vers la vidéo de "La statistique expliquée à mon chat" notamment pour mieux comprendre tout ça Yohan Drht et sokudohi renvoient à la vidéo de Dirty Biology sur la sagesse de youtube où il parle notamment du fait que la diversité permettrait de faire de meilleures prédictions alors j'ai des petits bémols à mettre à cette vidéo de Léo qui renvoie notamment à un théorème appelé le théorème de la diversité des prédictions alors à ma connaissance, ce théorème a été purement nommé ainsi, et étudié ainsi par le chercheur Scott Page et... voilà, je vais être un petit peu dur avec Scott Page, mais je pense que cette interprétation en fait c'est l'interprétation d'une autre équation, qui est le dilemme biais-variance dont on parlé dans l'épisode 12 je pense que cette interprétation est complètement fallacieuse tout simplement parce que lorsqu'on regarde l'équation qui est présentée par Dirty Biology lorsqu'on augmente la diversité dans une population, et bien on augmente aussi l'erreur individuelle moyenne de la même quantité, même, on peut prouver ça mathématiquement si on augmente le même bruit dans la diversité des prédictions, de façon complètement mécanique et bien il va avoir le même bruit qui est rajouté à l'erreur individuelle moyenne si bien que l'erreur collective reste complètement inchangée et ça c'est vraiment l'équation biais-variance dont on a déjà parlé dans l'épisode 12 avec Gilles de la chaîne Heu?rêka donc je dis ça, pas du tout pour critiquer Léo, puisque de toute façon il a bien fait son travail j'ai envie de dire il a cité ses sources et la vidéo est très bien mais l'explication qu'il donne est, à mon sens, pas du tout la bonne et tout ça vient d'une interprétation fallacieuse de Scott Page de cette équation En particulier, pour répondre à la question de Pierre Stöber : non, la sagesse des forêts n'est à mon sens pas du tout un argument en faveur de la démocratie En fait de façon plus rigoureuse, ce qui marche vraiment bien dans cette sagesse des forêts c'est si on applique quelque chose comme Ada Boost ou l'algorithme par "multiplicative weigths update" c'est à dire si non seulement on prend en compte une certaine diversité mais on fait quand même gaffe à donner plus d'importance à des arbres qui, non seulement expliquent bien les données, mais en plus de cela sont relativement simples En gros la diversité est importante, mais il faut quand même une sélection des arbres les plus performants ou plus généralement des avis les plus informés si on s'intéresse à des questions scientifiques Bref, la diversité aide mais ce n'est pas une condition suffisante il faut aussi quelque part avoir ce travail sur les pondérations, en fait, des différents arbres de la forêt et en fait il faut vraiment faire, en pratique, un vote pondéré et donner plus d'importance, non pas à des arbres aléatoires, mais plutôt à des arbres qui déjà sont pas si mal pour expliquer les données observées Nassim Augsburger nous demande si les arbres ne devraient pas appliquer le scrutin de Condorcet randomisé lorsqu'ils vont aller aux urnes bon, la réponse en fait est que, généralement il n'y a pas trop besoin, parce qu'en fait le scrutin de Condorcet randomisé, il a vraiment cette bonne propriété d'être robuste au dilemme du vote utile et si on suppose que les arbres votent de façon honnête en particulier vu qu'on contrôle la manière dont ils vont voter, qui correspond en gros à leur score, à leur capacité à expliquer les données observées vu qu'on contrôle ça et qu'on connaît toutes les données, on n'a pas besoin d'avoir cette robustesse au dilemme du vote utile et à ce moment là je pense qu'une votation par score ou par note, c'est quelque chose qui marche en fait beaucoup beaucoup mieux et en fait la version vraiment ultime de tout ça c'est, je pense en tout cas, une approche purement bayésienne où la prédiction de la forêt est une moyenne pondérée, qui est donnée par la loi des probabilités totales Encore une fois il y aurait beaucoup plus à dire sur ce sujet, mais  je laisse ça pour une autre fois Ocelot se pose la question de l'interprétabilité de ces forêts puisqu'en fait une des grosses difficultés de ces arbres, vu qu'ils nous donnent pleins d'explications, chaque arbre nous donne une explication qui est en fait incompatible avec les autres arbres c'est un peu bizarre d'utiliser ça comme des explications, et du coup c'est également très difficile à interpréter, tout ça Et bien, ma réponse à tout ça, ça serait surtout que peut-être que ce qu'on appelle explication de façon usuelle c'est pas forcément une bonne façon de penser l'épistémologie de savoir qu'est-ce qu'on sait et qu'est ce qu'on sait pas Finalement est-ce qu'on serait pas en train de chercher finalement des modèles prédictifs, à chaque fois pour essayer de prédire à chaque fois un maximum les choses qui nous arrivent et finalement tout ça pose vraiment en fait la question : qu'est-ce que ça signifie vraiment qu'expliquer quelque chose ou que comprendre quelque chose ? est-ce qu'avoir "l'explication" de quelque chose ça a un sens ? ou est-ce qu'on ne serait pas tout simplement de petites machines très limitées dont la complexité de Solomonoff est très inférieure à la complexité de Solomonoff de l'univers et du coup on ne peut de toute façon que se contenter d'algorithmes de compréhension qui sont à la fois incroyablement rapides et incroyablement simples à décrire et que du coup bah on n'est pas capable de décrire non seulement le fonctionnement de toute une forêt, mais également le fonctionnement de tout notre univers je pencherais plutôt vers cette option et du coup ça veut dire que la seule chose qui soit vraiment d'intérêt vis-à-vis de notre description du monde c'est d'en avoir des descriptions approximatives utiles : # tous les modèles sont faux, certains sont utiles ce hashtag il sous-entend aussi qu'il n'y a pas grand chose au delà des modèles, voire rien si ce n'est peut-être les données qui nous sont fournies pour ajuster nos modèles Enfin, Rémi Peyre nous demande si il n'y aurait pas une façon de concilier ces idées de sagesse des forêts avec des théories plus classiques de l'apprentissage notamment cette histoire de dimension VC, si vous y réfléchissez en fait c'est une façon de réduire toute la complexité d'un chapeau à un seul nombre, et c'est peut-être un petit peu trop simplificateur et que peut-être que pour bien parler des chapeaux à explications il faut avoir une notion beaucoup plus sophistiquée que juste sa dimension VC et typiquement dans le cas d'une forêt il y a beaucoup plus que la simple complexité VC de la forêt qui importe il y a la manière dont notamment elle évolue et elle s'adapte aux données au fur et à mesure notamment cette idée que à chaque fois qu'on donne une nouvelle donnée à la forêt alors la forêt va s'ajuster un petit peu à cette donnée, mais la forêt va a priori pas trop bouger parce que c'est toute une grosse structure intuitivement et en tout cas elle va pas complètement être bouleversée, et c'est je pense vraiment ça qui fait la sagesse de la forêt c'est le fait que toute donnée individuelle rajoutée ou modifiée ne va pas bouleverser complètement la forêt et ça, ça revient à aussi étudier non seulement le l'ensemble des hypothèses considérées mais aussi le mécanisme d'apprentissage c'est quelque chose qui je pense est pas forcément souvent étudié par les théories classiques de l'apprentissage il y a d'autres théories par contre qui étudient ce genre de choses et notamment il y a tout le champ de l'apprentissage en temps réel, ou "online learning" en anglais et je vous renvoie notamment vers les vidéos que j'ai faites sur Wandida sur le "Multiplicative Weigths Update Algorithm" qui est typiquement une façon de mesurer la capacité d'apprentissage  du "multiplicative weigths update algorithm" par "multiplicative weigths update" et ce qui est intéressant c'est que dans cette analyse en fait on a une analyse cumulative, dynamique de la manière dont l'apprentissage se fait et je pense que c'est peut-être plus pertinent que les approches un peu classiques le saint-graal vraiment de tout ça étant je pense notamment le théorème de complétude de Solomonoff pour la formule de Bayes, mais ça on en parlera mais pas aujourd'hui Hey, j'espère que vous avez aimé cet épisode qui clôt une mini-série dans cette maxi série et qui parlait d'overfiting et du problème de la surinterprétation, qui est vraiment un problème fondamental je pense qu'on ne peut pas comprendre l'intérêt par exemple de la sagesse des forêts sans prendre en compte ces idées de surinterprétation puisque, voilà, si il n'y avait pas de problème de surinterprétation il n'y aurait pas d'intérêt je pense à avoir une forêt d'explications ou à avoir ces idées de régularisation À partir de la prochaine vidéo, et pour quelques vidéos, on va parler du problème de la dimensionnalité on parle parfois du fléau de la dimensionnalité, ou encore de la malédiction de la dimensionnalité qui est le fait que, en fait en machine learning on s'intéresse souvent à des données qui sont de très très très grande dimension et que les espaces de très très très grande dimension sont aussi des espaces très très très bizarres et il faut absolument les simplifier d'une certaine manière en particulier la prochaine fois on va parler des hyperboules et des hypersphères ces boules, maxiboules de très grande dimension qui sont en fait incroyablement contre-intuitives et étranges et j'ai pas mal découvert notamment très récemment en m'intéressant à ce sujet, et ça beaucoup d'applications en machine learning si vous avez aimé cet épisode pensez à le liker, à le commenter, à le partager pensez à vous abonner pour ne pas manquer les futurs épisodes, merci aux tipeurs pour leurs dons et j'espère que vous serez là la prochaine fois