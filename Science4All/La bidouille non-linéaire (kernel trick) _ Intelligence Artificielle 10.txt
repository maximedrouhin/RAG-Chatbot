la dalle enfants à parler de régression et de classification linéaire pour résoudre le problème de l'apprentissage superviser en gros l'idée c'est que l'on à un nuage de pointe dans un espace de phil jones et chaque point est associé un live ball par exemple like dislike elles ont cherché à décrire ce nuage au point à l'aide d'une description simple fait de droite ou si l'on est en dimensions supérieures à l'aide d'hyper plan est la raison pour laquelle on adore ses approche linéaire c'est parce qu'ils ont de supers propriété en particulier elles peuvent être calculés efficacement par des algorithmes comme la méthode des moindres carrés la régression logistique svm ou encore le percepteur y compris quand l'espace des phisheurs et de grandes dimensions oui j'ai pas parlé du percepteur la dernière fois mon gros ça fait la même chose que la régression logistique ou svr mais il ya un mais le problème du machine learning linéaire c'est qu'ils ne marche que si les données ont une structure linéaire c'est à dire c'est le coefficient de régression air carré épreuve terrain dans le cas de la régression linéaire ou si les level sont séparables par un hyper plan dans le cas de la classification linéaire or ça en général assez pas le cas voici par exemple les nombres d'abonnés de 115 chaînes youtube scientifiques les données datent d'août 2010 est donc elles sont pas tout à fait à jour mais la lueur d'un combat sans doute pas trop changé depuis vous voulez que cette courbe n'a vraiment pas une structure linéaire en fait comme on aurait pu s'y attendre elle suit à peu près la loi de ce type c'est à dire que c'est x et le classement de la chaîne alors son nombre d'abonnés va être y à peu près égale à 1,5 millions / x et le problème c'est que pour trouver une relation comme y était bien là c / x bats avec une régression linéaire c'est mort tout ça parce que cette relation n'est pas une relation linéaire entre eux y est x oui je rappelle qu'une relation linéaire c'est y est égal à ax puce b mais du coup y'a t'il quand même en moyens d'utiliser la puissance de la régression linéaire pour déterminer une relation entre x et y la réponse est oui mais pour y arriver va falloir un peu faire une bidouille de physiciens justement notre physicien préféré de youtube david de la chaîne sur ses taux nantes a justement bidouiller les données pour faire ressortir la loi de ziff en tant que relation linéaire l'astuce c'est de bidouiller y en a un autre les bols qu'on va appeler grant y ait de bidouiller x en un autre fischer disons grand x et en trouvant la bonne bidouille qui dans son carré la fonction logarithme on se rend compte que la loi de ziff y est égale ac / serait écrit comme étant quand y est égal à constante - 30x magie magie en effet lorsque l'on trace loeb y en fonction de l'oc ii x vous n'obtient quelque chose qui ressemble à une droite bon c'est pas tip top on va dire que ça marche alors la bidouille il matheux puristes ils vont pas aimer ça et pareil si vous sacraliser les sciences et la méthode scientifique pas désolé du spoiler mais les sciences c'est avant tout un énorme tas de bidules c'est grâce à de telles vidéos et que les physiciens ont déterminé l'équation de goldbrook white que les biologistes ont déterminé la lois de kepler que les économistes ont déterminé l'équation de coop douglas et que l'om sera choisi d'utiliser lé indice de masse corporelle on trouve ça partout en finance et dans le milieu on appelle ça la méthode du gué steam est et si vous êtes en mode grove nerfs parce que c'est pas logique je vous renvoie vers l'épisode 6 la science et la quête de théories émergents pas de théorie véritablement vrai fondée sur des mathématiques élégante et pour faire ça la bidouille c'est pas beau mais ça marche en particulier ça permet d'exploiter les algorithmes efficace de la régression linéaire et de la classification linéaire centre de travail et les applications sont stupéfiantes par exemple considérant le caducc sort que j'ai mentionné la semaine dernière louxor n'est pas linéairement séparables autrement dit n'existe pas de droite qui séparent bien les like des dislike comment peut-on utiliser la puissance de la classification linéaire pour néanmoins bien apprendre louxor alors avant de traiter ce cas sophistiqués je vous propose une version plus simple de ce problème en dimensions 1 imaginons coordination un et qu'on a des dislike juste quand hicks et entre -1 et 1 comment séparer linéairement des like des dislike et bien l'astuce c'est l'habitude on va poser grand x est égal à x carré et on va faire la séparation linéaire pour grant x plairait c'est tout bête la séparation linéaire est tout simplement le point de rupture 30 x est égal à un cantique c'est inférieur à 1 n'a dé dislike et quantix est supérieur à 1 on a des laïcs alors il une façon plus élégante une façon géométrique de mieux visualiser la chose pour ce faire on reprend notre dimension du petit x initial est ce qu'on va faire ce qu on va rajouter une dimension qui va être égal à x car et l'espace des feature est alors de dimension 2 sauf que cette fois on peut facilement séparer les laïcs étaient dislike à l'est une droite joli nom de la même façon pour séparer les likes des dislike et dans le corps ce qu'on veut faire c'est rajouter une dimension supplémentaire avec des transformations non linéaire défi tu as initiaux typiquement il peut rajouter une dimension linéaire pour obtenir un espace de dimension 3 de sorte que les données se trouve maintenant sur une espèce de selle de cheval auquel cas on pourra séparer les like des dislike à l'aide d'un hyper plan horizontal cette astuce documentation de l'espace des futures à l'aide de bidules non linéaire et super utile en pratique si votre régression ou classification linéaire ne marche pas et bien rajouter tout bêtement d'autres fishers rajoutant des termes comme x car et xo cub depuis 106 x ou encore locke de x et tenter à nouveau votre régression ou votre classification nyha c'est une façon très facile de mieux expliquer les données en fait on peut même aller plus loin et rajouter un nombre infini de dimension oui vous avez bien entendu un nombre infini de dimension alors pour que ça marche bien il faut techniquement que cet espace infini soit un espace de gilberte c'est à dire en particulier qui dispose d'un produit scalaires dit comme ça c'est pas super rassurant puisque une fine et on veut que les calculs puisse être fait par un ordinateur et les ordinateurs et l'infini en général ça fait pas bon ménage forte heureusement on peut démontrer que la séparation optimale sera orthogonale à l'espace engendrés par les données ce qui signifie qu'elle sera identifié par un nombre fini de nombre en fait autant de nombreux qu'il ya de tonner nous encore le calcul de cette séparation optimale pourra se faire en un temps polynomiale ans le nombre de données et tout ce dont on aura besoin c'est de la fonction qui étant donné de feature x et exprime calcule le produit scalaires entre les bidules non linéaire 2x et exprime dans l'espace de gilberte cette astuce remarquable et ce que l'on appelle le coeur numérique elle est très utilisée en pratique bon je sens que j'en ai perdu plus d'un dans cette explication si j'ai pas tout suivi ses parts à tout ce que je voulais avoir dit c'est que c'est utile en pratique de faire beaucoup de bidouille dans des espaces de gilberte de dimension infinie surtout si c'est pour trouver une classification linéaire et je vous renvoie vers les vidéos de wanted a à ce sujet pour plus de détails techniques à ces sujets alors tout ce que je raconte la peut sembler être une tentative désespérée quoique pas si moche de sauver les algorithmes de classification ligne mais si c'était juste ça je vous en aura pas parler en fait cette bidouille non linéaire c'est aussi et surtout ce que font les meilleurs algorithmes de machine learning d'aujourd'hui oui je parle notamment des algues au rythme du diplôme ning en gros le diplôme ning s'est enchaîné plein de classification linéaire pour bidouiller l'espace des features de manière non linéaire oui parce que souvenez vous conteront fait de la classification linéaire on a une étape de couture à la fin aussi appelé fonction d'activation qui est une opération de non linéaire le diplôme ning bidouille donc les features de manière non linéaire pour keane fine et les features bidouiller deviennent linéaments séparables par la dernière couche du diplo ning autrement dit le diplôme hing c'est automatiser la bidouille non linéaire encore une fois si ça vous dépasse ne vous inquiétez pas reparlera de tout ça plus longuement un autre truc assez cool pour visualiser ce qui se passe c'est de prendre la séparation linéaire des features bidouiller et de projeter cette séparation dans l'espace des phisheurs initiaux on obtient alors une séparation non linéaire de l'espace qui par exemple dans le cas du luxe hors correspond à un découpage de l'espace en plusieurs morceaux en fait de façon générale si on s'autorise toutes les bidules nominé à imaginables notamment en passant par des espaces de gilberte de dimension infinie on finira toujours par réussir à séparer les like des dislike et ça c'est cool c'est aussi embêtant après tout si j'ai une erreur de mesure et en science empirique il ya toujours énormément d'erreurs de mesures je suis certain que quelque part dans ce processus je vais mal conclure à cause de cette erreur de mesure pareil si les figures sont théoriquement insuffisant pour des prédictions déterministe et les bosses ce qui en fait est toujours le cas vu qu'on cherche uniquement des théories émergentes bref il semble en fait plus pertinent de s'accorder le droit de ne pas parfaitement séparer les like des dix fake en considérant que c'est normal que notre modèle très incomplet soit justement incomplet et en effet vouloir à tout prix séparer les laïcs et des dislike c'est une erreur de base d'une machine learning to come de l'apprentissage en général que l'on appelle la surinterprétation ou lover fitting mais ça on en reparle la semaine prochaine un harfang n'a parlé de récréation et de classification linéaire il vous avez posé le problème du rrq sort gabriel frais et benoît avril avait proposé d'utiliser de droite de célébration plutôt qu'une le problème c'est les questions cherche à faire ça tel quel on n'a pas gagné au rythme simple de calcul de ces deux droites de séparation est en fait de façon générale cherché à trouver ces deux droites correspond à résoudre un problème d'optimisation on appelle non convexe bref tout ça pour dire que c'est un problème qui a de bonnes chances de ne pas être résolus de façon très générale par des séries qu'au rythme api un autre problème avec utilisation de deux droites plutôt qu'une c'est que bon ben ok pour le coeur en effet de droite c'était une bonne idée mais si on prend d'autres structures d'autres aux répartitions comment est ce qu'on fait pour choisir un nombre de droits de séparation comment est ce qu'on les place bref en fait on s'attarde or sur le point tu duc sort uniquement fait justement parce qu'on appelle de lover fitting surinterprétation c'est à dire qu'on cherche pas uniquement ajoutons ce problème alors que le problème du machine tournait bien plus générale est qu'il faut être capable d'adresser toutes sortes de données vous avez aussi demandé de réfléchir à la question à ce qu'il valait mieux la méthode des moindres des nations de la place et de le post kobe tu vois ce qu'il valait mieux à la méthode des moindres carrés de gosse et legendre et luc morin alors y fait remarquer que la méthode des moindres carrés à la volt ou bien se prêter au calcul est en effet les calculs notamment en dimensions un an dimension 2 de la méthode des moindres car elles sont beaucoup plus facile que la méthode des moindres déviation de la place et moscovitch ceci dit ce n'est plus vraiment voit déjà en dimensions supérieures notamment la méthode des moindres qui a ré demande à inverser une matrice est un calcul inverse d'une matrice en ce moment des grandes matrice à la main c'est quand même pas facile pour pouvoir sur avec des ordinateurs ça se fait très très très bien en fait les deux approches même toit des moindres carrés et la méthode de boscovich et la place des moindre déviation sont toutes les deux des approches qui sont en fait quelques aveux très rapidement par des ordinateurs vous dis que ce sont les algorithmes qui ont du temps de calcul paulino bio en la dimension de l'espace --en le nombre de données donc en effet avant l'avènement des ordinateurs et méthode des moindres des carrés et est beaucoup plus pratique et ça explique en grande partie pourquoi elle était utilisée plutôt que la méthode du moindre déviation ensuite luc morin allait faire marrer cas beaucoup de faiblesses en justifications théoriques et de la méthode des moindres carrés avant de vous en parler il faut que je vous parle du théorème centrale limite de la place en gros ce que ses théorèmes 10 et que d'une certaine manière si une erreur est une somme d'un nombre infini de petites perturbations aléatoire et indépendante publiquement si vous faites une mesure maya plein de petites erreurs dans la mesure différentes étapes de la mesure qui s'ajoute qu'ils font une erreur global le théorème centre halimi s'est dit que quand il ya plein de telles petites perturbations infinitésimale et indépendante alors les erreurs seront distribués selon une loi normale on peut aussi la loi haïtienne j'ai pas pourquoi on l'appelle côté nord la place avait tout inventé excusez mon organisme la place tien est ce qu'il y avait que cette distribution gaussienne sait que sa fonction de densité s'écrit bla bla bla exponentielle - bla bla bla erreur ou carré on suppose maintenant que les élans sont indépendantes est identiquement distribués alors la probabilité d'avoir toutes les erreurs qu'on a eus va être égal au produit des probabilités d'erreur c'est à dire en produit des bla bla bla exponentielle - bla bla bla erreur au carré et sachant que le produit d'expérience y est l'été galal exponentielle de la somme ça ça nous fait blablabla fois exponentielle de moins bla bla bla fois la somme des carrés des erreurs ensuite ce que propose de faire une place c'est de choisir les paramètres de la régression linéaire qui rendent les erreurs les plus crédible possible est plus probable qu'ils soient c'est ce qu'on appelle la méthode de la vraisemblance on va maximiser la vraisemblance des données sachant les paramètres en fait maximiser cette expression ça revient à minimiser la somme des cas et comme il ya beaucoup d'erreurs qui sont en fait une simulation de petites erreurs qui suivent du coup l'album gaussienne on en déduit que la méthode des moindres carrés est compatible avec l'hypothèse du cac sur laquelle les erreurs sont distribués chaunois dossier voilà comme l'expliquent paradoxe la justification de la place de la méthode des moindres carrés et via la distribution gaussienne des erreurs mais tout ça suppose bien sûr que les erreurs sont distribués selon une caution bon faut aussi admettre qu'il faut utiliser le maximum de bresson d'anciens hésite - payet c'est là que moi mais qui reviendra une autre fois en particulier en pratique les erreurs ne suivent pas nécessairement une loi gaussienne en particulier un truc qui peut arriver assez souvent c'est que justin est rare qui a été complètement mal fait voilà peut-être que l'expérience qu'il fallait faire ces tests et c'est pas un truc sur l'acide sulfurique et peut-être qu'à une expérience ou n'a oublié de mettre l'acide sulfurique et du cou et des résultats complètement aberrant à ce moment là la distribution des erreurs avaient pas une gaussienne mais c'est peut-être en général une gaussienne bêtes et on n'entend on pas voir des trucs complètement absurde si à ce moment là les erreurs ne suivent pas cette loi sociale alors la méthode des moindres car il peut donner des trucs complètement aberrant et ça en pratique imaginer qu'aujourd'hui on automatise énormément les procédures d analyse de données ça alors le truc ça arrive tout le temps on parle typiquement de donner aux flyers l'intérieur c'est une donnée qui vont vers elle et la n va pas forcément envie de l'expliquer et comme le fait remarquer plus tôt au rwanda chaillet le problème de la méthode des moindres carrés c'est qu'elle va donner beaucoup plus d'importance les points qui sont très très éloigné des autres points elle va donner énormément d'importance ou servent d'ailleurs et ça en pratique c'est un énorme défaut de la méthode des moindres car il est du coup là on peut lui préférer la méthode de posco beach la place parce que celle ci sera beaucoup plus robuste elle ne donnera pas plus d'importance est un point très éloignées qui est un point qui a un peu éloigné à avoir avec le fait que la dérive et d'un car et c'est proportionnel au nombre lui-même alors que la dérivée d'une valeur absolue toujours un vecteur unitaire qui dépend pas du coup de la distance à laquelle se trouve la donne est pour ça que je parle de tout ça bien plus longuement pour expliquer cette journée on dit que la méthode de boscovich la place et robuste aux erreurs ou encore qu'est l'art brighton de 0,5 zeller que typiquement c'est quelqu'un de malveillant peut modifier quelques unes de vos données il peut ainsi essayer de le bordel dans votre analyse de données non si vous utilisez la méthode des moindres car ils vous serez très vulnérables à ce genre d'attaqués mais si vous utilisez la méthode de boscovich en place et à des moindre déviation alors vous serez robuste à ce genre d'attaqués en effet la pouvoir modifier un petit peu votre analyse hédonique mais votre analyse de données ne sera pas énormément perturbé elle sera encore assez représentative des données non modifiés tout ça est en fait extrêmement similaire à la différence contre la moyenne et la médiane la moyenne en fait c'est la méthode des moindres carrés quand il ya aucun fishers qu'on cherche à expliquer à donner sans aucun fil d'or je cherche un résumé avec 1 seconde la moyenne c'est ce qui va mobiliser la somme des carrés des erreurs en disant que la moyenne représente a donné par que le média et ceux qui minimisent la somme des déviations et la médiane est beaucoup plus robuste c'est à dire que si on a une distribution et comprend un point qui est déjà au dessus de la médiane et que l'on envoie jusqu'à l'infini on exagère on fait n'importe quoi avec alors les médias ne va pas du tout bougé bref tout ça pour dire que la méthode de post coït sa place en fait en pratique c'est pas mal mais ce n'est pas suffisant notamment pour le baliser un que je suis comme luc morin alors if le fait remarquer peut-être que plus x est grand plus les variations de y sont grandes et à ce moment là ce qu'on aimerait c'est que la régression linéaire prennent en compte ce fait et on ne cherche pas forcément à coller aux données avec des grandes valeurs de x puisqu'on sait que ces grandes bird et qui sont allés à des grandes fluctuations du coup on peut pas faire confiance à chacune des données individuelles alors que peut-être que les petites valeurs de xl pour mort sur le coup plus fiables à ce moment là y'a pleins d'astuces pour essayer de mieux coller à cette structure des données bref pouvait qu'en général le projet de régression linéaire est en fait beaucoup beaucoup plus compliquée que ce qui est pas fonctionné à l'école et on pourrait même considérer que c'est un problème encore non résolues en tout cas donc il n'ya pas de solution satisfaisant sauf si on est bayésiens alors ne pas pouvoir prendre le temps donc bien pu s'expliquer ce que ça signifie de vraiment être bayésiens mais un aspect important du paysagisme c'est le fait qu' on ne fait jamais confiance à un seul modèle à la fois qui en fait utile de faire confiance à plusieurs modèles incompatible à la fois en particulier on y gagne et souvent à faire de prédictions qui est une combinaison de plusieurs modèles à la fois et ça dès lors on reviendra dessus dans cette série en particulier pour un malaisien de prédiction sera fiable que si c'est la même prédiction qui est fait par tous les modèles de régression linéaire dont je vous ai parlé et ça c'est génial parce que ça permet aussi aux plaisirs de mesurer l'étendue de son ignorance s'il différents modèles prédisent des valeurs différentes alors le parisien va pouvoir conclure qu'il ya beaucoup d'incertitudes à avoir quant à toute prédiction de l'evole mais bon comme vous l'imaginez il y aurait encore beaucoup plus à dire sur le bail zenit aurait un perdriau et commentaires signalent le fait qu'ils ont un petit peu lâché dans la vidéo précédente j'imagine aussi ça tu as dur aussi pour cette vidéo si vous n'avez pas tout compris c'est plus que normal notamment ou simplement parce que j'ai donné aucune définition rigoureuse et juste de donner une idée vidéos précédentes notamment de ce qui était une corrélation de ce que pouvait être des variations de giteau n'a aucune définition au format note le conseil de base que j'ai à vous donner c'est de regarder les vidéos plusieurs fois ça aide d'intérioriser au fur à mesure toutes les étapes des raisonnements des vidéos n'hésitez pas à mettre pause notamment de prendre le temps de réfléchir à donner mais enfin si vous n'avez pas tout compris c'est pas grave du tout les trucs importants vraiment retenir en fait ces trucs très basique très important à retenir c'est à quoi ressemble le problème de l'apprentissage supervisé qui elle est en gros la philosophie de la régression linéaire au tracé d'une droite effet réflexion avec la droite ou de la classification ine a tracé un hyper pente vers la traduction de quel côté de l'hyper plan mené il faut aussi retenir que les calculs sont très rapides à faire par un ordinateur mais que le problème c'est que parfois cette régression linéaire cette case fille solitaire ne marche pas ce qu'on a vu aujourd'hui c'est une façon de faire en sorte que ça marche tout le temps et ce qui va être encore plus intéressant dans les vidéos à venir c'est que ce n'est pas forcément une bonne chose de faire en sorte que cette séparation linéaire typique où on va vraiment marché qui en fait souhaitable de faire des erreurs du moins d'inclure le fait qu'il y à des erreurs de données enfin didier muller me fait remarquer que en fait la photo de legendre que j'utilisais les nerfs fois et pas une photo de rater les maris le jean dans les cimes photo d'un politicien français lui le gendre et le problème c'est que quatrième mari le genre bon connais pas de photo de lui à part cette caricature que je trouve assez horrible le mec il a quand même inventé la méthode la plus utilisée aujourd'hui en sciences il fonça régression linéaire c'est peut-être pour avoir le truc le plus utilisé dans science était la plus value et le pauvre legendre la seule image connais de lui c'est une caricature absolument horrible elle espère que vous avez aimé cet épisode à partir de la prochaine vidéo et pour plusieurs vidéos on va parler du problème le plus fondamental en fait d'une machine learning et de l'apprentissage statistiques de façon générale qui est le problème de leurs critiques la surinterprétation va voir que c'est vraiment un problème extrêmement notre idéal que beaucoup considèrent encore non résolus aujourd'hui et on va voir que ce problème de sur interprétations de pervitine c'est un problème qui nous concerne directement à lustin panne de machine learning d'intelligence artificielle c'est un problème de vue harding d'apprentissage est le problème l'intelligence si vous avez aimez cette vidéo pensez à la ligue et à la commenter avait partagé pense à vous abonner pour sur l'épisode merci aux tireurs fous leurs dons et j'espère que la prochaine fois comment est-ce que tu le fais pour calculer le taux d'intérêt que tu vas demander on rigole ça fait paniquer rim dit penser qu'une idée eh ben il existe une formule presque magique qui permet de calculer ce risque elle est très peu connu du grand public mais elle est applicable à n'importe quel investissement il s'agirait de la méthode du doigt mouillé une feinte this is your ride your bicycle machine onyango redemption déjà on y parle depuis causé des fuites de djeddah it can be gouraya tu crées news show room une fugueuse lusine dans l'ignorance formation in den plas y ayant opté charles les machines ordinaires gordon tu vises dame antonio piccioni be careful d'aulnat tu as tout mini new features d'une attitude over feat et or training 7